{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDj9It95WwZZ"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoHNbvqPXP0i"
      },
      "source": [
        "## Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siuTPLAHWlnh",
        "outputId": "519ed374-41f8-481d-ee4a-592702d4b757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haGp_TVoWqN8",
        "outputId": "ff962178-c7c3-460a-fea0-3026c6f90fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows and columns: (1259, 7)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv(\"/content/drive/My Drive/6998/project/TSLA.csv\")\n",
        "print(\"Number of rows and columns:\", df.shape)\n",
        "df.head(5)\n",
        "\n",
        "training_set = df.iloc[:800, 1:2].values\n",
        "test_set = df.iloc[800:, 1:2].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCP3A5IlWvYD",
        "outputId": "937ddd34-dd73-4481-e7cb-77c0bb0076a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(740,)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Feature Scaling\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "# Creating a data structure with 60 time-steps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(60, 800):\n",
        "    X_train.append(training_set_scaled[i-60:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "X_train.shape\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maC-G6XPXRc_"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FBz8dXDXOuB"
      },
      "outputs": [],
      "source": [
        "# Getting the predicted stock price of 2017\n",
        "dataset_train = df.iloc[:800, 1:2]\n",
        "dataset_test = df.iloc[800:, 1:2]\n",
        "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs)\n",
        "X_test = []\n",
        "y_test = []\n",
        "for i in range(60, 519):\n",
        "    X_test.append(inputs[i-60:i, 0])\n",
        "    y_test.append(inputs[i, 0])\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp6l6etXWzra"
      },
      "source": [
        "# Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZIWoAIkp4MU",
        "outputId": "51632f4c-ecd6-4868-d43c-b1aa1bc86eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR5FXSjtYBFd"
      },
      "source": [
        "## StateSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upB0m8LTon6y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pprint\n",
        "from collections import OrderedDict\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import os\n",
        "if not os.path.exists('weights/'):\n",
        "    os.makedirs('weights/')\n",
        "\n",
        "\n",
        "class StateSpace:\n",
        "    '''\n",
        "    State Space manager\n",
        "\n",
        "    Provides utilit functions for holding \"states\" / \"actions\" that the controller\n",
        "    must use to train and predict.\n",
        "\n",
        "    Also provides a more convenient way to define the search space\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.states = OrderedDict()\n",
        "        self.state_count_ = 0\n",
        "\n",
        "    def add_state(self, name, values):\n",
        "        '''\n",
        "        Adds a \"state\" to the state manager, along with some metadata for efficient\n",
        "        packing and unpacking of information required by the RNN Controller.\n",
        "\n",
        "        Stores metadata such as:\n",
        "        -   Global ID\n",
        "        -   Name\n",
        "        -   Valid Values\n",
        "        -   Number of valid values possible\n",
        "        -   Map from value ID to state value\n",
        "        -   Map from state value to value ID\n",
        "\n",
        "        Args:\n",
        "            name: name of the state / action\n",
        "            values: valid values that this state can take\n",
        "\n",
        "        Returns:\n",
        "            Global ID of the state. Can be used to refer to this state later.\n",
        "        '''\n",
        "        index_map = {}\n",
        "        for i, val in enumerate(values):\n",
        "            index_map[i] = val\n",
        "\n",
        "        value_map = {}\n",
        "        for i, val in enumerate(values):\n",
        "            value_map[val] = i\n",
        "\n",
        "        metadata = {\n",
        "            'id': self.state_count_,\n",
        "            'name': name,\n",
        "            'values': values,\n",
        "            'size': len(values),\n",
        "            'index_map_': index_map,\n",
        "            'value_map_': value_map,\n",
        "        }\n",
        "        self.states[self.state_count_] = metadata\n",
        "        self.state_count_ += 1\n",
        "\n",
        "        return self.state_count_ - 1\n",
        "\n",
        "    def embedding_encode(self, id, value):\n",
        "        '''\n",
        "        Embedding index encode the specific state value\n",
        "\n",
        "        Args:\n",
        "            id: global id of the state\n",
        "            value: state value\n",
        "\n",
        "        Returns:\n",
        "            embedding encoded representation of the state value\n",
        "        '''\n",
        "        state = self[id]\n",
        "        size = state['size']\n",
        "        value_map = state['value_map_']\n",
        "        value_idx = value_map[value]\n",
        "\n",
        "        one_hot = np.zeros((1, size), dtype=np.float32)\n",
        "        one_hot[np.arange(1), value_idx] = value_idx + 1\n",
        "        return one_hot\n",
        "\n",
        "    def get_state_value(self, id, index):\n",
        "        '''\n",
        "        Retrieves the state value from the state value ID\n",
        "\n",
        "        Args:\n",
        "            id: global id of the state\n",
        "            index: index of the state value (usually from argmax)\n",
        "\n",
        "        Returns:\n",
        "            The actual state value at given value index\n",
        "        '''\n",
        "        state = self[id]\n",
        "        index_map = state['index_map_']\n",
        "\n",
        "        if (type(index) == list or type(index) == np.ndarray) and len(index) == 1:\n",
        "            index = index[0]\n",
        "\n",
        "        value = index_map[index]\n",
        "        return value\n",
        "\n",
        "    def get_random_state_space(self, num_layers):\n",
        "        '''\n",
        "        Constructs a random initial state space for feeding as an initial value\n",
        "        to the Controller RNN\n",
        "\n",
        "        Args:\n",
        "            num_layers: number of layers to duplicate the search space\n",
        "\n",
        "        Returns:\n",
        "            A list of one hot encoded states\n",
        "        '''\n",
        "        states = []\n",
        "\n",
        "        for id in range(self.size * num_layers):\n",
        "            state = self[id]\n",
        "            size = state['size']\n",
        "\n",
        "            sample = np.random.choice(size, size=1)\n",
        "            sample = state['index_map_'][sample[0]]\n",
        "            state = self.embedding_encode(id, sample)\n",
        "            states.append(state)\n",
        "        return states\n",
        "\n",
        "    def parse_state_space_list(self, state_list):\n",
        "        '''\n",
        "        Parses a list of one hot encoded states to retrieve a list of state values\n",
        "\n",
        "        Args:\n",
        "            state_list: list of one hot encoded states\n",
        "\n",
        "        Returns:\n",
        "            list of state values\n",
        "        '''\n",
        "        state_values = []\n",
        "        for id, state_one_hot in enumerate(state_list):\n",
        "            state_val_idx = np.argmax(state_one_hot, axis=-1)[0]\n",
        "            value = self.get_state_value(id, state_val_idx)\n",
        "            state_values.append(value)\n",
        "\n",
        "        return state_values\n",
        "\n",
        "    def print_state_space(self):\n",
        "        ''' Pretty print the state space '''\n",
        "        print('*' * 40, 'STATE SPACE', '*' * 40)\n",
        "\n",
        "        pp = pprint.PrettyPrinter(indent=2, width=100)\n",
        "        for id, state in self.states.items():\n",
        "            pp.pprint(state)\n",
        "            print()\n",
        "\n",
        "    def print_actions(self, actions):\n",
        "        ''' Print the action space properly '''\n",
        "        print('Actions :')\n",
        "\n",
        "        for id, action in enumerate(actions):\n",
        "            if id % self.size == 0:\n",
        "                print(\"*\" * 20, \"Layer %d\" % (((id + 1) // self.size) + 1), \"*\" * 20)\n",
        "\n",
        "            state = self[id]\n",
        "            name = state['name']\n",
        "            vals = [(n, p) for n, p in zip(state['values'], *action)]\n",
        "            print(\"%s : \" % name, vals)\n",
        "        print()\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        return self.states[id % self.size]\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.state_count_\n",
        "\n",
        "\n",
        "class Controller:\n",
        "    '''\n",
        "    Utility class to manage the RNN Controller\n",
        "    '''\n",
        "    def __init__(self, policy_session, num_layers, state_space,\n",
        "                 reg_param=0.001,\n",
        "                 discount_factor=0.99,\n",
        "                 exploration=0.8,\n",
        "                 controller_cells=32,\n",
        "                 embedding_dim=20,\n",
        "                 clip_norm=0.0,\n",
        "                 restore_controller=False):\n",
        "        self.policy_session = policy_session  # type: tf.Session\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.state_space = state_space  # type: StateSpace\n",
        "        self.state_size = self.state_space.size\n",
        "\n",
        "        self.controller_cells = controller_cells\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.reg_strength = reg_param\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration = exploration\n",
        "        self.restore_controller = restore_controller\n",
        "        self.clip_norm = clip_norm\n",
        "\n",
        "        self.reward_buffer = []\n",
        "        self.state_buffer = []\n",
        "\n",
        "        self.cell_outputs = []\n",
        "        self.policy_classifiers = []\n",
        "        self.policy_actions = []\n",
        "        self.policy_labels = []\n",
        "\n",
        "        self.build_policy_network()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        '''\n",
        "        Gets a one hot encoded action list, either from random sampling or from\n",
        "        the Controller RNN\n",
        "\n",
        "        Args:\n",
        "            state: a list of one hot encoded states, whose first value is used as initial\n",
        "                state for the controller RNN\n",
        "\n",
        "        Returns:\n",
        "            A one hot encoded action list\n",
        "        '''\n",
        "        if np.random.random() < self.exploration:\n",
        "            print(\"Generating random action to explore\")\n",
        "            actions = []\n",
        "\n",
        "            for i in range(self.state_size * self.num_layers):\n",
        "                state_ = self.state_space[i]\n",
        "                size = state_['size']\n",
        "\n",
        "                sample = np.random.choice(size, size=1)\n",
        "                sample = state_['index_map_'][sample[0]]\n",
        "                action = self.state_space.embedding_encode(i, sample)\n",
        "                actions.append(action)\n",
        "            return actions\n",
        "\n",
        "        else:\n",
        "            print(\"Prediction action from Controller\")\n",
        "            initial_state = self.state_space[0]\n",
        "            size = initial_state['size']\n",
        "\n",
        "            if state[0].shape != (1, size):\n",
        "                state = state[0].reshape((1, size)).astype('int32')\n",
        "            else:\n",
        "                state = state[0]\n",
        "\n",
        "            print(\"State input to Controller for Action : \", state.flatten())\n",
        "\n",
        "            with self.policy_session.as_default():\n",
        "                tf.compat.v1.keras.backend.set_session(self.policy_session)\n",
        "\n",
        "                with tf.name_scope('action_prediction'):\n",
        "                    pred_actions = self.policy_session.run(self.policy_actions, feed_dict={self.state_input: state})\n",
        "\n",
        "                return pred_actions\n",
        "\n",
        "    def build_policy_network(self):\n",
        "        with self.policy_session.as_default():\n",
        "            tf.compat.v1.keras.backend.set_session(self.policy_session)\n",
        "\n",
        "            with tf.name_scope('controller'):\n",
        "                with tf.compat.v1.variable_scope('policy_network'):\n",
        "\n",
        "                    # state input is the first input fed into the controller RNN.\n",
        "                    # the rest of the inputs are fed to the RNN internally\n",
        "                    with tf.name_scope('state_input'):\n",
        "                        state_input = tf.placeholder(dtype=tf.int32, shape=(1, None), name='state_input')\n",
        "\n",
        "                    self.state_input = state_input\n",
        "\n",
        "                    # we can use LSTM as the controller as well\n",
        "                    nas_cell = tf.nn.rnn_cell.LSTMCell(self.controller_cells)\n",
        "                    cell_state = nas_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
        "\n",
        "                    embedding_weights = []\n",
        "\n",
        "                    # for each possible state, create a new embedding. Reuse the weights for multiple layers.\n",
        "                    with tf.compat.v1.variable_scope('embeddings', reuse=tf.AUTO_REUSE):\n",
        "                        for i in range(self.state_size):\n",
        "                            state_ = self.state_space[i]\n",
        "                            size = state_['size']\n",
        "\n",
        "                            # size + 1 is used so that 0th index is never updated and is \"default\" value\n",
        "                            weights = tf.get_variable('state_embeddings_%d' % i,\n",
        "                                                      shape=[size + 1, self.embedding_dim],\n",
        "                                                      initializer=tf.initializers.random_uniform(-1., 1.))\n",
        "\n",
        "                            embedding_weights.append(weights)\n",
        "\n",
        "                        # initially, cell input will be 1st state input\n",
        "                        embeddings = tf.nn.embedding_lookup(embedding_weights[0], state_input)\n",
        "\n",
        "                    cell_input = embeddings\n",
        "\n",
        "                    # we provide a flat list of chained input-output to the RNN\n",
        "                    for i in range(self.state_size * self.num_layers):\n",
        "                        state_id = i % self.state_size\n",
        "                        state_space = self.state_space[i]\n",
        "                        size = state_space['size']\n",
        "\n",
        "                        with tf.name_scope('controller_output_%d' % i):\n",
        "                            # feed the ith layer input (i-1 layer output) to the RNN\n",
        "                            outputs, final_state = tf.nn.dynamic_rnn(nas_cell,\n",
        "                                                                     cell_input,\n",
        "                                                                     initial_state=cell_state,\n",
        "                                                                     dtype=tf.float32)\n",
        "\n",
        "                            # add a new classifier for each layers output\n",
        "                            classifier = tf.layers.dense(outputs[:, -1, :], units=size, name='classifier_%d' % (i),\n",
        "                                                         reuse=False)\n",
        "                            preds = tf.nn.softmax(classifier)\n",
        "\n",
        "                            # feed the previous layer (i-1 layer output) to the next layers input, along with state\n",
        "                            # take the class label\n",
        "                            cell_input = tf.argmax(preds, axis=-1)\n",
        "                            cell_input = tf.expand_dims(cell_input, -1, name='pred_output_%d' % (i))\n",
        "                            cell_input = tf.cast(cell_input, tf.int32)\n",
        "                            cell_input = tf.add(cell_input, 1)  # we avoid using 0 so as to have a \"default\" embedding at 0th index\n",
        "\n",
        "                            # embedding lookup of this state using its state weights ; reuse weights\n",
        "                            cell_input = tf.nn.embedding_lookup(embedding_weights[state_id], cell_input,\n",
        "                                                           name='cell_output_%d' % (i))\n",
        "\n",
        "                            cell_state = final_state\n",
        "\n",
        "                        # store the tensors for later loss computation\n",
        "                        self.cell_outputs.append(cell_input)\n",
        "                        self.policy_classifiers.append(classifier)\n",
        "                        self.policy_actions.append(preds)\n",
        "\n",
        "            policy_net_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='policy_network')\n",
        "\n",
        "            with tf.name_scope('optimizer'):\n",
        "                self.global_step = tf.Variable(0, trainable=False)\n",
        "                starter_learning_rate = 0.1\n",
        "                learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
        "                                                           500, 0.95, staircase=True)\n",
        "\n",
        "                tf.summary.scalar('learning_rate', learning_rate)\n",
        "\n",
        "                self.optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "            with tf.name_scope('losses'):\n",
        "                self.discounted_rewards = tf.placeholder(tf.float32, shape=(None,), name='discounted_rewards')\n",
        "                tf.summary.scalar('discounted_reward', tf.reduce_sum(self.discounted_rewards))\n",
        "\n",
        "                # calculate sum of all the individual classifiers\n",
        "                cross_entropy_loss = 0\n",
        "                for i in range(self.state_size * self.num_layers):\n",
        "                    classifier = self.policy_classifiers[i]\n",
        "                    state_space = self.state_space[i]\n",
        "                    size = state_space['size']\n",
        "\n",
        "                    with tf.name_scope('state_%d' % (i + 1)):\n",
        "                        labels = tf.placeholder(dtype=tf.float32, shape=(None, size), name='cell_label_%d' % i)\n",
        "                        self.policy_labels.append(labels)\n",
        "\n",
        "                        ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=classifier, labels=labels)\n",
        "                        tf.summary.scalar('state_%d_ce_loss' % (i + 1), tf.reduce_mean(ce_loss))\n",
        "\n",
        "                    cross_entropy_loss += ce_loss\n",
        "\n",
        "                policy_gradient_loss = tf.reduce_mean(cross_entropy_loss)\n",
        "                reg_loss = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_net_variables])  # Regularization\n",
        "\n",
        "                # sum up policy gradient and regularization loss\n",
        "                self.total_loss = policy_gradient_loss + self.reg_strength * reg_loss\n",
        "                tf.summary.scalar('total_loss', self.total_loss)\n",
        "\n",
        "                self.gradients = self.optimizer.compute_gradients(self.total_loss)\n",
        "\n",
        "                with tf.name_scope('policy_gradients'):\n",
        "                    # normalize gradients so that they dont explode if argument passed\n",
        "                    if self.clip_norm is not None and self.clip_norm != 0.0:\n",
        "                        norm = tf.constant(self.clip_norm, dtype=tf.float32)\n",
        "                        gradients, vars = zip(*self.gradients)  # unpack the two lists of gradients and the variables\n",
        "                        gradients, _ = tf.clip_by_global_norm(gradients, norm)  # clip by the norm\n",
        "                        self.gradients = list(zip(gradients, vars))  # we need to set values later, convert to list\n",
        "\n",
        "                    # compute policy gradients\n",
        "                    for i, (grad, var) in enumerate(self.gradients):\n",
        "                        if grad is not None:\n",
        "                            self.gradients[i] = (grad * self.discounted_rewards, var)\n",
        "\n",
        "                # training update\n",
        "                with tf.name_scope(\"train_policy_network\"):\n",
        "                    # apply gradients to update policy network\n",
        "                    self.train_op = self.optimizer.apply_gradients(self.gradients, global_step=self.global_step)\n",
        "\n",
        "            self.summaries_op = tf.summary.merge_all()\n",
        "\n",
        "            timestr = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "            filename = 'logs/%s' % timestr\n",
        "\n",
        "            self.summary_writer = tf.summary.FileWriter(filename, graph=self.policy_session.graph)\n",
        "\n",
        "            self.policy_session.run(tf.global_variables_initializer())\n",
        "            self.saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "            if self.restore_controller:\n",
        "                path = tf.train.latest_checkpoint('weights/')\n",
        "\n",
        "                if path is not None and tf.train.checkpoint_exists(path):\n",
        "                    print(\"Loading Controller Checkpoint !\")\n",
        "                    self.saver.restore(self.policy_session, path)\n",
        "\n",
        "    def store_rollout(self, state, reward):\n",
        "        self.reward_buffer.append(reward)\n",
        "        self.state_buffer.append(state)\n",
        "\n",
        "        # dump buffers to file if it grows larger than 50 items\n",
        "        if len(self.reward_buffer) > 20:\n",
        "            with open('buffers.txt', mode='a+') as f:\n",
        "                for i in range(20):\n",
        "                    state_ = self.state_buffer[i]\n",
        "                    state_list = self.state_space.parse_state_space_list(state_)\n",
        "                    state_list = ','.join(str(v) for v in state_list)\n",
        "\n",
        "                    f.write(\"%0.4f,%s\\n\" % (self.reward_buffer[i], state_list))\n",
        "\n",
        "                print(\"Saved buffers to file `buffers.txt` !\")\n",
        "\n",
        "            self.reward_buffer = [self.reward_buffer[-1]]\n",
        "            self.state_buffer = [self.state_buffer[-1]]\n",
        "\n",
        "    def discount_rewards(self):\n",
        "        '''\n",
        "        Compute discounted rewards over the entire reward buffer\n",
        "\n",
        "        Returns:\n",
        "            Discounted reward value\n",
        "        '''\n",
        "        rewards = np.asarray(self.reward_buffer)\n",
        "        discounted_rewards = np.zeros_like(rewards)\n",
        "        running_add = 0\n",
        "        for t in reversed(range(0, rewards.size)):\n",
        "            if rewards[t] != 0:\n",
        "                running_add = 0\n",
        "            running_add = running_add * self.discount_factor + rewards[t]\n",
        "            discounted_rewards[t] = running_add\n",
        "        return discounted_rewards[-1]\n",
        "\n",
        "    def train_step(self):\n",
        "        '''\n",
        "        Perform a single train step on the Controller RNN\n",
        "\n",
        "        Returns:\n",
        "            the training loss\n",
        "        '''\n",
        "        states = self.state_buffer[-1]\n",
        "        label_list = []\n",
        "\n",
        "        # parse the state space to get real value of the states,\n",
        "        # then one hot encode them for comparison with the predictions\n",
        "        state_list = self.state_space.parse_state_space_list(states)\n",
        "        for id, state_value in enumerate(state_list):\n",
        "            state_one_hot = self.state_space.embedding_encode(id, state_value)\n",
        "            label_list.append(state_one_hot)\n",
        "\n",
        "        # the initial input to the controller RNN\n",
        "        state_input_size = self.state_space[0]['size']\n",
        "        state_input = states[0].reshape((1, state_input_size)).astype('int32')\n",
        "        print(\"State input to Controller for training : \", state_input.flatten())\n",
        "\n",
        "        # the discounted reward value\n",
        "        reward = self.discount_rewards()\n",
        "        reward = np.asarray([reward]).astype('float32')\n",
        "\n",
        "        feed_dict = {\n",
        "            self.state_input: state_input,\n",
        "            self.discounted_rewards: reward\n",
        "        }\n",
        "\n",
        "        # prepare the feed dict with the values of all the policy labels for each\n",
        "        # of the Controller outputs\n",
        "        for i, label in enumerate(label_list):\n",
        "            feed_dict[self.policy_labels[i]] = label\n",
        "\n",
        "        with self.policy_session.as_default():\n",
        "            tf.compat.v1.keras.backend.set_session(self.policy_session)\n",
        "\n",
        "            print(\"Training RNN (States ip) : \", state_list)\n",
        "            print(\"Training RNN (Reward ip) : \", reward.flatten())\n",
        "            _, loss, summary, global_step = self.policy_session.run([self.train_op, self.total_loss, self.summaries_op,\n",
        "                                                                     self.global_step],\n",
        "                                                                     feed_dict=feed_dict)\n",
        "\n",
        "            self.summary_writer.add_summary(summary, global_step)\n",
        "            self.saver.save(self.policy_session, save_path='weights/controller.ckpt', global_step=self.global_step)\n",
        "\n",
        "            # reduce exploration after many train steps\n",
        "            if global_step != 0 and global_step % 20 == 0 and self.exploration > 0.5:\n",
        "                self.exploration *= 0.99\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def remove_files(self):\n",
        "        files = ['train_history.csv', 'buffers.txt']\n",
        "\n",
        "        for file in files:\n",
        "            if os.path.exists(file):\n",
        "                os.remove(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlyU8FB0YFP7"
      },
      "source": [
        "## Starting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd6Cm3k5ovhm"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Input\n",
        "\n",
        "# generic model design\n",
        "def model_fn(actions):\n",
        "    unit_1, dropout_1, unit_2, dropout_2, unit_3, dropout_3, unit_4, dropout_4, unit_5, dropout_5, unit_6, dropout_6 = actions\n",
        "\n",
        "    model = Sequential()\n",
        "    #Adding the first LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = unit_1, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
        "    model.add(Dropout(dropout_1))\n",
        "    # Adding a second LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = unit_2, return_sequences = True))\n",
        "    model.add(Dropout(dropout_2))\n",
        "    # Adding a second LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = unit_3, return_sequences = True))\n",
        "    model.add(Dropout(dropout_3))\n",
        "    # Adding a second LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = unit_4, return_sequences = True))\n",
        "    model.add(Dropout(dropout_4))\n",
        "    # Adding a second LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = unit_5, return_sequences = True))\n",
        "    model.add(Dropout(dropout_5))\n",
        "    # Adding a second LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = unit_6, return_sequences = True))\n",
        "    model.add(Dropout(dropout_6))\n",
        "    \"\"\"\n",
        "    # Adding a third LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = 50, return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    # Adding a fourth LSTM layer and some Dropout regularisation\n",
        "    model.add(LSTM(units = 50))\n",
        "    model.add(Dropout(0.2))\n",
        "    \"\"\"\n",
        "    model.add(LSTM(units = 50))\n",
        "    model.add(Dropout(0.2))\n",
        "    # Adding the output layer\n",
        "    model.add(Dense(units = 1))\n",
        "\n",
        "    # Compiling the RNN\n",
        "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y38DdXtxYDti"
      },
      "source": [
        "## Network Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm6jDrvkosD6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "class NetworkManager:\n",
        "    '''\n",
        "    Helper class to manage the generation of subnetwork training given a dataset\n",
        "    '''\n",
        "    def __init__(self, dataset, epochs=5, child_batchsize=128, acc_beta=0.8, clip_rewards=0.0):\n",
        "        '''\n",
        "        Manager which is tasked with creating subnetworks, training them on a dataset, and retrieving\n",
        "        rewards in the term of accuracy, which is passed to the controller RNN.\n",
        "\n",
        "        Args:\n",
        "            dataset: a tuple of 4 arrays (X_train, y_train, X_val, y_val)\n",
        "            epochs: number of epochs to train the subnetworks\n",
        "            child_batchsize: batchsize of training the subnetworks\n",
        "            acc_beta: exponential weight for the accuracy\n",
        "            clip_rewards: float - to clip rewards in [-range, range] to prevent\n",
        "                large weight updates. Use when training is highly unstable.\n",
        "        '''\n",
        "        self.dataset = dataset\n",
        "        self.epochs = epochs\n",
        "        self.batchsize = child_batchsize\n",
        "        self.clip_rewards = clip_rewards\n",
        "\n",
        "        self.beta = acc_beta\n",
        "        self.beta_bias = acc_beta\n",
        "        self.moving_loss = float('inf')\n",
        "\n",
        "    def get_rewards(self, model_fn, actions):\n",
        "        '''\n",
        "        Creates a subnetwork given the actions predicted by the controller RNN,\n",
        "        trains it on the provided dataset, and then returns a reward.\n",
        "\n",
        "        Args:\n",
        "            model_fn: a function which accepts one argument, a list of\n",
        "                parsed actions, obtained via an inverse mapping from the\n",
        "                StateSpace.\n",
        "            actions: a list of parsed actions obtained via an inverse mapping\n",
        "                from the StateSpace. It is in a specific order as given below:\n",
        "\n",
        "                Consider 4 states were added to the StateSpace via the `add_state`\n",
        "                method. Then the `actions` array will be of length 4, with the\n",
        "                values of those states in the order that they were added.\n",
        "\n",
        "                If number of layers is greater than one, then the `actions` array\n",
        "                will be of length `4 * number of layers` (in the above scenario).\n",
        "                The index from [0:4] will be for layer 0, from [4:8] for layer 1,\n",
        "                etc for the number of layers.\n",
        "\n",
        "                These action values are for direct use in the construction of models.\n",
        "\n",
        "        Returns:\n",
        "            a reward for training a model with the given actions\n",
        "        '''\n",
        "        with tf.Session(graph=tf.Graph()) as network_sess:\n",
        "            tf.compat.v1.keras.backend.set_session(network_sess)\n",
        "\n",
        "            # generate a submodel given predicted actions\n",
        "            model = model_fn(actions)  # type: Model\n",
        "            model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "            # unpack the dataset\n",
        "            X_train, y_train, X_val, y_val = self.dataset\n",
        "\n",
        "            # train the model using Keras methods\n",
        "            model.fit(X_train, y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
        "                      verbose=1, validation_data=(X_val, y_val),\n",
        "                      callbacks=[ModelCheckpoint('weights/temp_network.h5',\n",
        "                                                 monitor='val_loss', verbose=1,\n",
        "                                                 save_best_only=True,\n",
        "                                                 save_weights_only=True)])\n",
        "\n",
        "            # load best performance epoch in this training session\n",
        "            model.load_weights('weights/temp_network.h5')\n",
        "\n",
        "            # evaluate the model\n",
        "            loss = model.evaluate(X_val, y_val, batch_size=self.batchsize)\n",
        "\n",
        "            # compute the reward\n",
        "            reward = (self.moving_loss - loss)\n",
        "\n",
        "            # if rewards are clipped, clip them in the range -0.05 to 0.05\n",
        "            if self.clip_rewards:\n",
        "                reward = np.clip(reward, -0.05, 0.05)\n",
        "\n",
        "            # update moving accuracy with bias correction for 1st update\n",
        "            if self.beta > 0.0 and self.beta < 1.0:\n",
        "                self.moving_loss = self.beta * self.moving_loss + (1 - self.beta) * loss\n",
        "                self.moving_loss = self.moving_loss / (1 - self.beta_bias)\n",
        "                self.beta_bias = 0\n",
        "\n",
        "                reward = np.clip(reward, -0.1, 0.1)\n",
        "\n",
        "            print()\n",
        "            print(\"Manager: EWA Loss = \", self.moving_loss)\n",
        "\n",
        "        # clean up resources and GPU memory\n",
        "        network_sess.close()\n",
        "\n",
        "        return reward, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrT-D8WvYGff"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1k6hhMCokLQ",
        "outputId": "5aaaa857-9075-4af8-f21a-26fdbafc1aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**************************************** STATE SPACE ****************************************\n",
            "{ 'id': 0,\n",
            "  'index_map_': {0: 20, 1: 40, 2: 50, 3: 60, 4: 80, 5: 100},\n",
            "  'name': 'unit',\n",
            "  'size': 6,\n",
            "  'value_map_': {20: 0, 40: 1, 50: 2, 60: 3, 80: 4, 100: 5},\n",
            "  'values': [20, 40, 50, 60, 80, 100]}\n",
            "\n",
            "{ 'id': 1,\n",
            "  'index_map_': {0: 0, 1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4, 5: 0.5},\n",
            "  'name': 'dropout',\n",
            "  'size': 6,\n",
            "  'value_map_': {0: 0, 0.1: 1, 0.2: 2, 0.3: 3, 0.4: 4, 0.5: 5},\n",
            "  'values': [0, 0.1, 0.2, 0.3, 0.4, 0.5]}\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-414a5f96e973>:310: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:992: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:276: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:314: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:255: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Initial Random State :  [20, 0.2, 60, 0.3, 80, 0.4, 100, 0.2, 50, 0.2, 100, 0.5]\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.3, 20, 0.4, 100, 0.3, 40, 0.5, 100, 0.1, 80, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1962"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0157 - val_loss: 0.5487\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0160\n",
            "Epoch 00008: val_loss did not improve from 0.51898\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0160 - val_loss: 0.6220\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00009: val_loss did not improve from 0.51898\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0167 - val_loss: 0.6430\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00010: val_loss did not improve from 0.51898\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0158 - val_loss: 0.6259\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5189817907503748\n",
            "Total reward :  19.0\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.3, 50, 0.1, 80, 0.4, 100, 0.5, 100, 0.3, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 190: Controller loss : 46.228104\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [40, 0.1, 50, 0.3, 20, 0.5, 40, 0.2, 20, 0.5, 100, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2016\n",
            "Epoch 00001: val_loss improved from inf to 0.73077, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.2016 - val_loss: 0.7308\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0754\n",
            "Epoch 00002: val_loss did not improve from 0.73077\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0754 - val_loss: 1.3231\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0628\n",
            "Epoch 00003: val_loss did not improve from 0.73077\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0628 - val_loss: 1.0498\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00004: val_loss did not improve from 0.73077\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0435 - val_loss: 0.9374\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00005: val_loss did not improve from 0.73077\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0331 - val_loss: 1.0586\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0274\n",
            "Epoch 00006: val_loss did not improve from 0.73077\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0274 - val_loss: 0.8167\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0222\n",
            "Epoch 00007: val_loss did not improve from 0.73077\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0222 - val_loss: 0.8344\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00008: val_loss improved from 0.73077 to 0.72551, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0187 - val_loss: 0.7255\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00009: val_loss did not improve from 0.72551\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0199 - val_loss: 0.7530\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00010: val_loss improved from 0.72551 to 0.71193, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0186 - val_loss: 0.7119\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7119285037055254\n",
            "Total reward :  19.1\n",
            "State input to Controller for training :  [0 2 0 0 0 0]\n",
            "Training RNN (States ip) :  [40, 0.1, 50, 0.3, 20, 0.5, 40, 0.2, 20, 0.5, 100, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 191: Controller loss : 1205.525757\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "\n",
            "Predicted actions :  [20, 0, 20, 0.3, 50, 0.2, 100, 0.3, 20, 0.3, 60, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1895\n",
            "Epoch 00001: val_loss improved from inf to 0.78722, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.1895 - val_loss: 0.7872\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0591\n",
            "Epoch 00002: val_loss did not improve from 0.78722\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0591 - val_loss: 1.2740\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0436\n",
            "Epoch 00003: val_loss did not improve from 0.78722\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0436 - val_loss: 0.8851\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00004: val_loss did not improve from 0.78722\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0384 - val_loss: 1.0105\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0302\n",
            "Epoch 00005: val_loss did not improve from 0.78722\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0302 - val_loss: 0.8795\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0242\n",
            "Epoch 00006: val_loss improved from 0.78722 to 0.78535, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0242 - val_loss: 0.7853\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0201\n",
            "Epoch 00007: val_loss improved from 0.78535 to 0.70567, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0201 - val_loss: 0.7057\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00008: val_loss improved from 0.70567 to 0.68831, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0191 - val_loss: 0.6883\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00009: val_loss improved from 0.68831 to 0.66333, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.6633\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00010: val_loss did not improve from 0.66333\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.7359\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6633284632157137\n",
            "Total reward :  19.200000000000003\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0, 20, 0.3, 50, 0.2, 100, 0.3, 20, 0.3, 60, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 192: Controller loss : 874.333435\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [1. 0. 0. 0. 0. 0.]\n",
            "trying these actions:  [array([[5.7631280e-37, 8.8955572e-20, 6.1393546e-16, 4.1856735e-22,\n",
            "        2.2642331e-11, 1.0000000e+00]], dtype=float32), array([[3.2621666e-15, 2.5517073e-01, 4.6674144e-03, 7.3963511e-01,\n",
            "        5.2555051e-04, 1.2032476e-06]], dtype=float32), array([[4.2587580e-15, 2.6463784e-10, 9.9995339e-01, 8.0185290e-09,\n",
            "        4.6568803e-05, 4.5347596e-21]], dtype=float32), array([[1.8293818e-08, 3.8678101e-01, 1.1256693e-02, 3.6487111e-07,\n",
            "        6.0155880e-01, 4.0308962e-04]], dtype=float32), array([[9.7591215e-31, 1.7354285e-09, 8.8894065e-09, 1.0754646e-04,\n",
            "        9.9910104e-01, 7.9148420e-04]], dtype=float32), array([[4.7540401e-29, 9.8120809e-09, 2.7179837e-13, 2.1287175e-08,\n",
            "        1.0000000e+00, 7.8477932e-11]], dtype=float32), array([[3.6496165e-34, 4.5949996e-27, 1.3897914e-25, 1.4931025e-07,\n",
            "        3.2980889e-09, 9.9999988e-01]], dtype=float32), array([[5.4277968e-11, 1.0874551e-12, 2.7341210e-13, 8.0696594e-07,\n",
            "        1.7065085e-08, 9.9999917e-01]], dtype=float32), array([[1.2007663e-16, 1.2504272e-20, 1.7863528e-25, 5.0145073e-30,\n",
            "        1.0200459e-08, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 2.6306459e-22, 2.4469877e-13, 9.9999356e-01,\n",
            "        6.4922137e-06, 5.9358600e-13]], dtype=float32), array([[7.6169943e-10, 9.6787516e-11, 1.0974457e-09, 9.8421469e-07,\n",
            "        6.1548257e-16, 9.9999905e-01]], dtype=float32), array([[1.5801450e-03, 2.1737074e-08, 9.9840146e-01, 1.5949129e-06,\n",
            "        1.5319719e-12, 1.6822207e-05]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 5.763128e-37), (40, 8.895557e-20), (50, 6.1393546e-16), (60, 4.1856735e-22), (80, 2.264233e-11), (100, 1.0)]\n",
            "dropout :  [(0, 3.2621666e-15), (0.1, 0.25517073), (0.2, 0.0046674144), (0.3, 0.7396351), (0.4, 0.0005255505), (0.5, 1.2032476e-06)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 4.258758e-15), (40, 2.6463784e-10), (50, 0.9999534), (60, 8.018529e-09), (80, 4.6568803e-05), (100, 4.5347596e-21)]\n",
            "dropout :  [(0, 1.8293818e-08), (0.1, 0.386781), (0.2, 0.011256693), (0.3, 3.648711e-07), (0.4, 0.6015588), (0.5, 0.00040308962)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 9.7591215e-31), (40, 1.7354285e-09), (50, 8.8894065e-09), (60, 0.00010754646), (80, 0.99910104), (100, 0.0007914842)]\n",
            "dropout :  [(0, 4.75404e-29), (0.1, 9.812081e-09), (0.2, 2.7179837e-13), (0.3, 2.1287175e-08), (0.4, 1.0), (0.5, 7.847793e-11)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 3.6496165e-34), (40, 4.5949996e-27), (50, 1.3897914e-25), (60, 1.4931025e-07), (80, 3.298089e-09), (100, 0.9999999)]\n",
            "dropout :  [(0, 5.4277968e-11), (0.1, 1.0874551e-12), (0.2, 2.734121e-13), (0.3, 8.0696594e-07), (0.4, 1.7065085e-08), (0.5, 0.99999917)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.2007663e-16), (40, 1.2504272e-20), (50, 1.7863528e-25), (60, 5.0145073e-30), (80, 1.0200459e-08), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.630646e-22), (0.2, 2.4469877e-13), (0.3, 0.99999356), (0.4, 6.4922137e-06), (0.5, 5.93586e-13)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 7.616994e-10), (40, 9.6787516e-11), (50, 1.0974457e-09), (60, 9.842147e-07), (80, 6.1548257e-16), (100, 0.99999905)]\n",
            "dropout :  [(0, 0.001580145), (0.1, 2.1737074e-08), (0.2, 0.99840146), (0.3, 1.5949129e-06), (0.4, 1.5319719e-12), (0.5, 1.6822207e-05)]\n",
            "\n",
            "Predicted actions :  [100, 0.3, 50, 0.4, 80, 0.4, 100, 0.5, 100, 0.3, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1678\n",
            "Epoch 00001: val_loss improved from inf to 1.21990, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1678 - val_loss: 1.2199\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0787\n",
            "Epoch 00002: val_loss improved from 1.21990 to 0.93142, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0787 - val_loss: 0.9314\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00003: val_loss improved from 0.93142 to 0.92947, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0435 - val_loss: 0.9295\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0348\n",
            "Epoch 00004: val_loss did not improve from 0.92947\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0348 - val_loss: 0.9334\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00005: val_loss improved from 0.92947 to 0.73972, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0291 - val_loss: 0.7397\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0249\n",
            "Epoch 00006: val_loss did not improve from 0.73972\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0249 - val_loss: 0.7901\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00007: val_loss improved from 0.73972 to 0.62694, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0217 - val_loss: 0.6269\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00008: val_loss improved from 0.62694 to 0.59507, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0186 - val_loss: 0.5951\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00009: val_loss did not improve from 0.59507\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0168 - val_loss: 0.6192\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0169\n",
            "Epoch 00010: val_loss improved from 0.59507 to 0.56515, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0169 - val_loss: 0.5652\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5651504370122175\n",
            "Total reward :  19.300000000000004\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.3, 50, 0.4, 80, 0.4, 100, 0.5, 100, 0.3, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 193: Controller loss : 50.863174\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [5.7631280e-37 8.8955572e-20 6.1393546e-16 4.1856735e-22 2.2642331e-11\n",
            " 1.0000000e+00]\n",
            "trying these actions:  [array([[0.0000000e+00, 1.2508786e-23, 1.4401729e-19, 4.2915337e-26,\n",
            "        1.9418693e-14, 1.0000000e+00]], dtype=float32), array([[5.0900563e-18, 6.1589183e-04, 2.0702704e-05, 9.9936098e-01,\n",
            "        2.4199196e-06, 4.3794781e-09]], dtype=float32), array([[4.21460036e-17, 3.57783529e-12, 9.99999046e-01, 1.12294334e-10,\n",
            "        9.34983291e-07, 3.67025519e-23]], dtype=float32), array([[3.9812063e-11, 3.3154504e-04, 3.0918120e-05, 9.0000019e-10,\n",
            "        9.9963629e-01, 1.1549419e-06]], dtype=float32), array([[4.2470773e-33, 1.1387807e-11, 5.9968135e-11, 7.8626175e-07,\n",
            "        9.9999321e-01, 5.9378040e-06]], dtype=float32), array([[1.8481918e-31, 5.4219244e-11, 1.4409012e-15, 1.1944382e-10,\n",
            "        1.0000000e+00, 4.3705165e-13]], dtype=float32), array([[2.2357520e-36, 3.2016377e-29, 9.7938487e-28, 1.2740321e-09,\n",
            "        2.8102404e-11, 1.0000000e+00]], dtype=float32), array([[2.2237085e-13, 5.0462276e-15, 1.2809970e-15, 3.9506105e-09,\n",
            "        8.2762824e-11, 1.0000000e+00]], dtype=float32), array([[5.7584778e-19, 6.0846590e-23, 8.5022951e-28, 2.3987448e-32,\n",
            "        5.3475661e-11, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 2.8120337e-24, 2.7279734e-15, 9.9999988e-01,\n",
            "        7.4431156e-08, 6.7123532e-15]], dtype=float32), array([[5.3196692e-12, 6.8080334e-13, 8.0415284e-12, 7.4844824e-09,\n",
            "        4.5593492e-18, 1.0000000e+00]], dtype=float32), array([[2.2437767e-05, 3.1912439e-10, 9.9997735e-01, 2.4126502e-08,\n",
            "        2.2998954e-14, 2.5763299e-07]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 1.2508786e-23), (50, 1.4401729e-19), (60, 4.2915337e-26), (80, 1.9418693e-14), (100, 1.0)]\n",
            "dropout :  [(0, 5.0900563e-18), (0.1, 0.0006158918), (0.2, 2.0702704e-05), (0.3, 0.999361), (0.4, 2.4199196e-06), (0.5, 4.379478e-09)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 4.2146004e-17), (40, 3.5778353e-12), (50, 0.99999905), (60, 1.12294334e-10), (80, 9.349833e-07), (100, 3.6702552e-23)]\n",
            "dropout :  [(0, 3.9812063e-11), (0.1, 0.00033154504), (0.2, 3.091812e-05), (0.3, 9.000002e-10), (0.4, 0.9996363), (0.5, 1.1549419e-06)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 4.2470773e-33), (40, 1.1387807e-11), (50, 5.9968135e-11), (60, 7.8626175e-07), (80, 0.9999932), (100, 5.937804e-06)]\n",
            "dropout :  [(0, 1.8481918e-31), (0.1, 5.4219244e-11), (0.2, 1.4409012e-15), (0.3, 1.1944382e-10), (0.4, 1.0), (0.5, 4.3705165e-13)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 2.235752e-36), (40, 3.2016377e-29), (50, 9.793849e-28), (60, 1.2740321e-09), (80, 2.8102404e-11), (100, 1.0)]\n",
            "dropout :  [(0, 2.2237085e-13), (0.1, 5.0462276e-15), (0.2, 1.280997e-15), (0.3, 3.9506105e-09), (0.4, 8.2762824e-11), (0.5, 1.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 5.758478e-19), (40, 6.084659e-23), (50, 8.502295e-28), (60, 2.3987448e-32), (80, 5.347566e-11), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.8120337e-24), (0.2, 2.7279734e-15), (0.3, 0.9999999), (0.4, 7.4431156e-08), (0.5, 6.7123532e-15)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 5.319669e-12), (40, 6.8080334e-13), (50, 8.041528e-12), (60, 7.484482e-09), (80, 4.5593492e-18), (100, 1.0)]\n",
            "dropout :  [(0, 2.2437767e-05), (0.1, 3.191244e-10), (0.2, 0.99997735), (0.3, 2.4126502e-08), (0.4, 2.2998954e-14), (0.5, 2.57633e-07)]\n",
            "\n",
            "Predicted actions :  [100, 0.3, 50, 0.4, 80, 0.4, 100, 0.5, 100, 0.3, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1491\n",
            "Epoch 00001: val_loss improved from inf to 1.09092, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1491 - val_loss: 1.0909\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0439\n",
            "Epoch 00002: val_loss improved from 1.09092 to 0.81165, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0439 - val_loss: 0.8117\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0317\n",
            "Epoch 00003: val_loss did not improve from 0.81165\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0317 - val_loss: 0.9170\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0214\n",
            "Epoch 00004: val_loss improved from 0.81165 to 0.72748, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0214 - val_loss: 0.7275\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00005: val_loss improved from 0.72748 to 0.61066, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0181 - val_loss: 0.6107\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00006: val_loss did not improve from 0.61066\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0175 - val_loss: 0.6490\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0166\n",
            "Epoch 00007: val_loss did not improve from 0.61066\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0166 - val_loss: 0.6668\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0162\n",
            "Epoch 00008: val_loss did not improve from 0.61066\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0162 - val_loss: 0.6783\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0159\n",
            "Epoch 00009: val_loss did not improve from 0.61066\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0159 - val_loss: 0.6895\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00010: val_loss did not improve from 0.61066\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0164 - val_loss: 0.6538\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6106550485739781\n",
            "Total reward :  19.400000000000006\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.3, 50, 0.4, 80, 0.4, 100, 0.5, 100, 0.3, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 194: Controller loss : 48.265411\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.4, 20, 0.4, 100, 0.2, 60, 0.4, 40, 0, 100, 0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1464\n",
            "Epoch 00001: val_loss improved from inf to 1.07758, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1464 - val_loss: 1.0776\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0482\n",
            "Epoch 00002: val_loss improved from 1.07758 to 0.85635, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0482 - val_loss: 0.8564\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0373\n",
            "Epoch 00003: val_loss did not improve from 0.85635\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0373 - val_loss: 0.9977\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0291\n",
            "Epoch 00004: val_loss improved from 0.85635 to 0.79793, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0291 - val_loss: 0.7979\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0258\n",
            "Epoch 00005: val_loss did not improve from 0.79793\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0258 - val_loss: 0.8280\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00006: val_loss improved from 0.79793 to 0.68803, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0195 - val_loss: 0.6880\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00007: val_loss improved from 0.68803 to 0.67364, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0173 - val_loss: 0.6736\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00008: val_loss did not improve from 0.67364\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.7000\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0166\n",
            "Epoch 00009: val_loss did not improve from 0.67364\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0166 - val_loss: 0.7486\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0155\n",
            "Epoch 00010: val_loss did not improve from 0.67364\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0155 - val_loss: 0.7416\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.673639029978667\n",
            "Total reward :  19.500000000000007\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.4, 20, 0.4, 100, 0.2, 60, 0.4, 40, 0, 100, 0]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 195: Controller loss : 1050.526733\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.4, 40, 0, 60, 0.1, 100, 0.2, 60, 0.2, 60, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1982\n",
            "Epoch 00001: val_loss improved from inf to 0.92081, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1982 - val_loss: 0.9208\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0648\n",
            "Epoch 00002: val_loss did not improve from 0.92081\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0648 - val_loss: 1.1773\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0425\n",
            "Epoch 00003: val_loss improved from 0.92081 to 0.87419, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0425 - val_loss: 0.8742\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0357\n",
            "Epoch 00004: val_loss did not improve from 0.87419\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0357 - val_loss: 1.0431\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0315\n",
            "Epoch 00005: val_loss improved from 0.87419 to 0.85052, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0315 - val_loss: 0.8505\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00006: val_loss improved from 0.85052 to 0.84760, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0248 - val_loss: 0.8476\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0213\n",
            "Epoch 00007: val_loss improved from 0.84760 to 0.71359, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0213 - val_loss: 0.7136\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00008: val_loss improved from 0.71359 to 0.70697, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0188 - val_loss: 0.7070\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00009: val_loss improved from 0.70697 to 0.65741, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.6574\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00010: val_loss did not improve from 0.65741\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.6928\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6574129466137855\n",
            "Total reward :  19.60000000000001\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.4, 40, 0, 60, 0.1, 100, 0.2, 60, 0.2, 60, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 196: Controller loss : 1052.266113\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.5, 50, 0.5, 20, 0.1, 20, 0.1, 80, 0.4, 40, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2051\n",
            "Epoch 00001: val_loss improved from inf to 0.83927, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2051 - val_loss: 0.8393\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0621\n",
            "Epoch 00002: val_loss did not improve from 0.83927\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0621 - val_loss: 1.2921\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0535\n",
            "Epoch 00003: val_loss did not improve from 0.83927\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0535 - val_loss: 1.0183\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0377\n",
            "Epoch 00004: val_loss did not improve from 0.83927\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0377 - val_loss: 0.9901\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0289\n",
            "Epoch 00005: val_loss did not improve from 0.83927\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0289 - val_loss: 0.9868\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0235\n",
            "Epoch 00006: val_loss improved from 0.83927 to 0.80493, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0235 - val_loss: 0.8049\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00007: val_loss improved from 0.80493 to 0.80344, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0191 - val_loss: 0.8034\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0192\n",
            "Epoch 00008: val_loss improved from 0.80344 to 0.76246, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0192 - val_loss: 0.7625\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00009: val_loss did not improve from 0.76246\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.7725\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00010: val_loss did not improve from 0.76246\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0179 - val_loss: 0.8226\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7624617822809157\n",
            "Total reward :  19.70000000000001\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.5, 50, 0.5, 20, 0.1, 20, 0.1, 80, 0.4, 40, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 197: Controller loss : 959.433105\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [50, 0.3, 50, 0, 20, 0, 20, 0.3, 60, 0.4, 80, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2113\n",
            "Epoch 00001: val_loss improved from inf to 0.75221, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2113 - val_loss: 0.7522\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0710\n",
            "Epoch 00002: val_loss did not improve from 0.75221\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0710 - val_loss: 1.3224\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0579\n",
            "Epoch 00003: val_loss did not improve from 0.75221\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0579 - val_loss: 1.0345\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0394\n",
            "Epoch 00004: val_loss did not improve from 0.75221\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0394 - val_loss: 0.9494\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0312\n",
            "Epoch 00005: val_loss did not improve from 0.75221\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0312 - val_loss: 1.0204\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0261\n",
            "Epoch 00006: val_loss did not improve from 0.75221\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0261 - val_loss: 0.7829\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00007: val_loss did not improve from 0.75221\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0208 - val_loss: 0.8265\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00008: val_loss improved from 0.75221 to 0.69852, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0206 - val_loss: 0.6985\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00009: val_loss did not improve from 0.69852\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.7424\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00010: val_loss did not improve from 0.69852\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.7191\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6985200942211941\n",
            "Total reward :  19.80000000000001\n",
            "State input to Controller for training :  [0 0 3 0 0 0]\n",
            "Training RNN (States ip) :  [50, 0.3, 50, 0, 20, 0, 20, 0.3, 60, 0.4, 80, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 198: Controller loss : 1159.768921\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "\n",
            "Predicted actions :  [40, 0.1, 100, 0.5, 60, 0.4, 60, 0.3, 20, 0.2, 50, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1956\n",
            "Epoch 00001: val_loss improved from inf to 0.93560, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.1956 - val_loss: 0.9356\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0594\n",
            "Epoch 00002: val_loss did not improve from 0.93560\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0594 - val_loss: 1.2916\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0489\n",
            "Epoch 00003: val_loss did not improve from 0.93560\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0489 - val_loss: 1.0108\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0354\n",
            "Epoch 00004: val_loss did not improve from 0.93560\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0354 - val_loss: 1.0033\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0263\n",
            "Epoch 00005: val_loss improved from 0.93560 to 0.79400, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0263 - val_loss: 0.7940\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0214\n",
            "Epoch 00006: val_loss improved from 0.79400 to 0.75874, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0214 - val_loss: 0.7587\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00007: val_loss improved from 0.75874 to 0.75193, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0191 - val_loss: 0.7519\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0196\n",
            "Epoch 00008: val_loss improved from 0.75193 to 0.73663, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0196 - val_loss: 0.7366\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00009: val_loss did not improve from 0.73663\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0179 - val_loss: 0.7700\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00010: val_loss did not improve from 0.73663\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.7819\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7366257690396444\n",
            "Total reward :  19.900000000000013\n",
            "State input to Controller for training :  [0 2 0 0 0 0]\n",
            "Training RNN (States ip) :  [40, 0.1, 100, 0.5, 60, 0.4, 60, 0.3, 20, 0.2, 50, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 199: Controller loss : 1037.686768\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 50, 0.3, 80, 0.4, 80, 0.3, 80, 0.4, 80, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1672\n",
            "Epoch 00001: val_loss improved from inf to 0.97278, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1672 - val_loss: 0.9728\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0722\n",
            "Epoch 00002: val_loss did not improve from 0.97278\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0722 - val_loss: 1.0234\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0403\n",
            "Epoch 00003: val_loss improved from 0.97278 to 0.84479, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0403 - val_loss: 0.8448\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0336\n",
            "Epoch 00004: val_loss did not improve from 0.84479\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0336 - val_loss: 0.9707\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0295\n",
            "Epoch 00005: val_loss improved from 0.84479 to 0.72721, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0295 - val_loss: 0.7272\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00006: val_loss did not improve from 0.72721\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0241 - val_loss: 0.7773\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00007: val_loss improved from 0.72721 to 0.61506, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0198 - val_loss: 0.6151\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00008: val_loss improved from 0.61506 to 0.60440, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0184 - val_loss: 0.6044\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00009: val_loss improved from 0.60440 to 0.60433, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0164 - val_loss: 0.6043\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0163\n",
            "Epoch 00010: val_loss did not improve from 0.60433\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0163 - val_loss: 0.6151\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6043284419834744\n",
            "Total reward :  20.000000000000014\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.2, 50, 0.3, 80, 0.4, 80, 0.3, 80, 0.4, 80, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 200: Controller loss : 799.164062\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0, 80, 0.3, 60, 0, 20, 0.4, 60, 0.2, 50, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2008\n",
            "Epoch 00001: val_loss improved from inf to 0.80452, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2008 - val_loss: 0.8045\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00002: val_loss did not improve from 0.80452\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0595 - val_loss: 1.2773\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0484\n",
            "Epoch 00003: val_loss did not improve from 0.80452\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0484 - val_loss: 0.9514\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0359\n",
            "Epoch 00004: val_loss did not improve from 0.80452\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0359 - val_loss: 1.0051\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0295\n",
            "Epoch 00005: val_loss did not improve from 0.80452\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0295 - val_loss: 0.9222\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0222\n",
            "Epoch 00006: val_loss did not improve from 0.80452\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0222 - val_loss: 0.8203\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00007: val_loss improved from 0.80452 to 0.76039, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0200 - val_loss: 0.7604\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00008: val_loss did not improve from 0.76039\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.7817\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0192\n",
            "Epoch 00009: val_loss improved from 0.76039 to 0.75392, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0192 - val_loss: 0.7539\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00010: val_loss did not improve from 0.75392\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.7824\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7539188357999382\n",
            "Total reward :  20.100000000000016\n",
            "Saved buffers to file `buffers.txt` !\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0, 80, 0.3, 60, 0, 20, 0.4, 60, 0.2, 50, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 201: Controller loss : 1003.913025\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.2, 20, 0, 60, 0.5, 100, 0.4, 50, 0.5, 40, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1889\n",
            "Epoch 00001: val_loss improved from inf to 0.74974, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1889 - val_loss: 0.7497\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0573\n",
            "Epoch 00002: val_loss did not improve from 0.74974\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0573 - val_loss: 1.2162\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0473\n",
            "Epoch 00003: val_loss did not improve from 0.74974\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0473 - val_loss: 0.8319\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0346\n",
            "Epoch 00004: val_loss did not improve from 0.74974\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0346 - val_loss: 1.0030\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0285\n",
            "Epoch 00005: val_loss did not improve from 0.74974\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0285 - val_loss: 0.7553\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0220\n",
            "Epoch 00006: val_loss did not improve from 0.74974\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0220 - val_loss: 0.7844\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0201\n",
            "Epoch 00007: val_loss improved from 0.74974 to 0.60949, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0201 - val_loss: 0.6095\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0201\n",
            "Epoch 00008: val_loss did not improve from 0.60949\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0201 - val_loss: 0.6289\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00009: val_loss did not improve from 0.60949\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0186 - val_loss: 0.6924\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00010: val_loss did not improve from 0.60949\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0178 - val_loss: 0.6625\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6094855425923997\n",
            "Total reward :  20.200000000000017\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.2, 20, 0, 60, 0.5, 100, 0.4, 50, 0.5, 40, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 202: Controller loss : 1042.134277\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.1, 40, 0.2, 20, 0.4, 60, 0.1, 60, 0.5, 40, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1923\n",
            "Epoch 00001: val_loss improved from inf to 0.78433, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1923 - val_loss: 0.7843\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0593\n",
            "Epoch 00002: val_loss did not improve from 0.78433\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0593 - val_loss: 1.2506\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0475\n",
            "Epoch 00003: val_loss did not improve from 0.78433\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0475 - val_loss: 0.9349\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0355\n",
            "Epoch 00004: val_loss did not improve from 0.78433\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0355 - val_loss: 0.9774\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0281\n",
            "Epoch 00005: val_loss did not improve from 0.78433\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0281 - val_loss: 0.8926\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0221\n",
            "Epoch 00006: val_loss did not improve from 0.78433\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0221 - val_loss: 0.7878\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00007: val_loss improved from 0.78433 to 0.71959, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.7196\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00008: val_loss did not improve from 0.71959\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0182 - val_loss: 0.7566\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00009: val_loss did not improve from 0.71959\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0179 - val_loss: 0.7305\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00010: val_loss did not improve from 0.71959\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0182 - val_loss: 0.7668\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7195933394961886\n",
            "Total reward :  20.30000000000002\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.1, 40, 0.2, 20, 0.4, 60, 0.1, 60, 0.5, 40, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 203: Controller loss : 731.855530\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [0. 0. 0. 0. 0. 6.]\n",
            "trying these actions:  [array([[0.0000000e+00, 2.0477580e-26, 1.7972930e-21, 6.0233165e-32,\n",
            "        7.4998113e-14, 1.0000000e+00]], dtype=float32), array([[2.3227888e-17, 7.8471732e-01, 2.0131695e-01, 8.0465665e-03,\n",
            "        5.9185177e-03, 6.6141661e-07]], dtype=float32), array([[3.3913148e-12, 1.0897785e-06, 9.9965096e-01, 5.4405994e-12,\n",
            "        3.4794415e-04, 3.3315846e-21]], dtype=float32), array([[4.0646095e-02, 7.0991511e-05, 5.5512555e-02, 1.0001484e-04,\n",
            "        4.8100638e-01, 4.2266402e-01]], dtype=float32), array([[1.8486381e-27, 2.6620854e-15, 1.8078321e-14, 9.9474490e-01,\n",
            "        5.2527813e-03, 2.3002970e-06]], dtype=float32), array([[3.4149338e-28, 2.4697027e-09, 4.3640959e-16, 4.9163954e-14,\n",
            "        1.0000000e+00, 8.7647939e-13]], dtype=float32), array([[7.6565194e-30, 2.1646495e-32, 5.4085487e-31, 3.2356169e-04,\n",
            "        9.7775898e-11, 9.9967647e-01]], dtype=float32), array([[1.7964205e-13, 8.3605961e-08, 8.9192299e-12, 2.9810390e-01,\n",
            "        4.1487527e-01, 2.8702074e-01]], dtype=float32), array([[1.8181742e-13, 2.7910546e-17, 4.2227223e-21, 7.4906176e-18,\n",
            "        4.2749036e-02, 9.5725089e-01]], dtype=float32), array([[0.0000000e+00, 6.0466094e-24, 2.4082635e-06, 7.1221717e-02,\n",
            "        9.2877579e-01, 6.6156701e-08]], dtype=float32), array([[1.3554365e-12, 8.1984429e-03, 7.4277763e-05, 2.1205801e-06,\n",
            "        1.3084483e-11, 9.9172509e-01]], dtype=float32), array([[4.1284926e-02, 3.2968321e-05, 2.1069804e-01, 3.2329240e-01,\n",
            "        4.1742824e-06, 4.2468756e-01]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.047758e-26), (50, 1.797293e-21), (60, 6.0233165e-32), (80, 7.499811e-14), (100, 1.0)]\n",
            "dropout :  [(0, 2.3227888e-17), (0.1, 0.7847173), (0.2, 0.20131695), (0.3, 0.0080465665), (0.4, 0.0059185177), (0.5, 6.614166e-07)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 3.3913148e-12), (40, 1.0897785e-06), (50, 0.99965096), (60, 5.4405994e-12), (80, 0.00034794415), (100, 3.3315846e-21)]\n",
            "dropout :  [(0, 0.040646095), (0.1, 7.099151e-05), (0.2, 0.055512555), (0.3, 0.00010001484), (0.4, 0.48100638), (0.5, 0.42266402)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.8486381e-27), (40, 2.6620854e-15), (50, 1.807832e-14), (60, 0.9947449), (80, 0.0052527813), (100, 2.300297e-06)]\n",
            "dropout :  [(0, 3.4149338e-28), (0.1, 2.4697027e-09), (0.2, 4.3640959e-16), (0.3, 4.9163954e-14), (0.4, 1.0), (0.5, 8.764794e-13)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 7.6565194e-30), (40, 2.1646495e-32), (50, 5.4085487e-31), (60, 0.0003235617), (80, 9.77759e-11), (100, 0.99967647)]\n",
            "dropout :  [(0, 1.7964205e-13), (0.1, 8.360596e-08), (0.2, 8.91923e-12), (0.3, 0.2981039), (0.4, 0.41487527), (0.5, 0.28702074)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.8181742e-13), (40, 2.7910546e-17), (50, 4.2227223e-21), (60, 7.4906176e-18), (80, 0.042749036), (100, 0.9572509)]\n",
            "dropout :  [(0, 0.0), (0.1, 6.0466094e-24), (0.2, 2.4082635e-06), (0.3, 0.07122172), (0.4, 0.9287758), (0.5, 6.61567e-08)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.3554365e-12), (40, 0.008198443), (50, 7.427776e-05), (60, 2.1205801e-06), (80, 1.3084483e-11), (100, 0.9917251)]\n",
            "dropout :  [(0, 0.041284926), (0.1, 3.296832e-05), (0.2, 0.21069804), (0.3, 0.3232924), (0.4, 4.1742824e-06), (0.5, 0.42468756)]\n",
            "\n",
            "Predicted actions :  [100, 0.1, 50, 0.4, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2332\n",
            "Epoch 00001: val_loss improved from inf to 0.93088, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.2332 - val_loss: 0.9309\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1038\n",
            "Epoch 00002: val_loss did not improve from 0.93088\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.1038 - val_loss: 1.4467\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0709\n",
            "Epoch 00003: val_loss improved from 0.93088 to 0.80245, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0709 - val_loss: 0.8025\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0494\n",
            "Epoch 00004: val_loss did not improve from 0.80245\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0494 - val_loss: 1.0857\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0450\n",
            "Epoch 00005: val_loss did not improve from 0.80245\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0450 - val_loss: 0.9825\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0357\n",
            "Epoch 00006: val_loss improved from 0.80245 to 0.79039, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0357 - val_loss: 0.7904\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0283\n",
            "Epoch 00007: val_loss did not improve from 0.79039\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0283 - val_loss: 0.8793\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00008: val_loss improved from 0.79039 to 0.64526, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0241 - val_loss: 0.6453\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0203\n",
            "Epoch 00009: val_loss did not improve from 0.64526\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0203 - val_loss: 0.6764\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00010: val_loss improved from 0.64526 to 0.58422, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0206 - val_loss: 0.5842\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5842229223978546\n",
            "Total reward :  20.40000000000002\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.1, 50, 0.4, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 204: Controller loss : 68.257736\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [60, 0.2, 50, 0.5, 80, 0.3, 20, 0.4, 80, 0.3, 80, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1737\n",
            "Epoch 00001: val_loss improved from inf to 0.89410, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1737 - val_loss: 0.8941\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0538\n",
            "Epoch 00002: val_loss did not improve from 0.89410\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0538 - val_loss: 1.0656\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0393\n",
            "Epoch 00003: val_loss improved from 0.89410 to 0.85014, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0393 - val_loss: 0.8501\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0320\n",
            "Epoch 00004: val_loss did not improve from 0.85014\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0320 - val_loss: 0.9972\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0287\n",
            "Epoch 00005: val_loss improved from 0.85014 to 0.74934, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0287 - val_loss: 0.7493\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0228\n",
            "Epoch 00006: val_loss did not improve from 0.74934\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0228 - val_loss: 0.7938\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0197\n",
            "Epoch 00007: val_loss improved from 0.74934 to 0.64839, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0197 - val_loss: 0.6484\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00008: val_loss did not improve from 0.64839\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0187 - val_loss: 0.6632\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00009: val_loss did not improve from 0.64839\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0187 - val_loss: 0.6565\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00010: val_loss did not improve from 0.64839\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.6633\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6483932575109478\n",
            "Total reward :  20.50000000000002\n",
            "State input to Controller for training :  [0 0 0 4 0 0]\n",
            "Training RNN (States ip) :  [60, 0.2, 50, 0.5, 80, 0.3, 20, 0.4, 80, 0.3, 80, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 205: Controller loss : 944.123413\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [40, 0.2, 100, 0.3, 100, 0.3, 80, 0, 20, 0.4, 100, 0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1851\n",
            "Epoch 00001: val_loss improved from inf to 0.96257, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1851 - val_loss: 0.9626\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0732\n",
            "Epoch 00002: val_loss did not improve from 0.96257\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0732 - val_loss: 1.2246\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00003: val_loss improved from 0.96257 to 0.86534, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0435 - val_loss: 0.8653\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0346\n",
            "Epoch 00004: val_loss did not improve from 0.86534\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0346 - val_loss: 1.0846\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00005: val_loss did not improve from 0.86534\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0296 - val_loss: 0.8810\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0245\n",
            "Epoch 00006: val_loss improved from 0.86534 to 0.86391, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0245 - val_loss: 0.8639\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0197\n",
            "Epoch 00007: val_loss improved from 0.86391 to 0.77859, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0197 - val_loss: 0.7786\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00008: val_loss improved from 0.77859 to 0.75566, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0183 - val_loss: 0.7557\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00009: val_loss improved from 0.75566 to 0.68285, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0165 - val_loss: 0.6828\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0163\n",
            "Epoch 00010: val_loss did not improve from 0.68285\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0163 - val_loss: 0.7348\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6828481351108593\n",
            "Total reward :  20.600000000000023\n",
            "State input to Controller for training :  [0 2 0 0 0 0]\n",
            "Training RNN (States ip) :  [40, 0.2, 100, 0.3, 100, 0.3, 80, 0, 20, 0.4, 100, 0]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 206: Controller loss : 996.761597\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 80, 0.5, 100, 0.4, 80, 0.5, 40, 0.2, 40, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1855\n",
            "Epoch 00001: val_loss improved from inf to 0.94254, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1855 - val_loss: 0.9425\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0621\n",
            "Epoch 00002: val_loss did not improve from 0.94254\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0621 - val_loss: 1.2470\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00003: val_loss did not improve from 0.94254\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0460 - val_loss: 0.9725\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0320\n",
            "Epoch 00004: val_loss did not improve from 0.94254\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0320 - val_loss: 1.0362\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0264\n",
            "Epoch 00005: val_loss improved from 0.94254 to 0.81379, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0264 - val_loss: 0.8138\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00006: val_loss improved from 0.81379 to 0.80832, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0217 - val_loss: 0.8083\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00007: val_loss improved from 0.80832 to 0.77213, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0191 - val_loss: 0.7721\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00008: val_loss did not improve from 0.77213\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.7799\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00009: val_loss did not improve from 0.77213\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0164 - val_loss: 0.7749\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00010: val_loss improved from 0.77213 to 0.75841, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0172 - val_loss: 0.7584\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7584149728413501\n",
            "Total reward :  20.700000000000024\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.5, 80, 0.5, 100, 0.4, 80, 0.5, 40, 0.2, 40, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 207: Controller loss : 637.773071\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [60, 0.4, 60, 0.1, 60, 0.3, 80, 0, 80, 0, 80, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1915\n",
            "Epoch 00001: val_loss improved from inf to 0.89289, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1915 - val_loss: 0.8929\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0681\n",
            "Epoch 00002: val_loss did not improve from 0.89289\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0681 - val_loss: 1.1943\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0459\n",
            "Epoch 00003: val_loss improved from 0.89289 to 0.80885, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0459 - val_loss: 0.8089\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0361\n",
            "Epoch 00004: val_loss did not improve from 0.80885\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0361 - val_loss: 1.0357\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00005: val_loss did not improve from 0.80885\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0321 - val_loss: 0.8321\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0255\n",
            "Epoch 00006: val_loss did not improve from 0.80885\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0255 - val_loss: 0.8156\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00007: val_loss improved from 0.80885 to 0.71508, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0202 - val_loss: 0.7151\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00008: val_loss improved from 0.71508 to 0.67499, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0170 - val_loss: 0.6750\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00009: val_loss improved from 0.67499 to 0.64866, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0177 - val_loss: 0.6487\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00010: val_loss did not improve from 0.64866\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0173 - val_loss: 0.6748\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6486613698514718\n",
            "Total reward :  20.800000000000026\n",
            "State input to Controller for training :  [0 0 0 4 0 0]\n",
            "Training RNN (States ip) :  [60, 0.4, 60, 0.1, 60, 0.3, 80, 0, 80, 0, 80, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 208: Controller loss : 1073.912720\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [50, 0.5, 50, 0, 40, 0.2, 100, 0.4, 60, 0.4, 100, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2051\n",
            "Epoch 00001: val_loss improved from inf to 0.95532, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.2051 - val_loss: 0.9553\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0783\n",
            "Epoch 00002: val_loss did not improve from 0.95532\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0783 - val_loss: 1.3241\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0467\n",
            "Epoch 00003: val_loss improved from 0.95532 to 0.79073, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0467 - val_loss: 0.7907\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0432\n",
            "Epoch 00004: val_loss did not improve from 0.79073\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0432 - val_loss: 1.0483\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0387\n",
            "Epoch 00005: val_loss did not improve from 0.79073\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0387 - val_loss: 0.9704\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0300\n",
            "Epoch 00006: val_loss improved from 0.79073 to 0.78183, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0300 - val_loss: 0.7818\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0255\n",
            "Epoch 00007: val_loss did not improve from 0.78183\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0255 - val_loss: 0.8355\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00008: val_loss improved from 0.78183 to 0.63979, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0199 - val_loss: 0.6398\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00009: val_loss did not improve from 0.63979\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0189 - val_loss: 0.6617\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00010: val_loss improved from 0.63979 to 0.59515, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0173 - val_loss: 0.5951\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.595146012981473\n",
            "Total reward :  20.900000000000027\n",
            "State input to Controller for training :  [0 0 3 0 0 0]\n",
            "Training RNN (States ip) :  [50, 0.5, 50, 0, 40, 0.2, 100, 0.4, 60, 0.4, 100, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 209: Controller loss : 735.655762\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.2, 40, 0.1, 40, 0.2, 50, 0, 100, 0.4, 40, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1947\n",
            "Epoch 00001: val_loss improved from inf to 0.80243, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1947 - val_loss: 0.8024\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0516\n",
            "Epoch 00002: val_loss did not improve from 0.80243\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0516 - val_loss: 1.2173\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0394\n",
            "Epoch 00003: val_loss did not improve from 0.80243\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0394 - val_loss: 0.9119\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0351\n",
            "Epoch 00004: val_loss did not improve from 0.80243\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0351 - val_loss: 1.0230\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0264\n",
            "Epoch 00005: val_loss did not improve from 0.80243\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0264 - val_loss: 0.8071\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00006: val_loss improved from 0.80243 to 0.79298, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0204 - val_loss: 0.7930\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00007: val_loss improved from 0.79298 to 0.75220, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.7522\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00008: val_loss improved from 0.75220 to 0.73319, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0165 - val_loss: 0.7332\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00009: val_loss did not improve from 0.73319\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0165 - val_loss: 0.7666\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0159\n",
            "Epoch 00010: val_loss did not improve from 0.73319\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0159 - val_loss: 0.7779\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7331941662790469\n",
            "Total reward :  21.00000000000003\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.2, 40, 0.1, 40, 0.2, 50, 0, 100, 0.4, 40, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 210: Controller loss : 793.926453\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [0. 0. 0. 0. 5. 0.]\n",
            "trying these actions:  [array([[0.0000000e+00, 3.2949441e-25, 1.1491875e-19, 9.3690267e-27,\n",
            "        1.8070288e-12, 1.0000000e+00]], dtype=float32), array([[9.4982646e-21, 5.6532532e-04, 9.9489576e-01, 4.8902757e-06,\n",
            "        2.7918152e-03, 1.7421484e-03]], dtype=float32), array([[5.8570106e-14, 3.8355600e-05, 9.8425573e-01, 3.1195670e-09,\n",
            "        1.5705923e-02, 1.2337351e-19]], dtype=float32), array([[6.6182837e-03, 5.8525499e-02, 1.6450205e-05, 9.4825173e-06,\n",
            "        2.0245016e-02, 9.1458523e-01]], dtype=float32), array([[6.1247786e-29, 1.8597942e-09, 8.0106383e-16, 8.6343485e-01,\n",
            "        4.7673374e-02, 8.8891797e-02]], dtype=float32), array([[8.8439185e-30, 7.6473390e-11, 6.7814768e-11, 1.0803569e-05,\n",
            "        9.9998915e-01, 3.4279725e-14]], dtype=float32), array([[2.6984093e-29, 1.1768813e-34, 6.9050493e-29, 2.4088945e-06,\n",
            "        3.0513402e-04, 9.9969256e-01]], dtype=float32), array([[1.9670227e-07, 4.1285721e-12, 4.6068703e-16, 1.0354820e-05,\n",
            "        9.9236888e-01, 7.6206573e-03]], dtype=float32), array([[1.5441773e-14, 7.5260226e-18, 3.1755798e-25, 3.8253226e-19,\n",
            "        3.1597748e-02, 9.6840221e-01]], dtype=float32), array([[0.0000000e+00, 3.2053439e-31, 1.1940351e-10, 1.8185241e-06,\n",
            "        9.9999821e-01, 5.6865299e-15]], dtype=float32), array([[3.0192374e-18, 1.4306230e-02, 4.8563592e-10, 1.4592836e-11,\n",
            "        5.0949543e-11, 9.8569381e-01]], dtype=float32), array([[1.0110737e-03, 1.2239169e-04, 1.6788432e-02, 2.4213564e-01,\n",
            "        6.5788245e-06, 7.3993582e-01]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 3.294944e-25), (50, 1.1491875e-19), (60, 9.369027e-27), (80, 1.8070288e-12), (100, 1.0)]\n",
            "dropout :  [(0, 9.4982646e-21), (0.1, 0.0005653253), (0.2, 0.99489576), (0.3, 4.8902757e-06), (0.4, 0.0027918152), (0.5, 0.0017421484)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 5.8570106e-14), (40, 3.83556e-05), (50, 0.98425573), (60, 3.119567e-09), (80, 0.015705923), (100, 1.2337351e-19)]\n",
            "dropout :  [(0, 0.0066182837), (0.1, 0.0585255), (0.2, 1.6450205e-05), (0.3, 9.482517e-06), (0.4, 0.020245016), (0.5, 0.91458523)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 6.1247786e-29), (40, 1.8597942e-09), (50, 8.0106383e-16), (60, 0.86343485), (80, 0.047673374), (100, 0.0888918)]\n",
            "dropout :  [(0, 8.8439185e-30), (0.1, 7.647339e-11), (0.2, 6.781477e-11), (0.3, 1.0803569e-05), (0.4, 0.99998915), (0.5, 3.4279725e-14)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 2.6984093e-29), (40, 1.1768813e-34), (50, 6.905049e-29), (60, 2.4088945e-06), (80, 0.00030513402), (100, 0.99969256)]\n",
            "dropout :  [(0, 1.9670227e-07), (0.1, 4.128572e-12), (0.2, 4.6068703e-16), (0.3, 1.035482e-05), (0.4, 0.9923689), (0.5, 0.0076206573)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.5441773e-14), (40, 7.5260226e-18), (50, 3.1755798e-25), (60, 3.8253226e-19), (80, 0.03159775), (100, 0.9684022)]\n",
            "dropout :  [(0, 0.0), (0.1, 3.2053439e-31), (0.2, 1.1940351e-10), (0.3, 1.8185241e-06), (0.4, 0.9999982), (0.5, 5.68653e-15)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 3.0192374e-18), (40, 0.01430623), (50, 4.856359e-10), (60, 1.4592836e-11), (80, 5.0949543e-11), (100, 0.9856938)]\n",
            "dropout :  [(0, 0.0010110737), (0.1, 0.00012239169), (0.2, 0.016788432), (0.3, 0.24213564), (0.4, 6.5788245e-06), (0.5, 0.7399358)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 50, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1892\n",
            "Epoch 00001: val_loss improved from inf to 0.95792, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1892 - val_loss: 0.9579\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0852\n",
            "Epoch 00002: val_loss did not improve from 0.95792\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0852 - val_loss: 1.2245\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0482\n",
            "Epoch 00003: val_loss improved from 0.95792 to 0.85797, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0482 - val_loss: 0.8580\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0429\n",
            "Epoch 00004: val_loss did not improve from 0.85797\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0429 - val_loss: 1.0839\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0368\n",
            "Epoch 00005: val_loss improved from 0.85797 to 0.81206, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0368 - val_loss: 0.8121\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0289\n",
            "Epoch 00006: val_loss did not improve from 0.81206\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0289 - val_loss: 0.8145\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0221\n",
            "Epoch 00007: val_loss improved from 0.81206 to 0.58317, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0221 - val_loss: 0.5832\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00008: val_loss improved from 0.58317 to 0.55631, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0190 - val_loss: 0.5563\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00009: val_loss did not improve from 0.55631\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0188 - val_loss: 0.5618\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00010: val_loss did not improve from 0.55631\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0186 - val_loss: 0.5655\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5563119216162654\n",
            "Total reward :  21.10000000000003\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.2, 50, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 211: Controller loss : 61.536358\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.5, 40, 0.5, 80, 0.3, 100, 0.1, 100, 0.4, 80, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2029\n",
            "Epoch 00001: val_loss improved from inf to 1.01131, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.2029 - val_loss: 1.0113\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0968\n",
            "Epoch 00002: val_loss did not improve from 1.01131\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0968 - val_loss: 1.4415\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0586\n",
            "Epoch 00003: val_loss improved from 1.01131 to 0.90373, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0586 - val_loss: 0.9037\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00004: val_loss did not improve from 0.90373\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0480 - val_loss: 1.1128\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0416\n",
            "Epoch 00005: val_loss did not improve from 0.90373\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0416 - val_loss: 1.0857\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0329\n",
            "Epoch 00006: val_loss improved from 0.90373 to 0.89648, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0329 - val_loss: 0.8965\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0269\n",
            "Epoch 00007: val_loss did not improve from 0.89648\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0269 - val_loss: 0.9590\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00008: val_loss improved from 0.89648 to 0.78459, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0241 - val_loss: 0.7846\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0218\n",
            "Epoch 00009: val_loss improved from 0.78459 to 0.78063, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0218 - val_loss: 0.7806\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00010: val_loss improved from 0.78063 to 0.68359, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0195 - val_loss: 0.6836\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6835940990572661\n",
            "Total reward :  21.20000000000003\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.5, 40, 0.5, 80, 0.3, 100, 0.1, 100, 0.4, 80, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 212: Controller loss : 592.488892\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [50, 0.2, 50, 0, 80, 0.3, 20, 0.5, 20, 0.1, 20, 0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2018\n",
            "Epoch 00001: val_loss improved from inf to 1.08820, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2018 - val_loss: 1.0882\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0726\n",
            "Epoch 00002: val_loss did not improve from 1.08820\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0726 - val_loss: 1.2846\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0622\n",
            "Epoch 00003: val_loss did not improve from 1.08820\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0622 - val_loss: 1.2668\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0402\n",
            "Epoch 00004: val_loss improved from 1.08820 to 1.02344, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0402 - val_loss: 1.0234\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0306\n",
            "Epoch 00005: val_loss improved from 1.02344 to 0.95558, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0306 - val_loss: 0.9556\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00006: val_loss improved from 0.95558 to 0.81568, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.8157\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00007: val_loss improved from 0.81568 to 0.77289, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.7729\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00008: val_loss did not improve from 0.77289\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0179 - val_loss: 0.8152\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0172\n",
            "Epoch 00009: val_loss did not improve from 0.77289\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0172 - val_loss: 0.7978\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0171\n",
            "Epoch 00010: val_loss did not improve from 0.77289\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0171 - val_loss: 0.7796\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7728927156244747\n",
            "Total reward :  21.300000000000033\n",
            "State input to Controller for training :  [0 0 3 0 0 0]\n",
            "Training RNN (States ip) :  [50, 0.2, 50, 0, 80, 0.3, 20, 0.5, 20, 0.1, 20, 0]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 213: Controller loss : 650.221069\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [0. 0. 3. 0. 0. 0.]\n",
            "trying these actions:  [array([[0.0000000e+00, 1.7045555e-26, 9.6544989e-18, 5.2535604e-28,\n",
            "        1.0199849e-13, 1.0000000e+00]], dtype=float32), array([[1.8005474e-23, 1.2829568e-06, 9.9788326e-01, 1.1652690e-08,\n",
            "        6.8379773e-06, 2.1086109e-03]], dtype=float32), array([[1.0692908e-16, 5.3232128e-05, 9.9991083e-01, 7.2175317e-12,\n",
            "        3.5841029e-05, 2.9808476e-22]], dtype=float32), array([[3.2658468e-04, 3.7841983e-06, 1.2903731e-09, 7.9819690e-10,\n",
            "        1.6345570e-06, 9.9966800e-01]], dtype=float32), array([[6.2784424e-33, 2.0891870e-13, 8.1830348e-20, 1.3710808e-03,\n",
            "        9.9861956e-01, 9.4377428e-06]], dtype=float32), array([[3.2000299e-31, 2.9316050e-12, 2.4595224e-12, 4.1025434e-02,\n",
            "        9.5897460e-01, 1.2970199e-15]], dtype=float32), array([[3.5236006e-30, 0.0000000e+00, 8.0188044e-33, 2.6315619e-10,\n",
            "        3.7576836e-08, 1.0000000e+00]], dtype=float32), array([[8.7163521e-09, 5.4314003e-10, 2.2777459e-17, 5.3232884e-07,\n",
            "        4.7135606e-01, 5.2864337e-01]], dtype=float32), array([[1.8010961e-15, 5.6473994e-22, 2.4265646e-29, 3.1977824e-23,\n",
            "        2.4932231e-06, 9.9999750e-01]], dtype=float32), array([[0.0000000e+00, 1.9116602e-30, 5.5118175e-14, 8.3900559e-10,\n",
            "        1.0000000e+00, 2.7177385e-18]], dtype=float32), array([[1.9177573e-15, 6.7207124e-04, 2.3499412e-11, 7.2650431e-13,\n",
            "        1.4204385e-09, 9.9932790e-01]], dtype=float32), array([[3.1025186e-02, 2.7146200e-06, 2.4284336e-01, 3.4076176e-03,\n",
            "        1.5712507e-07, 7.2272098e-01]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 1.7045555e-26), (50, 9.654499e-18), (60, 5.2535604e-28), (80, 1.0199849e-13), (100, 1.0)]\n",
            "dropout :  [(0, 1.8005474e-23), (0.1, 1.2829568e-06), (0.2, 0.99788326), (0.3, 1.165269e-08), (0.4, 6.8379773e-06), (0.5, 0.002108611)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0692908e-16), (40, 5.323213e-05), (50, 0.99991083), (60, 7.2175317e-12), (80, 3.584103e-05), (100, 2.9808476e-22)]\n",
            "dropout :  [(0, 0.00032658468), (0.1, 3.7841983e-06), (0.2, 1.2903731e-09), (0.3, 7.981969e-10), (0.4, 1.634557e-06), (0.5, 0.999668)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 6.2784424e-33), (40, 2.089187e-13), (50, 8.183035e-20), (60, 0.0013710808), (80, 0.99861956), (100, 9.437743e-06)]\n",
            "dropout :  [(0, 3.20003e-31), (0.1, 2.931605e-12), (0.2, 2.4595224e-12), (0.3, 0.041025434), (0.4, 0.9589746), (0.5, 1.2970199e-15)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 3.5236006e-30), (40, 0.0), (50, 8.0188044e-33), (60, 2.631562e-10), (80, 3.7576836e-08), (100, 1.0)]\n",
            "dropout :  [(0, 8.716352e-09), (0.1, 5.4314003e-10), (0.2, 2.2777459e-17), (0.3, 5.3232884e-07), (0.4, 0.47135606), (0.5, 0.52864337)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.8010961e-15), (40, 5.6473994e-22), (50, 2.4265646e-29), (60, 3.1977824e-23), (80, 2.493223e-06), (100, 0.9999975)]\n",
            "dropout :  [(0, 0.0), (0.1, 1.9116602e-30), (0.2, 5.5118175e-14), (0.3, 8.390056e-10), (0.4, 1.0), (0.5, 2.7177385e-18)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.9177573e-15), (40, 0.00067207124), (50, 2.3499412e-11), (60, 7.265043e-13), (80, 1.4204385e-09), (100, 0.9993279)]\n",
            "dropout :  [(0, 0.031025186), (0.1, 2.71462e-06), (0.2, 0.24284336), (0.3, 0.0034076176), (0.4, 1.5712507e-07), (0.5, 0.722721)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 50, 0.5, 80, 0.4, 100, 0.5, 100, 0.4, 100, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1991\n",
            "Epoch 00001: val_loss improved from inf to 1.31454, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1991 - val_loss: 1.3145\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1312\n",
            "Epoch 00002: val_loss did not improve from 1.31454\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.1312 - val_loss: 1.3220\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0568\n",
            "Epoch 00003: val_loss improved from 1.31454 to 0.85365, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0568 - val_loss: 0.8537\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00004: val_loss did not improve from 0.85365\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0480 - val_loss: 1.1328\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00005: val_loss did not improve from 0.85365\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0435 - val_loss: 0.9696\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00006: val_loss did not improve from 0.85365\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0331 - val_loss: 0.8591\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00007: val_loss improved from 0.85365 to 0.85343, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0254 - val_loss: 0.8534\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00008: val_loss improved from 0.85343 to 0.68471, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0209 - val_loss: 0.6847\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00009: val_loss improved from 0.68471 to 0.66862, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0189 - val_loss: 0.6686\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00010: val_loss improved from 0.66862 to 0.63655, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0181 - val_loss: 0.6366\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6365510222698868\n",
            "Total reward :  21.400000000000034\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.2, 50, 0.5, 80, 0.4, 100, 0.5, 100, 0.4, 100, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 214: Controller loss : 66.079926\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [0.0000000e+00 1.7045555e-26 9.6544989e-18 5.2535604e-28 1.0199849e-13\n",
            " 1.0000000e+00]\n",
            "trying these actions:  [array([[0.0000000e+00, 1.0654868e-28, 6.1890048e-20, 3.3831371e-30,\n",
            "        6.5846573e-16, 1.0000000e+00]], dtype=float32), array([[3.6889998e-25, 2.8132952e-08, 9.9995196e-01, 2.6014135e-10,\n",
            "        1.5454633e-07, 4.7940797e-05]], dtype=float32), array([[1.8902909e-18, 1.0206757e-06, 9.9999821e-01, 1.3926208e-13,\n",
            "        6.9690111e-07, 5.8442512e-24]], dtype=float32), array([[2.44227795e-06, 2.92050188e-08, 9.89016768e-12, 6.28197060e-12,\n",
            "        1.28535005e-08, 9.99997497e-01]], dtype=float32), array([[5.6670474e-35, 1.9525463e-15, 7.3796925e-22, 1.3080369e-05,\n",
            "        9.9998677e-01, 9.0420599e-08]], dtype=float32), array([[1.8676963e-33, 1.7802898e-14, 1.5739554e-14, 2.5071972e-04,\n",
            "        9.9974924e-01, 8.3188347e-18]], dtype=float32), array([[2.8065357e-32, 0.0000000e+00, 6.5881861e-35, 2.1727526e-12,\n",
            "        3.1422231e-10, 1.0000000e+00]], dtype=float32), array([[4.1795161e-11, 2.7033291e-12, 1.1228390e-19, 2.6860332e-09,\n",
            "        1.2824112e-03, 9.9871767e-01]], dtype=float32), array([[1.2530889e-17, 4.0327879e-24, 1.7423194e-31, 2.3733317e-25,\n",
            "        1.8497158e-08, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 2.4967559e-32, 7.2933343e-16, 1.1116958e-11,\n",
            "        1.0000000e+00, 3.6442175e-20]], dtype=float32), array([[1.0026193e-17, 3.7062812e-06, 1.2904398e-13, 4.0309925e-15,\n",
            "        8.0632463e-12, 9.9999630e-01]], dtype=float32), array([[1.1724082e-04, 1.3568846e-08, 6.5429613e-04, 1.7261946e-05,\n",
            "        8.0707668e-10, 9.9921119e-01]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 1.0654868e-28), (50, 6.189005e-20), (60, 3.383137e-30), (80, 6.5846573e-16), (100, 1.0)]\n",
            "dropout :  [(0, 3.6889998e-25), (0.1, 2.8132952e-08), (0.2, 0.99995196), (0.3, 2.6014135e-10), (0.4, 1.5454633e-07), (0.5, 4.7940797e-05)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.890291e-18), (40, 1.0206757e-06), (50, 0.9999982), (60, 1.3926208e-13), (80, 6.969011e-07), (100, 5.844251e-24)]\n",
            "dropout :  [(0, 2.442278e-06), (0.1, 2.9205019e-08), (0.2, 9.890168e-12), (0.3, 6.2819706e-12), (0.4, 1.28535005e-08), (0.5, 0.9999975)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 5.6670474e-35), (40, 1.9525463e-15), (50, 7.3796925e-22), (60, 1.3080369e-05), (80, 0.99998677), (100, 9.04206e-08)]\n",
            "dropout :  [(0, 1.8676963e-33), (0.1, 1.7802898e-14), (0.2, 1.5739554e-14), (0.3, 0.00025071972), (0.4, 0.99974924), (0.5, 8.318835e-18)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 2.8065357e-32), (40, 0.0), (50, 6.588186e-35), (60, 2.1727526e-12), (80, 3.1422231e-10), (100, 1.0)]\n",
            "dropout :  [(0, 4.179516e-11), (0.1, 2.703329e-12), (0.2, 1.122839e-19), (0.3, 2.6860332e-09), (0.4, 0.0012824112), (0.5, 0.99871767)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.2530889e-17), (40, 4.032788e-24), (50, 1.7423194e-31), (60, 2.3733317e-25), (80, 1.8497158e-08), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.496756e-32), (0.2, 7.2933343e-16), (0.3, 1.1116958e-11), (0.4, 1.0), (0.5, 3.6442175e-20)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.0026193e-17), (40, 3.7062812e-06), (50, 1.2904398e-13), (60, 4.0309925e-15), (80, 8.063246e-12), (100, 0.9999963)]\n",
            "dropout :  [(0, 0.00011724082), (0.1, 1.3568846e-08), (0.2, 0.00065429613), (0.3, 1.7261946e-05), (0.4, 8.070767e-10), (0.5, 0.9992112)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 50, 0.5, 80, 0.4, 100, 0.5, 100, 0.4, 100, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1435\n",
            "Epoch 00001: val_loss improved from inf to 1.13418, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1435 - val_loss: 1.1342\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0434\n",
            "Epoch 00002: val_loss improved from 1.13418 to 0.76537, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0434 - val_loss: 0.7654\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0284\n",
            "Epoch 00003: val_loss did not improve from 0.76537\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0284 - val_loss: 0.8299\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0230\n",
            "Epoch 00004: val_loss improved from 0.76537 to 0.71230, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0230 - val_loss: 0.7123\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0215\n",
            "Epoch 00005: val_loss improved from 0.71230 to 0.56885, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0215 - val_loss: 0.5688\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00006: val_loss improved from 0.56885 to 0.52587, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0194 - val_loss: 0.5259\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00007: val_loss did not improve from 0.52587\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0188 - val_loss: 0.5420\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00008: val_loss did not improve from 0.52587\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0187 - val_loss: 0.6277\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00009: val_loss did not improve from 0.52587\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0175 - val_loss: 0.6234\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0163\n",
            "Epoch 00010: val_loss did not improve from 0.52587\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0163 - val_loss: 0.5514\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5258749300098627\n",
            "Total reward :  21.500000000000036\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.2, 50, 0.5, 80, 0.4, 100, 0.5, 100, 0.4, 100, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 215: Controller loss : 60.802738\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0, 20, 0.5, 100, 0.1, 60, 0.5, 40, 0.1, 60, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1921\n",
            "Epoch 00001: val_loss improved from inf to 0.78309, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1921 - val_loss: 0.7831\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0593\n",
            "Epoch 00002: val_loss did not improve from 0.78309\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0593 - val_loss: 1.2923\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0425\n",
            "Epoch 00003: val_loss did not improve from 0.78309\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0425 - val_loss: 0.8970\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0389\n",
            "Epoch 00004: val_loss did not improve from 0.78309\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0389 - val_loss: 1.1017\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0328\n",
            "Epoch 00005: val_loss did not improve from 0.78309\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0328 - val_loss: 0.8603\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00006: val_loss did not improve from 0.78309\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0254 - val_loss: 0.8804\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0205\n",
            "Epoch 00007: val_loss improved from 0.78309 to 0.67147, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0205 - val_loss: 0.6715\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00008: val_loss did not improve from 0.67147\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0191 - val_loss: 0.7107\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00009: val_loss did not improve from 0.67147\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.7451\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00010: val_loss did not improve from 0.67147\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0175 - val_loss: 0.7190\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6714654789511155\n",
            "Total reward :  21.600000000000037\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0, 20, 0.5, 100, 0.1, 60, 0.5, 40, 0.1, 60, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 216: Controller loss : 1081.165771\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0, 100, 0.3, 40, 0.1, 80, 0.1, 100, 0.3, 60, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2100\n",
            "Epoch 00001: val_loss improved from inf to 0.76227, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.2100 - val_loss: 0.7623\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0748\n",
            "Epoch 00002: val_loss did not improve from 0.76227\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0748 - val_loss: 1.3583\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0555\n",
            "Epoch 00003: val_loss did not improve from 0.76227\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0555 - val_loss: 0.8528\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00004: val_loss did not improve from 0.76227\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0443 - val_loss: 1.1064\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0376\n",
            "Epoch 00005: val_loss did not improve from 0.76227\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0376 - val_loss: 0.8769\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0295\n",
            "Epoch 00006: val_loss did not improve from 0.76227\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0295 - val_loss: 0.8334\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0236\n",
            "Epoch 00007: val_loss improved from 0.76227 to 0.71044, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0236 - val_loss: 0.7104\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00008: val_loss improved from 0.71044 to 0.65635, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0188 - val_loss: 0.6563\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00009: val_loss improved from 0.65635 to 0.57148, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0189 - val_loss: 0.5715\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00010: val_loss did not improve from 0.57148\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.6314\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5714832318374534\n",
            "Total reward :  21.70000000000004\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0, 100, 0.3, 40, 0.1, 80, 0.1, 100, 0.3, 60, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 217: Controller loss : 1276.945435\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.1, 60, 0.3, 60, 0.4, 80, 0.1, 60, 0.4, 60, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2062\n",
            "Epoch 00001: val_loss improved from inf to 0.77012, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2062 - val_loss: 0.7701\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0505\n",
            "Epoch 00002: val_loss did not improve from 0.77012\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0505 - val_loss: 1.1256\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0402\n",
            "Epoch 00003: val_loss did not improve from 0.77012\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0402 - val_loss: 0.8740\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00004: val_loss did not improve from 0.77012\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0296 - val_loss: 0.9185\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0229\n",
            "Epoch 00005: val_loss improved from 0.77012 to 0.76132, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0229 - val_loss: 0.7613\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0208\n",
            "Epoch 00006: val_loss improved from 0.76132 to 0.67997, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0208 - val_loss: 0.6800\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00007: val_loss did not improve from 0.67997\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.6966\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00008: val_loss improved from 0.67997 to 0.67791, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.6779\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00009: val_loss did not improve from 0.67791\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0179 - val_loss: 0.7014\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00010: val_loss did not improve from 0.67791\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.7167\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6779125801618323\n",
            "Total reward :  21.80000000000004\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.1, 60, 0.3, 60, 0.4, 80, 0.1, 60, 0.4, 60, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 218: Controller loss : 1157.932983\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0, 20, 0, 20, 0.2, 60, 0.1, 80, 0, 50, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1912\n",
            "Epoch 00001: val_loss improved from inf to 0.74098, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.1912 - val_loss: 0.7410\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0493\n",
            "Epoch 00002: val_loss did not improve from 0.74098\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0493 - val_loss: 1.2332\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0408\n",
            "Epoch 00003: val_loss did not improve from 0.74098\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0408 - val_loss: 0.8128\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0356\n",
            "Epoch 00004: val_loss did not improve from 0.74098\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0356 - val_loss: 1.0197\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00005: val_loss improved from 0.74098 to 0.73789, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0265 - val_loss: 0.7379\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00006: val_loss did not improve from 0.73789\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0233 - val_loss: 0.7997\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0205\n",
            "Epoch 00007: val_loss improved from 0.73789 to 0.60170, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0205 - val_loss: 0.6017\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00008: val_loss improved from 0.60170 to 0.60022, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.6002\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00009: val_loss did not improve from 0.60022\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.6390\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00010: val_loss did not improve from 0.60022\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0170 - val_loss: 0.6494\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6002188559970565\n",
            "Total reward :  21.90000000000004\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0, 20, 0, 20, 0.2, 60, 0.1, 80, 0, 50, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 219: Controller loss : 918.135498\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [1. 0. 0. 0. 0. 0.]\n",
            "trying these actions:  [array([[6.5421486e-37, 5.2677055e-29, 3.5465702e-20, 1.9916681e-30,\n",
            "        7.8185608e-13, 1.0000000e+00]], dtype=float32), array([[6.5244129e-14, 5.1449281e-03, 9.9094802e-01, 1.8672999e-08,\n",
            "        1.1927259e-05, 3.8951689e-03]], dtype=float32), array([[7.2448064e-10, 8.0861013e-05, 9.9985921e-01, 6.4898991e-08,\n",
            "        5.9893991e-05, 1.6520119e-18]], dtype=float32), array([[3.84681334e-06, 6.16993887e-11, 1.97500165e-14, 1.30141276e-08,\n",
            "        2.96991563e-11, 9.99996185e-01]], dtype=float32), array([[3.8390515e-31, 4.6633188e-12, 8.3894900e-22, 1.5684607e-02,\n",
            "        9.8407549e-01, 2.3985034e-04]], dtype=float32), array([[5.3126189e-36, 5.7259653e-10, 1.8371975e-13, 1.7850386e-06,\n",
            "        9.9999821e-01, 4.9160339e-20]], dtype=float32), array([[1.2247414e-32, 0.0000000e+00, 3.4796409e-35, 4.5494508e-06,\n",
            "        6.9222573e-05, 9.9992621e-01]], dtype=float32), array([[3.7517912e-14, 6.3225451e-07, 1.1522689e-22, 3.2237301e-12,\n",
            "        1.7081099e-06, 9.9999774e-01]], dtype=float32), array([[1.5192980e-20, 2.3606745e-23, 2.5343885e-34, 6.3837178e-25,\n",
            "        7.4609680e-08, 9.9999988e-01]], dtype=float32), array([[0.0000000e+00, 3.1027992e-31, 3.6736721e-18, 2.1932235e-10,\n",
            "        1.0000000e+00, 1.9863648e-22]], dtype=float32), array([[3.7812366e-18, 1.9316617e-06, 6.3610722e-10, 8.1427816e-06,\n",
            "        4.9183387e-12, 9.9998987e-01]], dtype=float32), array([[2.8187030e-06, 1.0959656e-06, 2.0954747e-05, 9.4235820e-01,\n",
            "        7.7854722e-08, 5.7616822e-02]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 6.5421486e-37), (40, 5.2677055e-29), (50, 3.54657e-20), (60, 1.9916681e-30), (80, 7.818561e-13), (100, 1.0)]\n",
            "dropout :  [(0, 6.524413e-14), (0.1, 0.005144928), (0.2, 0.990948), (0.3, 1.8673e-08), (0.4, 1.1927259e-05), (0.5, 0.003895169)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 7.2448064e-10), (40, 8.086101e-05), (50, 0.9998592), (60, 6.489899e-08), (80, 5.989399e-05), (100, 1.6520119e-18)]\n",
            "dropout :  [(0, 3.8468133e-06), (0.1, 6.169939e-11), (0.2, 1.9750017e-14), (0.3, 1.30141276e-08), (0.4, 2.9699156e-11), (0.5, 0.9999962)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 3.8390515e-31), (40, 4.6633188e-12), (50, 8.38949e-22), (60, 0.015684607), (80, 0.9840755), (100, 0.00023985034)]\n",
            "dropout :  [(0, 5.312619e-36), (0.1, 5.725965e-10), (0.2, 1.8371975e-13), (0.3, 1.7850386e-06), (0.4, 0.9999982), (0.5, 4.916034e-20)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.2247414e-32), (40, 0.0), (50, 3.479641e-35), (60, 4.5494508e-06), (80, 6.922257e-05), (100, 0.9999262)]\n",
            "dropout :  [(0, 3.751791e-14), (0.1, 6.322545e-07), (0.2, 1.1522689e-22), (0.3, 3.22373e-12), (0.4, 1.70811e-06), (0.5, 0.99999774)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.519298e-20), (40, 2.3606745e-23), (50, 2.5343885e-34), (60, 6.383718e-25), (80, 7.460968e-08), (100, 0.9999999)]\n",
            "dropout :  [(0, 0.0), (0.1, 3.1027992e-31), (0.2, 3.673672e-18), (0.3, 2.1932235e-10), (0.4, 1.0), (0.5, 1.9863648e-22)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 3.7812366e-18), (40, 1.9316617e-06), (50, 6.361072e-10), (60, 8.142782e-06), (80, 4.9183387e-12), (100, 0.99998987)]\n",
            "dropout :  [(0, 2.818703e-06), (0.1, 1.0959656e-06), (0.2, 2.0954747e-05), (0.3, 0.9423582), (0.4, 7.785472e-08), (0.5, 0.057616822)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 50, 0.5, 80, 0.4, 100, 0.5, 100, 0.4, 100, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1750\n",
            "Epoch 00001: val_loss improved from inf to 1.29309, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1750 - val_loss: 1.2931\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1129\n",
            "Epoch 00002: val_loss improved from 1.29309 to 1.23319, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.1129 - val_loss: 1.2332\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0560\n",
            "Epoch 00003: val_loss improved from 1.23319 to 0.88738, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0560 - val_loss: 0.8874\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0421\n",
            "Epoch 00004: val_loss did not improve from 0.88738\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0421 - val_loss: 1.1342\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0392\n",
            "Epoch 00005: val_loss did not improve from 0.88738\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0392 - val_loss: 0.9059\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0319\n",
            "Epoch 00006: val_loss improved from 0.88738 to 0.84286, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0319 - val_loss: 0.8429\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0256\n",
            "Epoch 00007: val_loss improved from 0.84286 to 0.78461, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0256 - val_loss: 0.7846\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0206\n",
            "Epoch 00008: val_loss improved from 0.78461 to 0.65159, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0206 - val_loss: 0.6516\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00009: val_loss improved from 0.65159 to 0.60554, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0195 - val_loss: 0.6055\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00010: val_loss did not improve from 0.60554\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0193 - val_loss: 0.6356\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6055373620883052\n",
            "Total reward :  22.000000000000043\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.2, 50, 0.5, 80, 0.4, 100, 0.5, 100, 0.4, 100, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 220: Controller loss : 64.262543\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0, 80, 0.3, 50, 0.3, 20, 0.4, 100, 0.5, 80, 0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2000\n",
            "Epoch 00001: val_loss improved from inf to 0.76151, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.2000 - val_loss: 0.7615\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0597\n",
            "Epoch 00002: val_loss did not improve from 0.76151\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0597 - val_loss: 1.2537\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0427\n",
            "Epoch 00003: val_loss did not improve from 0.76151\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0427 - val_loss: 0.8584\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0367\n",
            "Epoch 00004: val_loss did not improve from 0.76151\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0367 - val_loss: 1.0385\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0280\n",
            "Epoch 00005: val_loss did not improve from 0.76151\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0280 - val_loss: 0.7906\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0213\n",
            "Epoch 00006: val_loss did not improve from 0.76151\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0213 - val_loss: 0.7864\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00007: val_loss improved from 0.76151 to 0.60684, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0195 - val_loss: 0.6068\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00008: val_loss did not improve from 0.60684\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.6526\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00009: val_loss did not improve from 0.60684\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0168 - val_loss: 0.6687\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00010: val_loss did not improve from 0.60684\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0173 - val_loss: 0.6665\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6068423641013684\n",
            "Total reward :  22.100000000000044\n",
            "Saved buffers to file `buffers.txt` !\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0, 80, 0.3, 50, 0.3, 20, 0.4, 100, 0.5, 80, 0]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 221: Controller loss : 1252.427124\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.1, 80, 0, 60, 0.5, 40, 0.5, 80, 0.2, 60, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1731\n",
            "Epoch 00001: val_loss improved from inf to 0.85686, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1731 - val_loss: 0.8569\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0503\n",
            "Epoch 00002: val_loss did not improve from 0.85686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0503 - val_loss: 1.1259\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0409\n",
            "Epoch 00003: val_loss did not improve from 0.85686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0409 - val_loss: 0.9107\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00004: val_loss did not improve from 0.85686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0299 - val_loss: 0.9536\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00005: val_loss improved from 0.85686 to 0.78098, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0227 - val_loss: 0.7810\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00006: val_loss improved from 0.78098 to 0.75826, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0199 - val_loss: 0.7583\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00007: val_loss improved from 0.75826 to 0.74369, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.7437\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00008: val_loss improved from 0.74369 to 0.70078, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0182 - val_loss: 0.7008\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00009: val_loss did not improve from 0.70078\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0182 - val_loss: 0.7881\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0163\n",
            "Epoch 00010: val_loss did not improve from 0.70078\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0163 - val_loss: 0.7525\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7007767489273304\n",
            "Total reward :  22.200000000000045\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.1, 80, 0, 60, 0.5, 40, 0.5, 80, 0.2, 60, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 222: Controller loss : 1137.880493\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.4, 50, 0.2, 100, 0.4, 100, 0.4, 20, 0.2, 80, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1720\n",
            "Epoch 00001: val_loss improved from inf to 1.03561, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1720 - val_loss: 1.0356\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0647\n",
            "Epoch 00002: val_loss did not improve from 1.03561\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0647 - val_loss: 1.2636\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0471\n",
            "Epoch 00003: val_loss improved from 1.03561 to 0.96814, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0471 - val_loss: 0.9681\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00004: val_loss did not improve from 0.96814\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0370 - val_loss: 1.1281\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00005: val_loss improved from 0.96814 to 0.89879, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0278 - val_loss: 0.8988\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0233\n",
            "Epoch 00006: val_loss did not improve from 0.89879\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0233 - val_loss: 0.9099\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00007: val_loss improved from 0.89879 to 0.76419, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0202 - val_loss: 0.7642\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00008: val_loss did not improve from 0.76419\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0186 - val_loss: 0.7893\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00009: val_loss did not improve from 0.76419\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.7768\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00010: val_loss did not improve from 0.76419\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.7942\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7641906063021657\n",
            "Total reward :  22.300000000000047\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.4, 50, 0.2, 100, 0.4, 100, 0.4, 20, 0.2, 80, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 223: Controller loss : 753.331238\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.3, 40, 0.1, 50, 0.1, 80, 0.2, 50, 0.5, 40, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1959\n",
            "Epoch 00001: val_loss improved from inf to 0.71340, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.1959 - val_loss: 0.7134\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0543\n",
            "Epoch 00002: val_loss did not improve from 0.71340\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0543 - val_loss: 1.1459\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0356\n",
            "Epoch 00003: val_loss did not improve from 0.71340\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0356 - val_loss: 0.8105\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0299\n",
            "Epoch 00004: val_loss did not improve from 0.71340\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0299 - val_loss: 0.9678\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0266\n",
            "Epoch 00005: val_loss did not improve from 0.71340\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0266 - val_loss: 0.7921\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0224\n",
            "Epoch 00006: val_loss did not improve from 0.71340\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0224 - val_loss: 0.7962\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00007: val_loss improved from 0.71340 to 0.67901, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0198 - val_loss: 0.6790\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00008: val_loss did not improve from 0.67901\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0194 - val_loss: 0.7253\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0171\n",
            "Epoch 00009: val_loss improved from 0.67901 to 0.67383, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0171 - val_loss: 0.6738\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00010: val_loss did not improve from 0.67383\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0177 - val_loss: 0.7191\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6738321085121637\n",
            "Total reward :  22.40000000000005\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.3, 40, 0.1, 50, 0.1, 80, 0.2, 50, 0.5, 40, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 224: Controller loss : 1275.113037\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.4, 100, 0.4, 80, 0.3, 60, 0, 100, 0, 100, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1424\n",
            "Epoch 00001: val_loss improved from inf to 1.14089, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1424 - val_loss: 1.1409\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0412\n",
            "Epoch 00002: val_loss improved from 1.14089 to 0.76977, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0412 - val_loss: 0.7698\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0312\n",
            "Epoch 00003: val_loss did not improve from 0.76977\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0312 - val_loss: 0.9726\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00004: val_loss improved from 0.76977 to 0.71094, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0278 - val_loss: 0.7109\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00005: val_loss improved from 0.71094 to 0.65973, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0204 - val_loss: 0.6597\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0196\n",
            "Epoch 00006: val_loss did not improve from 0.65973\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0196 - val_loss: 0.6972\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0209\n",
            "Epoch 00007: val_loss improved from 0.65973 to 0.59943, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0209 - val_loss: 0.5994\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00008: val_loss improved from 0.59943 to 0.59518, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0187 - val_loss: 0.5952\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00009: val_loss did not improve from 0.59518\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0181 - val_loss: 0.7146\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00010: val_loss did not improve from 0.59518\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0181 - val_loss: 0.6831\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5951826084153584\n",
            "Total reward :  22.50000000000005\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.4, 100, 0.4, 80, 0.3, 60, 0, 100, 0, 100, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 225: Controller loss : 860.763123\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0, 80, 0.2, 20, 0.1, 40, 0.4, 60, 0.4, 50, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1707\n",
            "Epoch 00001: val_loss improved from inf to 0.96340, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1707 - val_loss: 0.9634\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0460\n",
            "Epoch 00002: val_loss did not improve from 0.96340\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0460 - val_loss: 1.1436\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0354\n",
            "Epoch 00003: val_loss improved from 0.96340 to 0.93234, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0354 - val_loss: 0.9323\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00004: val_loss did not improve from 0.93234\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0272 - val_loss: 0.9693\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0231\n",
            "Epoch 00005: val_loss improved from 0.93234 to 0.79935, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0231 - val_loss: 0.7993\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00006: val_loss improved from 0.79935 to 0.71824, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.7182\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00007: val_loss did not improve from 0.71824\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.7785\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00008: val_loss did not improve from 0.71824\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0195 - val_loss: 0.7342\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00009: val_loss did not improve from 0.71824\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0170 - val_loss: 0.7516\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00010: val_loss did not improve from 0.71824\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0173 - val_loss: 0.7706\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7182408620589179\n",
            "Total reward :  22.60000000000005\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0, 80, 0.2, 20, 0.1, 40, 0.4, 60, 0.4, 50, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 226: Controller loss : 839.625122\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 80, 0.2, 50, 0, 40, 0, 60, 0.2, 40, 0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1866\n",
            "Epoch 00001: val_loss improved from inf to 0.78928, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1866 - val_loss: 0.7893\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0591\n",
            "Epoch 00002: val_loss did not improve from 0.78928\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0591 - val_loss: 1.2708\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0457\n",
            "Epoch 00003: val_loss did not improve from 0.78928\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0457 - val_loss: 0.8865\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0394\n",
            "Epoch 00004: val_loss did not improve from 0.78928\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0394 - val_loss: 1.0565\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0302\n",
            "Epoch 00005: val_loss did not improve from 0.78928\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0302 - val_loss: 0.8945\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0240\n",
            "Epoch 00006: val_loss did not improve from 0.78928\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0240 - val_loss: 0.8337\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00007: val_loss improved from 0.78928 to 0.74641, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0178 - val_loss: 0.7464\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0162\n",
            "Epoch 00008: val_loss improved from 0.74641 to 0.71490, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0162 - val_loss: 0.7149\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0169\n",
            "Epoch 00009: val_loss improved from 0.71490 to 0.68830, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0169 - val_loss: 0.6883\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00010: val_loss did not improve from 0.68830\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0164 - val_loss: 0.7359\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6882972758839593\n",
            "Total reward :  22.700000000000053\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.2, 80, 0.2, 50, 0, 40, 0, 60, 0.2, 40, 0]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 227: Controller loss : 853.647156\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [50, 0.4, 60, 0.5, 20, 0.3, 100, 0, 60, 0.4, 20, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1901\n",
            "Epoch 00001: val_loss improved from inf to 0.82746, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1901 - val_loss: 0.8275\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0526\n",
            "Epoch 00002: val_loss did not improve from 0.82746\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0526 - val_loss: 1.2503\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0443\n",
            "Epoch 00003: val_loss did not improve from 0.82746\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0443 - val_loss: 0.8627\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0362\n",
            "Epoch 00004: val_loss did not improve from 0.82746\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0362 - val_loss: 1.0343\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0307\n",
            "Epoch 00005: val_loss did not improve from 0.82746\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0307 - val_loss: 0.8751\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0234\n",
            "Epoch 00006: val_loss improved from 0.82746 to 0.82414, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0234 - val_loss: 0.8241\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00007: val_loss improved from 0.82414 to 0.74892, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0216 - val_loss: 0.7489\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0187\n",
            "Epoch 00008: val_loss improved from 0.74892 to 0.70463, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0187 - val_loss: 0.7046\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00009: val_loss improved from 0.70463 to 0.69400, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.6940\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00010: val_loss did not improve from 0.69400\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0190 - val_loss: 0.6981\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6939956949686952\n",
            "Total reward :  22.800000000000054\n",
            "State input to Controller for training :  [0 0 3 0 0 0]\n",
            "Training RNN (States ip) :  [50, 0.4, 60, 0.5, 20, 0.3, 100, 0, 60, 0.4, 20, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 228: Controller loss : 715.261230\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [60, 0.1, 60, 0.1, 60, 0, 100, 0.1, 40, 0, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2137\n",
            "Epoch 00001: val_loss improved from inf to 0.69500, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.2137 - val_loss: 0.6950\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0491\n",
            "Epoch 00002: val_loss did not improve from 0.69500\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0491 - val_loss: 1.0525\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0387\n",
            "Epoch 00003: val_loss did not improve from 0.69500\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0387 - val_loss: 0.8675\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0315\n",
            "Epoch 00004: val_loss did not improve from 0.69500\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0315 - val_loss: 0.8958\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00005: val_loss did not improve from 0.69500\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0246 - val_loss: 0.7763\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00006: val_loss improved from 0.69500 to 0.68522, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0190 - val_loss: 0.6852\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0171\n",
            "Epoch 00007: val_loss improved from 0.68522 to 0.67761, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0171 - val_loss: 0.6776\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00008: val_loss improved from 0.67761 to 0.65546, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0165 - val_loss: 0.6555\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00009: val_loss did not improve from 0.65546\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0165 - val_loss: 0.6671\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0160\n",
            "Epoch 00010: val_loss did not improve from 0.65546\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0160 - val_loss: 0.7053\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.655455292180213\n",
            "Total reward :  22.900000000000055\n",
            "State input to Controller for training :  [0 0 0 4 0 0]\n",
            "Training RNN (States ip) :  [60, 0.1, 60, 0.1, 60, 0, 100, 0.1, 40, 0, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 229: Controller loss : 932.159668\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [0. 0. 0. 4. 0. 0.]\n",
            "trying these actions:  [array([[6.4401245e-32, 9.6688363e-33, 5.8165163e-20, 6.3670928e-30,\n",
            "        2.8880300e-13, 1.0000000e+00]], dtype=float32), array([[4.6753944e-11, 2.4349011e-01, 3.0569143e-03, 3.7927913e-07,\n",
            "        7.5344342e-01, 9.1634683e-06]], dtype=float32), array([[2.5710514e-15, 1.1806037e-06, 6.8194327e-05, 1.2121654e-06,\n",
            "        9.9992943e-01, 3.9438501e-20]], dtype=float32), array([[5.3098798e-04, 6.4432644e-04, 3.5127235e-04, 1.2921644e-05,\n",
            "        2.2417893e-07, 9.9846029e-01]], dtype=float32), array([[9.0050716e-28, 3.9287269e-15, 4.1796058e-14, 9.9586833e-01,\n",
            "        3.6495030e-03, 4.8215609e-04]], dtype=float32), array([[9.0129252e-31, 6.0985976e-06, 2.5151081e-15, 9.7875434e-01,\n",
            "        2.1239590e-02, 1.1116778e-17]], dtype=float32), array([[9.5629952e-36, 9.5199321e-37, 0.0000000e+00, 3.9980939e-09,\n",
            "        3.3824783e-08, 1.0000000e+00]], dtype=float32), array([[7.6235199e-07, 9.8171804e-06, 2.8525643e-20, 4.0580061e-14,\n",
            "        5.2120292e-01, 4.7878653e-01]], dtype=float32), array([[2.2425005e-20, 1.1337842e-22, 3.9783277e-33, 2.3777426e-19,\n",
            "        7.5276361e-08, 9.9999988e-01]], dtype=float32), array([[0.00000000e+00, 8.80857610e-34, 3.62752085e-11, 7.53201267e-13,\n",
            "        1.00000000e+00, 1.03762735e-17]], dtype=float32), array([[2.7779684e-18, 3.1160217e-04, 3.8811068e-10, 6.6985211e-07,\n",
            "        8.2225737e-10, 9.9968779e-01]], dtype=float32), array([[2.0785579e-02, 8.7858140e-01, 9.6644543e-04, 2.1486399e-03,\n",
            "        5.6023514e-10, 9.7517893e-02]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 6.4401245e-32), (40, 9.668836e-33), (50, 5.816516e-20), (60, 6.367093e-30), (80, 2.88803e-13), (100, 1.0)]\n",
            "dropout :  [(0, 4.6753944e-11), (0.1, 0.24349011), (0.2, 0.0030569143), (0.3, 3.7927913e-07), (0.4, 0.7534434), (0.5, 9.163468e-06)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 2.5710514e-15), (40, 1.1806037e-06), (50, 6.819433e-05), (60, 1.2121654e-06), (80, 0.9999294), (100, 3.94385e-20)]\n",
            "dropout :  [(0, 0.000530988), (0.1, 0.00064432644), (0.2, 0.00035127235), (0.3, 1.2921644e-05), (0.4, 2.2417893e-07), (0.5, 0.9984603)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 9.005072e-28), (40, 3.928727e-15), (50, 4.1796058e-14), (60, 0.9958683), (80, 0.003649503), (100, 0.0004821561)]\n",
            "dropout :  [(0, 9.012925e-31), (0.1, 6.0985976e-06), (0.2, 2.515108e-15), (0.3, 0.97875434), (0.4, 0.02123959), (0.5, 1.1116778e-17)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 9.562995e-36), (40, 9.519932e-37), (50, 0.0), (60, 3.998094e-09), (80, 3.3824783e-08), (100, 1.0)]\n",
            "dropout :  [(0, 7.62352e-07), (0.1, 9.81718e-06), (0.2, 2.8525643e-20), (0.3, 4.058006e-14), (0.4, 0.5212029), (0.5, 0.47878653)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 2.2425005e-20), (40, 1.1337842e-22), (50, 3.9783277e-33), (60, 2.3777426e-19), (80, 7.527636e-08), (100, 0.9999999)]\n",
            "dropout :  [(0, 0.0), (0.1, 8.808576e-34), (0.2, 3.627521e-11), (0.3, 7.5320127e-13), (0.4, 1.0), (0.5, 1.03762735e-17)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 2.7779684e-18), (40, 0.00031160217), (50, 3.8811068e-10), (60, 6.698521e-07), (80, 8.222574e-10), (100, 0.9996878)]\n",
            "dropout :  [(0, 0.02078558), (0.1, 0.8785814), (0.2, 0.0009664454), (0.3, 0.0021486399), (0.4, 5.6023514e-10), (0.5, 0.09751789)]\n",
            "\n",
            "Predicted actions :  [100, 0.4, 80, 0.5, 60, 0.3, 100, 0.4, 100, 0.4, 100, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1575\n",
            "Epoch 00001: val_loss improved from inf to 1.12596, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1575 - val_loss: 1.1260\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0430\n",
            "Epoch 00002: val_loss improved from 1.12596 to 0.79772, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0430 - val_loss: 0.7977\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0315\n",
            "Epoch 00003: val_loss did not improve from 0.79772\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0315 - val_loss: 0.9478\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0244\n",
            "Epoch 00004: val_loss improved from 0.79772 to 0.74220, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0244 - val_loss: 0.7422\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0200\n",
            "Epoch 00005: val_loss improved from 0.74220 to 0.61979, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0200 - val_loss: 0.6198\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00006: val_loss improved from 0.61979 to 0.61796, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0190 - val_loss: 0.6180\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00007: val_loss did not improve from 0.61796\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0168 - val_loss: 0.6663\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00008: val_loss did not improve from 0.61796\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0168 - val_loss: 0.6482\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00009: val_loss did not improve from 0.61796\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0164 - val_loss: 0.6707\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0162\n",
            "Epoch 00010: val_loss did not improve from 0.61796\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0162 - val_loss: 0.6831\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.617958410632896\n",
            "Total reward :  23.000000000000057\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.4, 80, 0.5, 60, 0.3, 100, 0.4, 100, 0.4, 100, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 230: Controller loss : 74.513420\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [6.4401245e-32 9.6688363e-33 5.8165163e-20 6.3670928e-30 2.8880300e-13\n",
            " 1.0000000e+00]\n",
            "trying these actions:  [array([[3.6271329e-34, 5.1085423e-35, 3.3511422e-22, 3.7165243e-32,\n",
            "        1.6841985e-15, 1.0000000e+00]], dtype=float32), array([[8.2781892e-14, 3.4597112e-04, 5.9033548e-05, 3.1233995e-09,\n",
            "        9.9959487e-01, 6.6511738e-08]], dtype=float32), array([[1.5641410e-17, 7.4138939e-09, 3.9032466e-07, 8.3831155e-09,\n",
            "        9.9999964e-01, 2.5366859e-22]], dtype=float32), array([[1.7879219e-06, 2.2875893e-06, 1.2724583e-06, 4.6504436e-08,\n",
            "        8.3924440e-10, 9.9999464e-01]], dtype=float32), array([[6.5338625e-30, 2.8275859e-17, 3.1510221e-16, 9.9996889e-01,\n",
            "        2.7432847e-05, 3.6287877e-06]], dtype=float32), array([[8.7481759e-33, 6.0463464e-08, 2.4909355e-17, 9.9979085e-01,\n",
            "        2.0908412e-04, 1.1267072e-19]], dtype=float32), array([[7.3609485e-38, 0.0000000e+00, 0.0000000e+00, 3.3325617e-11,\n",
            "        2.8432706e-10, 1.0000000e+00]], dtype=float32), array([[5.2084372e-09, 6.9329637e-08, 2.0156084e-22, 2.7975283e-16,\n",
            "        9.9827409e-01, 1.7258015e-03]], dtype=float32), array([[9.6753052e-23, 5.3006942e-25, 1.8745449e-35, 1.1484763e-21,\n",
            "        3.6031261e-10, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 6.6888943e-36, 2.8739625e-13, 5.8632912e-15,\n",
            "        1.0000000e+00, 8.3199603e-20]], dtype=float32), array([[1.4350860e-20, 1.6965519e-06, 2.1393247e-12, 3.7311221e-09,\n",
            "        4.6070851e-12, 9.9999833e-01]], dtype=float32), array([[8.07473494e-04, 9.94837582e-01, 4.74973313e-05, 1.05930405e-04,\n",
            "        2.77317214e-11, 4.20139404e-03]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 3.627133e-34), (40, 5.1085423e-35), (50, 3.3511422e-22), (60, 3.7165243e-32), (80, 1.6841985e-15), (100, 1.0)]\n",
            "dropout :  [(0, 8.278189e-14), (0.1, 0.00034597112), (0.2, 5.9033548e-05), (0.3, 3.1233995e-09), (0.4, 0.99959487), (0.5, 6.651174e-08)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.564141e-17), (40, 7.413894e-09), (50, 3.9032466e-07), (60, 8.3831155e-09), (80, 0.99999964), (100, 2.5366859e-22)]\n",
            "dropout :  [(0, 1.7879219e-06), (0.1, 2.2875893e-06), (0.2, 1.2724583e-06), (0.3, 4.6504436e-08), (0.4, 8.392444e-10), (0.5, 0.99999464)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 6.5338625e-30), (40, 2.827586e-17), (50, 3.151022e-16), (60, 0.9999689), (80, 2.7432847e-05), (100, 3.6287877e-06)]\n",
            "dropout :  [(0, 8.748176e-33), (0.1, 6.0463464e-08), (0.2, 2.4909355e-17), (0.3, 0.99979085), (0.4, 0.00020908412), (0.5, 1.1267072e-19)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 7.3609485e-38), (40, 0.0), (50, 0.0), (60, 3.3325617e-11), (80, 2.8432706e-10), (100, 1.0)]\n",
            "dropout :  [(0, 5.2084372e-09), (0.1, 6.932964e-08), (0.2, 2.0156084e-22), (0.3, 2.7975283e-16), (0.4, 0.9982741), (0.5, 0.0017258015)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 9.675305e-23), (40, 5.300694e-25), (50, 1.8745449e-35), (60, 1.1484763e-21), (80, 3.603126e-10), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 6.688894e-36), (0.2, 2.8739625e-13), (0.3, 5.8632912e-15), (0.4, 1.0), (0.5, 8.31996e-20)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.435086e-20), (40, 1.6965519e-06), (50, 2.1393247e-12), (60, 3.731122e-09), (80, 4.607085e-12), (100, 0.99999833)]\n",
            "dropout :  [(0, 0.0008074735), (0.1, 0.9948376), (0.2, 4.749733e-05), (0.3, 0.000105930405), (0.4, 2.7731721e-11), (0.5, 0.004201394)]\n",
            "\n",
            "Predicted actions :  [100, 0.4, 80, 0.5, 60, 0.3, 100, 0.4, 100, 0.4, 100, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1470\n",
            "Epoch 00001: val_loss improved from inf to 1.18667, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1470 - val_loss: 1.1867\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0577\n",
            "Epoch 00002: val_loss improved from 1.18667 to 0.84434, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0577 - val_loss: 0.8443\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00003: val_loss did not improve from 0.84434\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0384 - val_loss: 1.0099\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00004: val_loss improved from 0.84434 to 0.77125, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0338 - val_loss: 0.7712\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00005: val_loss did not improve from 0.77125\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0241 - val_loss: 0.7832\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00006: val_loss improved from 0.77125 to 0.58017, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0194 - val_loss: 0.5802\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00007: val_loss did not improve from 0.58017\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0189 - val_loss: 0.5892\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00008: val_loss did not improve from 0.58017\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0179 - val_loss: 0.6212\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0171\n",
            "Epoch 00009: val_loss did not improve from 0.58017\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0171 - val_loss: 0.6363\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00010: val_loss did not improve from 0.58017\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0167 - val_loss: 0.6304\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5801676893545911\n",
            "Total reward :  23.10000000000006\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.4, 80, 0.5, 60, 0.3, 100, 0.4, 100, 0.4, 100, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 231: Controller loss : 71.276085\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.5, 40, 0.4, 40, 0.2, 50, 0.2, 20, 0.5, 40, 0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2448\n",
            "Epoch 00001: val_loss improved from inf to 1.07053, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2448 - val_loss: 1.0705\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0623\n",
            "Epoch 00002: val_loss did not improve from 1.07053\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0623 - val_loss: 1.2654\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0573\n",
            "Epoch 00003: val_loss did not improve from 1.07053\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0573 - val_loss: 1.1777\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0397\n",
            "Epoch 00004: val_loss improved from 1.07053 to 0.97086, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0397 - val_loss: 0.9709\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0330\n",
            "Epoch 00005: val_loss did not improve from 0.97086\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0330 - val_loss: 1.0361\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00006: val_loss improved from 0.97086 to 0.82933, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0254 - val_loss: 0.8293\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0216\n",
            "Epoch 00007: val_loss improved from 0.82933 to 0.82546, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0216 - val_loss: 0.8255\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0190\n",
            "Epoch 00008: val_loss improved from 0.82546 to 0.74127, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0190 - val_loss: 0.7413\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00009: val_loss did not improve from 0.74127\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.8070\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00010: val_loss did not improve from 0.74127\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.8051\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.741266222561107\n",
            "Total reward :  23.20000000000006\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.5, 40, 0.4, 40, 0.2, 50, 0.2, 20, 0.5, 40, 0]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 232: Controller loss : 1649.239990\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [40, 0.1, 20, 0, 100, 0.1, 80, 0, 40, 0.3, 100, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1722\n",
            "Epoch 00001: val_loss improved from inf to 0.96996, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1722 - val_loss: 0.9700\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0645\n",
            "Epoch 00002: val_loss did not improve from 0.96996\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0645 - val_loss: 1.1912\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0401\n",
            "Epoch 00003: val_loss improved from 0.96996 to 0.86989, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0401 - val_loss: 0.8699\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0334\n",
            "Epoch 00004: val_loss did not improve from 0.86989\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0334 - val_loss: 1.0618\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0277\n",
            "Epoch 00005: val_loss improved from 0.86989 to 0.82353, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0277 - val_loss: 0.8235\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0231\n",
            "Epoch 00006: val_loss did not improve from 0.82353\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0231 - val_loss: 0.8511\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00007: val_loss improved from 0.82353 to 0.69987, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0186 - val_loss: 0.6999\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00008: val_loss did not improve from 0.69987\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.7091\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00009: val_loss improved from 0.69987 to 0.65577, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0177 - val_loss: 0.6558\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0169\n",
            "Epoch 00010: val_loss did not improve from 0.65577\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0169 - val_loss: 0.6985\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6557749460465508\n",
            "Total reward :  23.30000000000006\n",
            "State input to Controller for training :  [0 2 0 0 0 0]\n",
            "Training RNN (States ip) :  [40, 0.1, 20, 0, 100, 0.1, 80, 0, 40, 0.3, 100, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 233: Controller loss : 870.539978\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [50, 0.1, 80, 0.5, 50, 0.4, 50, 0.3, 40, 0.4, 80, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1895\n",
            "Epoch 00001: val_loss improved from inf to 0.76962, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1895 - val_loss: 0.7696\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0626\n",
            "Epoch 00002: val_loss did not improve from 0.76962\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0626 - val_loss: 1.2740\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0457\n",
            "Epoch 00003: val_loss did not improve from 0.76962\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0457 - val_loss: 0.8941\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0338\n",
            "Epoch 00004: val_loss did not improve from 0.76962\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0338 - val_loss: 1.0256\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0296\n",
            "Epoch 00005: val_loss did not improve from 0.76962\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0296 - val_loss: 0.8687\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0227\n",
            "Epoch 00006: val_loss did not improve from 0.76962\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0227 - val_loss: 0.7956\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00007: val_loss improved from 0.76962 to 0.72423, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0193 - val_loss: 0.7242\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00008: val_loss improved from 0.72423 to 0.67370, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.6737\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00009: val_loss did not improve from 0.67370\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0182 - val_loss: 0.7158\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00010: val_loss did not improve from 0.67370\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0168 - val_loss: 0.6922\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6736986315068596\n",
            "Total reward :  23.400000000000063\n",
            "State input to Controller for training :  [0 0 3 0 0 0]\n",
            "Training RNN (States ip) :  [50, 0.1, 80, 0.5, 50, 0.4, 50, 0.3, 40, 0.4, 80, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 234: Controller loss : 1242.320190\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [40, 0.2, 60, 0, 50, 0.2, 50, 0, 60, 0.2, 40, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2020\n",
            "Epoch 00001: val_loss improved from inf to 0.82037, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2020 - val_loss: 0.8204\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0587\n",
            "Epoch 00002: val_loss did not improve from 0.82037\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0587 - val_loss: 1.1816\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0427\n",
            "Epoch 00003: val_loss did not improve from 0.82037\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0427 - val_loss: 0.8715\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0349\n",
            "Epoch 00004: val_loss did not improve from 0.82037\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0349 - val_loss: 0.9323\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0246\n",
            "Epoch 00005: val_loss improved from 0.82037 to 0.72550, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0246 - val_loss: 0.7255\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0207\n",
            "Epoch 00006: val_loss improved from 0.72550 to 0.67242, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0207 - val_loss: 0.6724\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0210\n",
            "Epoch 00007: val_loss did not improve from 0.67242\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0210 - val_loss: 0.7357\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00008: val_loss did not improve from 0.67242\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.7010\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00009: val_loss did not improve from 0.67242\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0175 - val_loss: 0.7070\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00010: val_loss did not improve from 0.67242\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.7503\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6724223836055249\n",
            "Total reward :  23.500000000000064\n",
            "State input to Controller for training :  [0 2 0 0 0 0]\n",
            "Training RNN (States ip) :  [40, 0.2, 60, 0, 50, 0.2, 50, 0, 60, 0.2, 40, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 235: Controller loss : 1178.174561\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.5, 60, 0.4, 50, 0.2, 100, 0.4, 60, 0.3, 40, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1669\n",
            "Epoch 00001: val_loss improved from inf to 0.95589, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1669 - val_loss: 0.9559\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0521\n",
            "Epoch 00002: val_loss did not improve from 0.95589\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0521 - val_loss: 1.1653\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0445\n",
            "Epoch 00003: val_loss did not improve from 0.95589\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0445 - val_loss: 1.0371\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0376\n",
            "Epoch 00004: val_loss did not improve from 0.95589\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0376 - val_loss: 1.0228\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0286\n",
            "Epoch 00005: val_loss improved from 0.95589 to 0.85482, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0286 - val_loss: 0.8548\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00006: val_loss improved from 0.85482 to 0.74482, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0226 - val_loss: 0.7448\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0213\n",
            "Epoch 00007: val_loss did not improve from 0.74482\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0213 - val_loss: 0.7593\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0203\n",
            "Epoch 00008: val_loss improved from 0.74482 to 0.64942, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0203 - val_loss: 0.6494\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0219\n",
            "Epoch 00009: val_loss did not improve from 0.64942\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0219 - val_loss: 0.7529\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00010: val_loss did not improve from 0.64942\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.7780\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6494228143837457\n",
            "Total reward :  23.600000000000065\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.5, 60, 0.4, 50, 0.2, 100, 0.4, 60, 0.3, 40, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 236: Controller loss : 877.335632\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.4, 100, 0.2, 60, 0.5, 40, 0.2, 100, 0.1, 20, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1815\n",
            "Epoch 00001: val_loss improved from inf to 0.94207, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1815 - val_loss: 0.9421\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0603\n",
            "Epoch 00002: val_loss did not improve from 0.94207\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0603 - val_loss: 1.2977\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0477\n",
            "Epoch 00003: val_loss did not improve from 0.94207\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0477 - val_loss: 1.0038\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0357\n",
            "Epoch 00004: val_loss did not improve from 0.94207\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0357 - val_loss: 1.0380\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0271\n",
            "Epoch 00005: val_loss improved from 0.94207 to 0.83711, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0271 - val_loss: 0.8371\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0201\n",
            "Epoch 00006: val_loss improved from 0.83711 to 0.82570, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0201 - val_loss: 0.8257\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00007: val_loss improved from 0.82570 to 0.77275, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0202 - val_loss: 0.7727\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00008: val_loss did not improve from 0.77275\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0186 - val_loss: 0.7844\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00009: val_loss did not improve from 0.77275\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0165 - val_loss: 0.7975\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00010: val_loss improved from 0.77275 to 0.76562, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.7656\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7656191839371891\n",
            "Total reward :  23.700000000000067\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.4, 100, 0.2, 60, 0.5, 40, 0.2, 100, 0.1, 20, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 237: Controller loss : 1339.005615\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [1. 0. 0. 0. 0. 0.]\n",
            "trying these actions:  [array([[2.7641686e-28, 6.2149836e-27, 4.7106145e-18, 2.4715789e-31,\n",
            "        2.4700582e-11, 1.0000000e+00]], dtype=float32), array([[1.2182203e-16, 7.0316286e-04, 1.2871368e-04, 6.3822519e-12,\n",
            "        9.9787748e-01, 1.2907120e-03]], dtype=float32), array([[2.1298042e-15, 7.2343664e-07, 1.4672237e-08, 1.0711062e-04,\n",
            "        9.9989212e-01, 5.8213934e-20]], dtype=float32), array([[9.8030148e-03, 1.0582181e-08, 1.2249228e-05, 2.4106131e-10,\n",
            "        2.8002420e-05, 9.9015677e-01]], dtype=float32), array([[8.3332581e-31, 2.8959728e-14, 9.0253618e-09, 9.9891388e-01,\n",
            "        4.9587402e-06, 1.0811989e-03]], dtype=float32), array([[7.7885517e-32, 7.1554631e-04, 1.1630717e-06, 4.7721952e-01,\n",
            "        5.2206379e-01, 1.7438403e-14]], dtype=float32), array([[0.0000000e+00, 1.6193757e-37, 3.5869117e-35, 3.3821212e-13,\n",
            "        4.9874904e-09, 1.0000000e+00]], dtype=float32), array([[2.6094101e-05, 2.1215607e-09, 8.0310019e-17, 1.8588593e-13,\n",
            "        9.9990964e-01, 6.4304251e-05]], dtype=float32), array([[1.4640007e-21, 3.8589531e-21, 1.1802316e-37, 1.7085205e-18,\n",
            "        2.5924811e-12, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 3.1596752e-33, 1.7989346e-11, 4.6681872e-09,\n",
            "        1.0000000e+00, 9.1418115e-18]], dtype=float32), array([[2.2103421e-19, 2.8393960e-01, 9.3290811e-15, 1.7807616e-11,\n",
            "        5.3055126e-11, 7.1606046e-01]], dtype=float32), array([[1.9686783e-04, 5.7832664e-04, 9.9902022e-01, 4.7864141e-06,\n",
            "        1.7123033e-05, 1.8267406e-04]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 2.7641686e-28), (40, 6.2149836e-27), (50, 4.7106145e-18), (60, 2.471579e-31), (80, 2.4700582e-11), (100, 1.0)]\n",
            "dropout :  [(0, 1.2182203e-16), (0.1, 0.00070316286), (0.2, 0.00012871368), (0.3, 6.382252e-12), (0.4, 0.9978775), (0.5, 0.001290712)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 2.1298042e-15), (40, 7.2343664e-07), (50, 1.4672237e-08), (60, 0.00010711062), (80, 0.9998921), (100, 5.8213934e-20)]\n",
            "dropout :  [(0, 0.009803015), (0.1, 1.0582181e-08), (0.2, 1.2249228e-05), (0.3, 2.410613e-10), (0.4, 2.800242e-05), (0.5, 0.99015677)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 8.333258e-31), (40, 2.8959728e-14), (50, 9.025362e-09), (60, 0.9989139), (80, 4.95874e-06), (100, 0.0010811989)]\n",
            "dropout :  [(0, 7.7885517e-32), (0.1, 0.0007155463), (0.2, 1.1630717e-06), (0.3, 0.47721952), (0.4, 0.5220638), (0.5, 1.7438403e-14)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 1.6193757e-37), (50, 3.5869117e-35), (60, 3.3821212e-13), (80, 4.9874904e-09), (100, 1.0)]\n",
            "dropout :  [(0, 2.6094101e-05), (0.1, 2.1215607e-09), (0.2, 8.031002e-17), (0.3, 1.8588593e-13), (0.4, 0.99990964), (0.5, 6.430425e-05)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.4640007e-21), (40, 3.858953e-21), (50, 1.1802316e-37), (60, 1.7085205e-18), (80, 2.5924811e-12), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 3.159675e-33), (0.2, 1.7989346e-11), (0.3, 4.668187e-09), (0.4, 1.0), (0.5, 9.1418115e-18)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 2.2103421e-19), (40, 0.2839396), (50, 9.329081e-15), (60, 1.7807616e-11), (80, 5.3055126e-11), (100, 0.71606046)]\n",
            "dropout :  [(0, 0.00019686783), (0.1, 0.00057832664), (0.2, 0.9990202), (0.3, 4.786414e-06), (0.4, 1.7123033e-05), (0.5, 0.00018267406)]\n",
            "\n",
            "Predicted actions :  [100, 0.4, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1504\n",
            "Epoch 00001: val_loss improved from inf to 1.06443, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1504 - val_loss: 1.0644\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0573\n",
            "Epoch 00002: val_loss improved from 1.06443 to 0.80657, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0573 - val_loss: 0.8066\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0370\n",
            "Epoch 00003: val_loss did not improve from 0.80657\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0370 - val_loss: 1.0083\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0313\n",
            "Epoch 00004: val_loss improved from 0.80657 to 0.74589, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0313 - val_loss: 0.7459\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0251\n",
            "Epoch 00005: val_loss did not improve from 0.74589\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0251 - val_loss: 0.7499\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00006: val_loss improved from 0.74589 to 0.58664, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0191 - val_loss: 0.5866\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0173\n",
            "Epoch 00007: val_loss improved from 0.58664 to 0.52339, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0173 - val_loss: 0.5234\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00008: val_loss did not improve from 0.52339\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0182 - val_loss: 0.6197\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0170\n",
            "Epoch 00009: val_loss did not improve from 0.52339\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0170 - val_loss: 0.6508\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0161\n",
            "Epoch 00010: val_loss did not improve from 0.52339\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0161 - val_loss: 0.6342\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.523392319419545\n",
            "Total reward :  23.800000000000068\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.4, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 238: Controller loss : 81.392746\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 100, 0.2, 20, 0.2, 50, 0.3, 60, 0.4, 100, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1403\n",
            "Epoch 00001: val_loss improved from inf to 1.19435, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1403 - val_loss: 1.1944\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0480\n",
            "Epoch 00002: val_loss improved from 1.19435 to 0.94495, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0480 - val_loss: 0.9449\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0330\n",
            "Epoch 00003: val_loss did not improve from 0.94495\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0330 - val_loss: 1.0308\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00004: val_loss improved from 0.94495 to 0.80464, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0278 - val_loss: 0.8046\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0217\n",
            "Epoch 00005: val_loss improved from 0.80464 to 0.80118, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0217 - val_loss: 0.8012\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0204\n",
            "Epoch 00006: val_loss improved from 0.80118 to 0.68068, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0204 - val_loss: 0.6807\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00007: val_loss did not improve from 0.68068\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.7021\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00008: val_loss did not improve from 0.68068\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0177 - val_loss: 0.7506\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0176\n",
            "Epoch 00009: val_loss did not improve from 0.68068\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.7392\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0165\n",
            "Epoch 00010: val_loss did not improve from 0.68068\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0165 - val_loss: 0.6930\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6806760265416829\n",
            "Total reward :  23.90000000000007\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.5, 100, 0.2, 20, 0.2, 50, 0.3, 60, 0.4, 100, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 239: Controller loss : 1221.792847\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [80, 0.1, 50, 0.2, 60, 0.1, 20, 0.2, 50, 0.3, 50, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2167\n",
            "Epoch 00001: val_loss improved from inf to 0.76738, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2167 - val_loss: 0.7674\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0552\n",
            "Epoch 00002: val_loss did not improve from 0.76738\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0552 - val_loss: 1.2055\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0409\n",
            "Epoch 00003: val_loss did not improve from 0.76738\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0409 - val_loss: 0.9322\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0326\n",
            "Epoch 00004: val_loss did not improve from 0.76738\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0326 - val_loss: 0.9522\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0265\n",
            "Epoch 00005: val_loss did not improve from 0.76738\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0265 - val_loss: 0.8421\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0193\n",
            "Epoch 00006: val_loss did not improve from 0.76738\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0193 - val_loss: 0.7942\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00007: val_loss improved from 0.76738 to 0.69169, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.6917\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00008: val_loss did not improve from 0.69169\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.7424\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00009: val_loss did not improve from 0.69169\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0178 - val_loss: 0.7313\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0166\n",
            "Epoch 00010: val_loss did not improve from 0.69169\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0166 - val_loss: 0.7611\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6916913191477457\n",
            "Total reward :  24.00000000000007\n",
            "State input to Controller for training :  [0 0 0 0 5 0]\n",
            "Training RNN (States ip) :  [80, 0.1, 50, 0.2, 60, 0.1, 20, 0.2, 50, 0.3, 50, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 240: Controller loss : 1175.979980\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.5, 50, 0.3, 100, 0, 100, 0.2, 40, 0.3, 60, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1402\n",
            "Epoch 00001: val_loss improved from inf to 1.10760, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.1402 - val_loss: 1.1076\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0391\n",
            "Epoch 00002: val_loss improved from 1.10760 to 0.88630, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0391 - val_loss: 0.8863\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0331\n",
            "Epoch 00003: val_loss did not improve from 0.88630\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0331 - val_loss: 1.0076\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00004: val_loss improved from 0.88630 to 0.74640, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0278 - val_loss: 0.7464\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0231\n",
            "Epoch 00005: val_loss did not improve from 0.74640\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0231 - val_loss: 0.7961\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0198\n",
            "Epoch 00006: val_loss improved from 0.74640 to 0.70183, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0198 - val_loss: 0.7018\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0192\n",
            "Epoch 00007: val_loss improved from 0.70183 to 0.63887, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0192 - val_loss: 0.6389\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0191\n",
            "Epoch 00008: val_loss did not improve from 0.63887\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0191 - val_loss: 0.7446\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0182\n",
            "Epoch 00009: val_loss did not improve from 0.63887\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0182 - val_loss: 0.7060\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00010: val_loss did not improve from 0.63887\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0188 - val_loss: 0.6848\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6388728317352161\n",
            "Total reward :  24.100000000000072\n",
            "Saved buffers to file `buffers.txt` !\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.5, 50, 0.3, 100, 0, 100, 0.2, 40, 0.3, 60, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 241: Controller loss : 936.899780\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [1. 0. 0. 0. 0. 0.]\n",
            "trying these actions:  [array([[1.47237329e-29, 1.15723582e-30, 9.08963350e-22, 4.78353163e-35,\n",
            "        1.07435675e-11, 1.00000000e+00]], dtype=float32), array([[2.4275153e-19, 7.0798543e-04, 3.5669694e-07, 1.6302274e-14,\n",
            "        8.3946884e-03, 9.9089688e-01]], dtype=float32), array([[5.5501432e-16, 2.3352456e-07, 1.9664114e-02, 4.1203399e-05,\n",
            "        9.8029447e-01, 2.5801198e-17]], dtype=float32), array([[7.3952315e-04, 9.6176145e-10, 2.1634728e-01, 1.8966067e-07,\n",
            "        3.1124473e-06, 7.8290999e-01]], dtype=float32), array([[4.24348683e-30, 5.33962291e-17, 1.96298013e-11, 9.93206739e-01,\n",
            "        1.05183595e-08, 6.79330900e-03]], dtype=float32), array([[6.4340697e-29, 1.6950004e-01, 1.0372823e-04, 4.0910639e-02,\n",
            "        7.8948557e-01, 4.1404135e-15]], dtype=float32), array([[0.0000000e+00, 0.0000000e+00, 7.4168212e-36, 1.1734161e-16,\n",
            "        1.8817096e-12, 1.0000000e+00]], dtype=float32), array([[8.03595140e-06, 7.04799774e-10, 1.06201575e-11, 2.50783144e-10,\n",
            "        9.99966979e-01, 2.49972873e-05]], dtype=float32), array([[1.5540329e-22, 5.9817509e-19, 2.4862796e-34, 8.2195607e-17,\n",
            "        4.0473750e-13, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 2.0926461e-36, 1.2588240e-14, 1.4252653e-06,\n",
            "        9.9999857e-01, 6.8251585e-21]], dtype=float32), array([[5.7260024e-23, 3.8205199e-05, 2.2944615e-14, 4.0722196e-11,\n",
            "        1.8645301e-14, 9.9996173e-01]], dtype=float32), array([[0.00053231, 0.373938  , 0.43074816, 0.1221282 , 0.07198854,\n",
            "        0.00066483]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.4723733e-29), (40, 1.1572358e-30), (50, 9.089633e-22), (60, 4.7835316e-35), (80, 1.07435675e-11), (100, 1.0)]\n",
            "dropout :  [(0, 2.4275153e-19), (0.1, 0.0007079854), (0.2, 3.5669694e-07), (0.3, 1.6302274e-14), (0.4, 0.008394688), (0.5, 0.9908969)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 5.550143e-16), (40, 2.3352456e-07), (50, 0.019664114), (60, 4.12034e-05), (80, 0.98029447), (100, 2.5801198e-17)]\n",
            "dropout :  [(0, 0.00073952315), (0.1, 9.617614e-10), (0.2, 0.21634728), (0.3, 1.8966067e-07), (0.4, 3.1124473e-06), (0.5, 0.78291)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 4.243487e-30), (40, 5.339623e-17), (50, 1.9629801e-11), (60, 0.99320674), (80, 1.05183595e-08), (100, 0.006793309)]\n",
            "dropout :  [(0, 6.43407e-29), (0.1, 0.16950004), (0.2, 0.00010372823), (0.3, 0.04091064), (0.4, 0.7894856), (0.5, 4.1404135e-15)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 7.416821e-36), (60, 1.173416e-16), (80, 1.8817096e-12), (100, 1.0)]\n",
            "dropout :  [(0, 8.035951e-06), (0.1, 7.047998e-10), (0.2, 1.06201575e-11), (0.3, 2.5078314e-10), (0.4, 0.999967), (0.5, 2.4997287e-05)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.5540329e-22), (40, 5.981751e-19), (50, 2.4862796e-34), (60, 8.219561e-17), (80, 4.047375e-13), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0926461e-36), (0.2, 1.258824e-14), (0.3, 1.4252653e-06), (0.4, 0.99999857), (0.5, 6.8251585e-21)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 5.7260024e-23), (40, 3.82052e-05), (50, 2.2944615e-14), (60, 4.0722196e-11), (80, 1.8645301e-14), (100, 0.99996173)]\n",
            "dropout :  [(0, 0.00053230615), (0.1, 0.373938), (0.2, 0.43074816), (0.3, 0.1221282), (0.4, 0.07198854), (0.5, 0.00066482514)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1581\n",
            "Epoch 00001: val_loss improved from inf to 1.05744, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1581 - val_loss: 1.0574\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0442\n",
            "Epoch 00002: val_loss improved from 1.05744 to 0.80378, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0442 - val_loss: 0.8038\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0316\n",
            "Epoch 00003: val_loss did not improve from 0.80378\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0316 - val_loss: 0.9650\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0269\n",
            "Epoch 00004: val_loss improved from 0.80378 to 0.69804, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0269 - val_loss: 0.6980\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0202\n",
            "Epoch 00005: val_loss improved from 0.69804 to 0.62174, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0202 - val_loss: 0.6217\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0203\n",
            "Epoch 00006: val_loss did not improve from 0.62174\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0203 - val_loss: 0.6301\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00007: val_loss did not improve from 0.62174\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0179 - val_loss: 0.6718\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00008: val_loss did not improve from 0.62174\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0175 - val_loss: 0.6484\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00009: val_loss did not improve from 0.62174\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0174 - val_loss: 0.6477\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0167\n",
            "Epoch 00010: val_loss did not improve from 0.62174\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0167 - val_loss: 0.6374\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6217377502674111\n",
            "Total reward :  24.200000000000074\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.5, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 242: Controller loss : 84.265121\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 100, 0.2, 60, 0, 20, 0.3, 100, 0.3, 60, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1684\n",
            "Epoch 00001: val_loss improved from inf to 0.99950, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1684 - val_loss: 0.9995\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0458\n",
            "Epoch 00002: val_loss did not improve from 0.99950\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0458 - val_loss: 1.1053\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0347\n",
            "Epoch 00003: val_loss improved from 0.99950 to 0.97353, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0347 - val_loss: 0.9735\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0241\n",
            "Epoch 00004: val_loss improved from 0.97353 to 0.86197, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0241 - val_loss: 0.8620\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00005: val_loss improved from 0.86197 to 0.84581, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0194 - val_loss: 0.8458\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00006: val_loss improved from 0.84581 to 0.72846, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0188 - val_loss: 0.7285\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00007: val_loss did not improve from 0.72846\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.7714\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00008: val_loss did not improve from 0.72846\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0178 - val_loss: 0.7821\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00009: val_loss did not improve from 0.72846\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0158 - val_loss: 0.7740\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00010: val_loss did not improve from 0.72846\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0168 - val_loss: 0.7662\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7284573007512976\n",
            "Total reward :  24.300000000000075\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.5, 100, 0.2, 60, 0, 20, 0.3, 100, 0.3, 60, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 243: Controller loss : 825.640808\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [0. 0. 0. 0. 0. 6.]\n",
            "trying these actions:  [array([[7.2115435e-34, 5.7658164e-35, 4.6246330e-26, 0.0000000e+00,\n",
            "        5.7426266e-16, 1.0000000e+00]], dtype=float32), array([[2.1640280e-23, 7.6133411e-08, 3.8595870e-11, 1.6808502e-18,\n",
            "        9.3623953e-07, 9.9999893e-01]], dtype=float32), array([[8.9289995e-18, 4.2547375e-09, 3.7520964e-04, 8.3178696e-07,\n",
            "        9.9962389e-01, 4.1190863e-16]], dtype=float32), array([[5.6226963e-06, 7.8302356e-12, 3.4626642e-01, 1.7236570e-09,\n",
            "        2.8505930e-08, 6.5372795e-01]], dtype=float32), array([[4.8676721e-34, 6.1387016e-21, 2.4849456e-15, 9.9999917e-01,\n",
            "        1.3120615e-12, 8.6990610e-07]], dtype=float32), array([[9.4606889e-28, 1.1678436e-03, 1.4816331e-06, 5.2535319e-04,\n",
            "        9.9830532e-01, 5.9972341e-17]], dtype=float32), array([[4.5442313e-38, 0.0000000e+00, 1.7421946e-37, 2.5877109e-18,\n",
            "        4.3574797e-14, 1.0000000e+00]], dtype=float32), array([[1.45595706e-07, 1.33459155e-11, 2.24782622e-13, 7.46577200e-09,\n",
            "        9.99999404e-01, 5.18834838e-07]], dtype=float32), array([[3.8885254e-27, 1.8995491e-23, 0.0000000e+00, 2.7389978e-21,\n",
            "        1.2678792e-17, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 5.4693811e-38, 3.3973237e-16, 1.1142597e-05,\n",
            "        9.9998891e-01, 1.9130182e-22]], dtype=float32), array([[9.3332855e-25, 7.1140647e-07, 4.3692898e-16, 1.0273432e-09,\n",
            "        3.6305688e-16, 9.9999928e-01]], dtype=float32), array([[5.1381829e-08, 1.2133760e-05, 9.9996960e-01, 1.0445281e-05,\n",
            "        7.6693832e-06, 7.6514326e-08]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 7.2115435e-34), (40, 5.7658164e-35), (50, 4.624633e-26), (60, 0.0), (80, 5.7426266e-16), (100, 1.0)]\n",
            "dropout :  [(0, 2.164028e-23), (0.1, 7.613341e-08), (0.2, 3.859587e-11), (0.3, 1.6808502e-18), (0.4, 9.3623953e-07), (0.5, 0.9999989)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 8.9289995e-18), (40, 4.2547375e-09), (50, 0.00037520964), (60, 8.3178696e-07), (80, 0.9996239), (100, 4.1190863e-16)]\n",
            "dropout :  [(0, 5.6226963e-06), (0.1, 7.830236e-12), (0.2, 0.34626642), (0.3, 1.723657e-09), (0.4, 2.850593e-08), (0.5, 0.65372795)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 4.867672e-34), (40, 6.1387016e-21), (50, 2.4849456e-15), (60, 0.99999917), (80, 1.3120615e-12), (100, 8.699061e-07)]\n",
            "dropout :  [(0, 9.460689e-28), (0.1, 0.0011678436), (0.2, 1.4816331e-06), (0.3, 0.0005253532), (0.4, 0.9983053), (0.5, 5.997234e-17)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 4.5442313e-38), (40, 0.0), (50, 1.7421946e-37), (60, 2.5877109e-18), (80, 4.3574797e-14), (100, 1.0)]\n",
            "dropout :  [(0, 1.455957e-07), (0.1, 1.33459155e-11), (0.2, 2.2478262e-13), (0.3, 7.465772e-09), (0.4, 0.9999994), (0.5, 5.1883484e-07)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 3.8885254e-27), (40, 1.8995491e-23), (50, 0.0), (60, 2.7389978e-21), (80, 1.2678792e-17), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 5.469381e-38), (0.2, 3.3973237e-16), (0.3, 1.1142597e-05), (0.4, 0.9999889), (0.5, 1.9130182e-22)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 9.333286e-25), (40, 7.1140647e-07), (50, 4.3692898e-16), (60, 1.0273432e-09), (80, 3.6305688e-16), (100, 0.9999993)]\n",
            "dropout :  [(0, 5.138183e-08), (0.1, 1.213376e-05), (0.2, 0.9999696), (0.3, 1.0445281e-05), (0.4, 7.669383e-06), (0.5, 7.6514326e-08)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1403\n",
            "Epoch 00001: val_loss improved from inf to 1.07894, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1403 - val_loss: 1.0789\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0410\n",
            "Epoch 00002: val_loss improved from 1.07894 to 0.79105, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0410 - val_loss: 0.7910\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0278\n",
            "Epoch 00003: val_loss did not improve from 0.79105\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0278 - val_loss: 0.7987\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0219\n",
            "Epoch 00004: val_loss improved from 0.79105 to 0.68976, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0219 - val_loss: 0.6898\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00005: val_loss improved from 0.68976 to 0.58109, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0199 - val_loss: 0.5811\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00006: val_loss improved from 0.58109 to 0.50644, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0184 - val_loss: 0.5064\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0179\n",
            "Epoch 00007: val_loss did not improve from 0.50644\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0179 - val_loss: 0.5797\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0169\n",
            "Epoch 00008: val_loss did not improve from 0.50644\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0169 - val_loss: 0.6410\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0164\n",
            "Epoch 00009: val_loss did not improve from 0.50644\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0164 - val_loss: 0.6464\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0168\n",
            "Epoch 00010: val_loss did not improve from 0.50644\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0168 - val_loss: 0.6119\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5064443451127195\n",
            "Total reward :  24.400000000000077\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.5, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 244: Controller loss : 82.824562\n",
            "\n",
            "Prediction action from Controller\n",
            "State input to Controller for Action :  [7.2115435e-34 5.7658164e-35 4.6246330e-26 0.0000000e+00 5.7426266e-16\n",
            " 1.0000000e+00]\n",
            "trying these actions:  [array([[7.7018076e-36, 6.2151892e-37, 5.0415184e-28, 0.0000000e+00,\n",
            "        6.4291240e-18, 1.0000000e+00]], dtype=float32), array([[2.7412916e-25, 1.0685784e-09, 5.4250224e-13, 2.3022755e-20,\n",
            "        1.3467098e-08, 1.0000000e+00]], dtype=float32), array([[4.9892581e-20, 2.5432160e-11, 2.3564698e-06, 5.2554632e-09,\n",
            "        9.9999762e-01, 2.6380179e-18]], dtype=float32), array([[3.3934732e-08, 4.8848902e-14, 1.1638204e-03, 1.1412183e-11,\n",
            "        1.8950579e-10, 9.9883622e-01]], dtype=float32), array([[6.6762504e-36, 8.4313573e-23, 3.5960142e-17, 1.0000000e+00,\n",
            "        1.8838192e-14, 1.2734836e-08]], dtype=float32), array([[4.3363836e-30, 5.3970302e-06, 7.0576220e-09, 2.4917645e-06,\n",
            "        9.9999213e-01, 2.8770244e-19]], dtype=float32), array([[0.000000e+00, 0.000000e+00, 0.000000e+00, 2.308396e-20,\n",
            "        3.991725e-16, 1.000000e+00]], dtype=float32), array([[8.7137675e-10, 8.1801916e-14, 1.4632125e-15, 4.8748134e-11,\n",
            "        1.0000000e+00, 3.3419600e-09]], dtype=float32), array([[3.0778341e-29, 1.7101250e-25, 0.0000000e+00, 2.5309733e-23,\n",
            "        1.1331505e-19, 1.0000000e+00]], dtype=float32), array([[0.0000000e+00, 0.0000000e+00, 2.8806393e-18, 9.7140237e-08,\n",
            "        9.9999988e-01, 1.6555579e-24]], dtype=float32), array([[6.5343498e-27, 5.3542264e-09, 3.3276080e-18, 7.9703084e-12,\n",
            "        2.7983667e-18, 1.0000000e+00]], dtype=float32), array([[7.5930928e-10, 1.9551130e-07, 9.9999952e-01, 1.6997986e-07,\n",
            "        1.2625664e-07, 1.2406507e-09]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 7.7018076e-36), (40, 6.215189e-37), (50, 5.0415184e-28), (60, 0.0), (80, 6.429124e-18), (100, 1.0)]\n",
            "dropout :  [(0, 2.7412916e-25), (0.1, 1.0685784e-09), (0.2, 5.4250224e-13), (0.3, 2.3022755e-20), (0.4, 1.3467098e-08), (0.5, 1.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 4.989258e-20), (40, 2.543216e-11), (50, 2.3564698e-06), (60, 5.255463e-09), (80, 0.9999976), (100, 2.638018e-18)]\n",
            "dropout :  [(0, 3.3934732e-08), (0.1, 4.88489e-14), (0.2, 0.0011638204), (0.3, 1.1412183e-11), (0.4, 1.8950579e-10), (0.5, 0.9988362)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 6.6762504e-36), (40, 8.4313573e-23), (50, 3.5960142e-17), (60, 1.0), (80, 1.8838192e-14), (100, 1.2734836e-08)]\n",
            "dropout :  [(0, 4.3363836e-30), (0.1, 5.3970302e-06), (0.2, 7.057622e-09), (0.3, 2.4917645e-06), (0.4, 0.99999213), (0.5, 2.8770244e-19)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 2.308396e-20), (80, 3.991725e-16), (100, 1.0)]\n",
            "dropout :  [(0, 8.7137675e-10), (0.1, 8.1801916e-14), (0.2, 1.4632125e-15), (0.3, 4.8748134e-11), (0.4, 1.0), (0.5, 3.34196e-09)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 3.077834e-29), (40, 1.710125e-25), (50, 0.0), (60, 2.5309733e-23), (80, 1.1331505e-19), (100, 1.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 2.8806393e-18), (0.3, 9.714024e-08), (0.4, 0.9999999), (0.5, 1.655558e-24)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 6.53435e-27), (40, 5.3542264e-09), (50, 3.327608e-18), (60, 7.9703084e-12), (80, 2.7983667e-18), (100, 1.0)]\n",
            "dropout :  [(0, 7.593093e-10), (0.1, 1.955113e-07), (0.2, 0.9999995), (0.3, 1.6997986e-07), (0.4, 1.2625664e-07), (0.5, 1.2406507e-09)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1827\n",
            "Epoch 00001: val_loss improved from inf to 1.26059, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.1827 - val_loss: 1.2606\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1023\n",
            "Epoch 00002: val_loss improved from 1.26059 to 1.08456, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.1023 - val_loss: 1.0846\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0472\n",
            "Epoch 00003: val_loss improved from 1.08456 to 0.89929, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0472 - val_loss: 0.8993\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0385\n",
            "Epoch 00004: val_loss did not improve from 0.89929\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0385 - val_loss: 1.0608\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0336\n",
            "Epoch 00005: val_loss improved from 0.89929 to 0.77648, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0336 - val_loss: 0.7765\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0272\n",
            "Epoch 00006: val_loss did not improve from 0.77648\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0272 - val_loss: 0.8717\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0226\n",
            "Epoch 00007: val_loss improved from 0.77648 to 0.69158, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0226 - val_loss: 0.6916\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0189\n",
            "Epoch 00008: val_loss improved from 0.69158 to 0.66563, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0189 - val_loss: 0.6656\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00009: val_loss improved from 0.66563 to 0.56274, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0177 - val_loss: 0.5627\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0178\n",
            "Epoch 00010: val_loss did not improve from 0.56274\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0178 - val_loss: 0.6103\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.5627352001879989\n",
            "Total reward :  24.500000000000078\n",
            "State input to Controller for training :  [0 0 0 0 0 1]\n",
            "Training RNN (States ip) :  [100, 0.5, 80, 0.5, 60, 0.4, 100, 0.4, 100, 0.4, 100, 0.2]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 245: Controller loss : 81.064163\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [20, 0.2, 20, 0, 100, 0.2, 20, 0.3, 80, 0.4, 40, 0.3]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2039\n",
            "Epoch 00001: val_loss improved from inf to 0.76506, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.2039 - val_loss: 0.7651\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0729\n",
            "Epoch 00002: val_loss did not improve from 0.76506\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0729 - val_loss: 1.3211\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0589\n",
            "Epoch 00003: val_loss did not improve from 0.76506\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0589 - val_loss: 1.0143\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0384\n",
            "Epoch 00004: val_loss did not improve from 0.76506\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0384 - val_loss: 0.9567\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0343\n",
            "Epoch 00005: val_loss did not improve from 0.76506\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0343 - val_loss: 1.0156\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0275\n",
            "Epoch 00006: val_loss did not improve from 0.76506\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0275 - val_loss: 0.7706\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0225\n",
            "Epoch 00007: val_loss did not improve from 0.76506\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0225 - val_loss: 0.7805\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0195\n",
            "Epoch 00008: val_loss improved from 0.76506 to 0.64024, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0195 - val_loss: 0.6402\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00009: val_loss did not improve from 0.64024\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.6475\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0186\n",
            "Epoch 00010: val_loss did not improve from 0.64024\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0186 - val_loss: 0.6752\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6402442107273343\n",
            "Total reward :  24.60000000000008\n",
            "State input to Controller for training :  [1 0 0 0 0 0]\n",
            "Training RNN (States ip) :  [20, 0.2, 20, 0, 100, 0.2, 20, 0.3, 80, 0.4, 40, 0.3]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 246: Controller loss : 1115.821533\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.2, 20, 0.5, 80, 0.4, 50, 0.3, 40, 0.2, 50, 0.5]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1968\n",
            "Epoch 00001: val_loss improved from inf to 0.82686, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1968 - val_loss: 0.8269\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0595\n",
            "Epoch 00002: val_loss did not improve from 0.82686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0595 - val_loss: 1.3450\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00003: val_loss did not improve from 0.82686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0537 - val_loss: 0.9217\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0435\n",
            "Epoch 00004: val_loss did not improve from 0.82686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0435 - val_loss: 1.0623\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0389\n",
            "Epoch 00005: val_loss did not improve from 0.82686\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0389 - val_loss: 0.9421\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0268\n",
            "Epoch 00006: val_loss improved from 0.82686 to 0.82015, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0268 - val_loss: 0.8201\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0203\n",
            "Epoch 00007: val_loss improved from 0.82015 to 0.76847, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0203 - val_loss: 0.7685\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0183\n",
            "Epoch 00008: val_loss improved from 0.76847 to 0.69461, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0183 - val_loss: 0.6946\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0199\n",
            "Epoch 00009: val_loss improved from 0.69461 to 0.69367, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0199 - val_loss: 0.6937\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0177\n",
            "Epoch 00010: val_loss did not improve from 0.69367\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0177 - val_loss: 0.7569\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6936733120147439\n",
            "Total reward :  24.70000000000008\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.2, 20, 0.5, 80, 0.4, 50, 0.3, 40, 0.2, 50, 0.5]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 247: Controller loss : 1261.238770\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 1.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [40, 0.1, 60, 0.1, 100, 0.4, 80, 0.4, 20, 0, 80, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1883\n",
            "Epoch 00001: val_loss improved from inf to 0.78087, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 5ms/sample - loss: 0.1883 - val_loss: 0.7809\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0513\n",
            "Epoch 00002: val_loss did not improve from 0.78087\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0513 - val_loss: 1.1536\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0375\n",
            "Epoch 00003: val_loss did not improve from 0.78087\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0375 - val_loss: 0.8664\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0308\n",
            "Epoch 00004: val_loss did not improve from 0.78087\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0308 - val_loss: 1.0059\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0248\n",
            "Epoch 00005: val_loss improved from 0.78087 to 0.77823, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0248 - val_loss: 0.7782\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00006: val_loss did not improve from 0.77823\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.7909\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00007: val_loss improved from 0.77823 to 0.68284, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.6828\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0181\n",
            "Epoch 00008: val_loss did not improve from 0.68284\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0181 - val_loss: 0.6948\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0174\n",
            "Epoch 00009: val_loss did not improve from 0.68284\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0174 - val_loss: 0.7660\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0158\n",
            "Epoch 00010: val_loss did not improve from 0.68284\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0158 - val_loss: 0.7494\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6828427512141874\n",
            "Total reward :  24.800000000000082\n",
            "State input to Controller for training :  [0 2 0 0 0 0]\n",
            "Training RNN (States ip) :  [40, 0.1, 60, 0.1, 100, 0.4, 80, 0.4, 20, 0, 80, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 248: Controller loss : 1183.394775\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 3., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 2.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 3.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 3.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [50, 0.2, 60, 0.4, 50, 0.2, 40, 0.3, 50, 0.2, 60, 0.4]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.1993\n",
            "Epoch 00001: val_loss improved from inf to 0.67801, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 3s 4ms/sample - loss: 0.1993 - val_loss: 0.6780\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0717\n",
            "Epoch 00002: val_loss did not improve from 0.67801\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0717 - val_loss: 1.2363\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0534\n",
            "Epoch 00003: val_loss did not improve from 0.67801\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0534 - val_loss: 0.9592\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0397\n",
            "Epoch 00004: val_loss did not improve from 0.67801\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0397 - val_loss: 0.9073\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0321\n",
            "Epoch 00005: val_loss did not improve from 0.67801\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0321 - val_loss: 0.9658\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0251\n",
            "Epoch 00006: val_loss did not improve from 0.67801\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0251 - val_loss: 0.7423\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0184\n",
            "Epoch 00007: val_loss did not improve from 0.67801\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0184 - val_loss: 0.7306\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00008: val_loss improved from 0.67801 to 0.62822, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.6282\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0194\n",
            "Epoch 00009: val_loss did not improve from 0.62822\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0194 - val_loss: 0.6520\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0188\n",
            "Epoch 00010: val_loss did not improve from 0.62822\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0188 - val_loss: 0.6693\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.6282179438730211\n",
            "Total reward :  24.900000000000084\n",
            "State input to Controller for training :  [0 0 3 0 0 0]\n",
            "Training RNN (States ip) :  [50, 0.2, 60, 0.4, 50, 0.2, 40, 0.3, 50, 0.2, 60, 0.4]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 249: Controller loss : 1580.574341\n",
            "\n",
            "Generating random action to explore\n",
            "trying these actions:  [array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 0., 6.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[1., 0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 5., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 4., 0., 0.]], dtype=float32), array([[0., 2., 0., 0., 0., 0.]], dtype=float32)]\n",
            "Actions :\n",
            "******************** Layer 1 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 6.0)]\n",
            "******************** Layer 2 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 6.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 0.0), (0.4, 5.0), (0.5, 0.0)]\n",
            "******************** Layer 3 ********************\n",
            "unit :  [(20, 1.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 0.0), (0.2, 0.0), (0.3, 4.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 4 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 5 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 0.0), (80, 5.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "******************** Layer 6 ********************\n",
            "unit :  [(20, 0.0), (40, 0.0), (50, 0.0), (60, 4.0), (80, 0.0), (100, 0.0)]\n",
            "dropout :  [(0, 0.0), (0.1, 2.0), (0.2, 0.0), (0.3, 0.0), (0.4, 0.0), (0.5, 0.0)]\n",
            "\n",
            "Predicted actions :  [100, 0.5, 100, 0.4, 20, 0.3, 80, 0.1, 80, 0.1, 60, 0.1]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Train on 740 samples, validate on 459 samples\n",
            "Epoch 1/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.2117\n",
            "Epoch 00001: val_loss improved from inf to 0.78637, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 4s 5ms/sample - loss: 0.2117 - val_loss: 0.7864\n",
            "Epoch 2/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0702\n",
            "Epoch 00002: val_loss did not improve from 0.78637\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0702 - val_loss: 1.3283\n",
            "Epoch 3/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0542\n",
            "Epoch 00003: val_loss did not improve from 0.78637\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0542 - val_loss: 0.8742\n",
            "Epoch 4/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0394\n",
            "Epoch 00004: val_loss did not improve from 0.78637\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0394 - val_loss: 1.0876\n",
            "Epoch 5/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0324\n",
            "Epoch 00005: val_loss did not improve from 0.78637\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0324 - val_loss: 0.9123\n",
            "Epoch 6/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0254\n",
            "Epoch 00006: val_loss did not improve from 0.78637\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0254 - val_loss: 0.8629\n",
            "Epoch 7/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0197\n",
            "Epoch 00007: val_loss improved from 0.78637 to 0.77086, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 3ms/sample - loss: 0.0197 - val_loss: 0.7709\n",
            "Epoch 8/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0185\n",
            "Epoch 00008: val_loss did not improve from 0.77086\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0185 - val_loss: 0.7713\n",
            "Epoch 9/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0180\n",
            "Epoch 00009: val_loss improved from 0.77086 to 0.73014, saving model to weights/temp_network.h5\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0180 - val_loss: 0.7301\n",
            "Epoch 10/10\n",
            "740/740 [==============================] - ETA: 0s - loss: 0.0175\n",
            "Epoch 00010: val_loss did not improve from 0.73014\n",
            "740/740 [==============================] - 2s 2ms/sample - loss: 0.0175 - val_loss: 0.7651\n",
            "\n",
            "Manager: EWA Loss =  inf\n",
            "Rewards :  0.1 Loss :  0.7301424843033933\n",
            "Total reward :  25.000000000000085\n",
            "State input to Controller for training :  [0 0 0 0 0 6]\n",
            "Training RNN (States ip) :  [100, 0.5, 100, 0.4, 20, 0.3, 80, 0.1, 80, 0.1, 60, 0.1]\n",
            "Training RNN (Reward ip) :  [0.1]\n",
            "Trial 250: Controller loss : 1262.985962\n",
            "\n",
            "Total Reward :  25.000000000000085\n",
            "Time taken to run:5581.277840267\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# create a shared session between Keras and Tensorflow\n",
        "policy_sess = tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session(policy_sess)\n",
        "\n",
        "NUM_LAYERS = 6  # number of layers of the state space\n",
        "MAX_TRIALS = 250  # maximum number of models generated\n",
        "\n",
        "MAX_EPOCHS = 10  # maximum number of epochs to train\n",
        "CHILD_BATCHSIZE = 128  # batchsize of the child models\n",
        "EXPLORATION = 0.8  # high exploration for the first 1000 steps\n",
        "REGULARIZATION = 1e-3  # regularization strength\n",
        "CONTROLLER_CELLS = 32  # number of cells in RNN controller\n",
        "EMBEDDING_DIM = 20  # dimension of the embeddings for each state\n",
        "ACCURACY_BETA = 0.8  # beta value for the moving average of the accuracy\n",
        "CLIP_REWARDS = 0.0  # clip rewards in the [-0.05, 0.05] range\n",
        "RESTORE_CONTROLLER = True  # restore controller to continue training\n",
        "\n",
        "# construct a state space\n",
        "state_space = StateSpace()\n",
        "\n",
        "# add states\n",
        "state_space.add_state(name='unit', values=[20, 40, 50, 60, 80, 100])\n",
        "state_space.add_state(name='dropout', values=[0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "\n",
        "# print the state space being searched\n",
        "state_space.print_state_space()\n",
        "\n",
        "\"\"\"\n",
        "# prepare the training data for the NetworkManager\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "print(y_test.shape)\n",
        "\"\"\"\n",
        "\n",
        "dataset = [X_train, y_train, X_test, y_test]  # pack the dataset for the NetworkManager\n",
        "\n",
        "previous_loss = 10000000\n",
        "total_reward = 0.0\n",
        "\n",
        "with policy_sess.as_default():\n",
        "    # create the Controller and build the internal policy network\n",
        "    controller = Controller(policy_sess, NUM_LAYERS, state_space,\n",
        "                            reg_param=REGULARIZATION,\n",
        "                            exploration=EXPLORATION,\n",
        "                            controller_cells=CONTROLLER_CELLS,\n",
        "                            embedding_dim=EMBEDDING_DIM,\n",
        "                            restore_controller=RESTORE_CONTROLLER)\n",
        "\n",
        "# create the Network Manager\n",
        "manager = NetworkManager(dataset, epochs=MAX_EPOCHS, child_batchsize=CHILD_BATCHSIZE, clip_rewards=CLIP_REWARDS,\n",
        "                         acc_beta=ACCURACY_BETA)\n",
        "\n",
        "# get an initial random state space if controller needs to predict an\n",
        "# action from the initial state\n",
        "state = state_space.get_random_state_space(NUM_LAYERS)\n",
        "print(\"Initial Random State : \", state_space.parse_state_space_list(state))\n",
        "print()\n",
        "\n",
        "# clear the previous files\n",
        "controller.remove_files()\n",
        "\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "# train for number of trails\n",
        "for trial in range(MAX_TRIALS):\n",
        "    with policy_sess.as_default():\n",
        "        tf.compat.v1.keras.backend.set_session(policy_sess)\n",
        "        actions = controller.get_action(state)  # get an action for the previous state\n",
        "\n",
        "    print(\"trying these actions: \", actions)\n",
        "    # print the action probabilities\n",
        "    state_space.print_actions(actions)\n",
        "    print(\"Predicted actions : \", state_space.parse_state_space_list(actions))\n",
        "\n",
        "    # build a model, train and get reward and accuracy from the network manager\n",
        "    reward, previous_loss = manager.get_rewards(model_fn, state_space.parse_state_space_list(actions))\n",
        "    print(\"Rewards : \", reward, \"Loss : \", previous_loss)\n",
        "\n",
        "    with policy_sess.as_default():\n",
        "        tf.compat.v1.keras.backend.set_session(policy_sess)\n",
        "\n",
        "        total_reward += reward\n",
        "        print(\"Total reward : \", total_reward)\n",
        "\n",
        "        # actions and states are equivalent, save the state and reward\n",
        "        state = actions\n",
        "        controller.store_rollout(state, reward)\n",
        "\n",
        "        # train the controller on the saved state and the discounted rewards\n",
        "        loss = controller.train_step()\n",
        "        print(\"Trial %d: Controller loss : %0.6f\" % (trial + 1, loss))\n",
        "\n",
        "        # write the results of this trial into a file\n",
        "        with open('train_history.csv', mode='a+') as f:\n",
        "            data = [previous_loss, reward]\n",
        "            data.extend(state_space.parse_state_space_list(state))\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(data)\n",
        "    print()\n",
        "\n",
        "print(\"Total Reward : \", total_reward)\n",
        "\n",
        "t2 = time.perf_counter()\n",
        "time = t2-t1\n",
        "time_taken = 'Time taken to run:' + str(t2-t1)\n",
        "print(time_taken)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RL-NAS-6.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}