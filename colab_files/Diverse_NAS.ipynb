{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Argument Parsing"
      ],
      "metadata": {
        "id": "G_TQ8eI2P-Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--num_layers', type=int, default=8)\n",
        "parser.add_argument('--max_trials', type=int, default=300)\n",
        "parser.add_argument('--max_epochs', type=int, default=10)\n",
        "parser.add_argument('--child_batchsize', type=int, default=128)\n",
        "parser.add_argument('--exploration', type=float, default=0.8)\n",
        "parser.add_argument('--regularization', type=float, default=1e-3)\n",
        "parser.add_argument('--controller_cells', type=int, default=32)\n",
        "parser.add_argument('--embedding_dim', type=int, default=20)\n",
        "parser.add_argument('--accuracy_beta', type=float, default=0.8)\n",
        "parser.add_argument('--clip_rewards', type=float, default=0.0)\n",
        "parser.add_argument('--restore_controller', type=bool, default=True)\n",
        "parser.add_argument('--sequence_length', type=int, default=5)\n",
        "parser.add_argument('--batch_size', type=int, default=32)\n",
        "parser.add_argument('--num_features', type=int, default=1)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "NUM_LAYERS = args.num_layers\n",
        "MAX_TRIALS = args.max_trials\n",
        "\n",
        "MAX_EPOCHS = args.max_epochs\n",
        "CHILD_BATCHSIZE = args.child_batchsize\n",
        "EXPLORATION = args.exploration\n",
        "REGULARIZATION = args.regularization\n",
        "CONTROLLER_CELLS = args.controller_cells\n",
        "EMBEDDING_DIM = args.embedding_dim\n",
        "ACCURACY_BETA = args.accuracy_beta\n",
        "CLIP_REWARDS = args.clip_rewards\n",
        "RESTORE_CONTROLLER = args.restore_controller\n",
        "\n",
        "SEQUENCE_LENGTH = args.sequence_length\n",
        "BATCH_SIZE = args.batch_size\n",
        "NUM_FEATURES = args.num_features\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Remove below in script, they are only for colab\n",
        "SEQUENCE_LENGTH = 5\n",
        "BATCH_SIZE = 32\n",
        "NUM_FEATURES = 1\n",
        "\n",
        "NUM_LAYERS = 8  # number of layers of the state space\n",
        "MAX_TRIALS = 300  # maximum number of models generated\n",
        "\n",
        "MAX_EPOCHS = 10  # maximum number of epochs to train\n",
        "CHILD_BATCHSIZE = 128  # batchsize of the child models\n",
        "EXPLORATION = 0.8  # high exploration for the first 1000 steps\n",
        "REGULARIZATION = 1e-3  # regularization strength\n",
        "CONTROLLER_CELLS = 32  # number of cells in RNN controller\n",
        "EMBEDDING_DIM = 20  # dimension of the embeddings for each state\n",
        "ACCURACY_BETA = 0.8  # beta value for the moving average of the accuracy\n",
        "CLIP_REWARDS = 0.0  # clip rewards in the [-0.05, 0.05] range\n",
        "RESTORE_CONTROLLER = True  # restore controller to continue training"
      ],
      "metadata": {
        "id": "amd_2wFJQANj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDj9It95WwZZ"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoHNbvqPXP0i"
      },
      "source": [
        "## Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "siuTPLAHWlnh",
        "outputId": "794f674c-ff4b-4bc7-ae55-fd185f1ea66d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 136\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haGp_TVoWqN8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv(\"/content/drive/My Drive/6998/project/TSLA.csv\")\n",
        "print(\"Number of rows and columns:\", df.shape)\n",
        "df.head(5)\n",
        "\n",
        "training_set = df.iloc[:800, 1:2].values\n",
        "test_set = df.iloc[800:, 1:2].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCP3A5IlWvYD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Feature Scaling\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        "# Creating a data structure with 60 time-steps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(SEQUENCE_LENGTH, 800):\n",
        "    X_train.append(training_set_scaled[i-SEQUENCE_LENGTH:i, 0])\n",
        "    y_train.append(training_set_scaled[i, 0])\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], NUM_FEATURES))\n",
        "\n",
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maC-G6XPXRc_"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FBz8dXDXOuB"
      },
      "outputs": [],
      "source": [
        "# Getting the predicted stock price of 2017\n",
        "dataset_train = df.iloc[:800, 1:2]\n",
        "dataset_test = df.iloc[800:, 1:2]\n",
        "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs)\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for i in range(SEQUENCE_LENGTH, 519):\n",
        "    X_test.append(inputs[i-SEQUENCE_LENGTH:i, 0])\n",
        "    y_test.append(inputs[i, 0])\n",
        "\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], NUM_FEATURES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp6l6etXWzra"
      },
      "source": [
        "# Building Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZIWoAIkp4MU"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        " \n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR5FXSjtYBFd"
      },
      "source": [
        "## StateSpace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_search_space(i, sample):\n",
        "  # For Dense add number of units \n",
        "  if sample == 0:\n",
        "    return []\n",
        "  elif sample == 1:\n",
        "    return [i + sample]\n",
        "  # For LSTM add number of units and activation\n",
        "  elif sample == 2:\n",
        "    return [i + sample, i + sample + 1] \n",
        "  # For Dropout add percentage of dropout\n",
        "  elif sample == 3:\n",
        "    return [i + sample + 1]\n",
        "  # For GRU add number of units and activation \n",
        "  elif sample == 4:\n",
        "    return [i + sample + 1, i + sample + 2] "
      ],
      "metadata": {
        "id": "wc6_rrKD_LiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upB0m8LTon6y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pprint\n",
        "from collections import OrderedDict\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "import os\n",
        "if not os.path.exists('weights/'):\n",
        "    os.makedirs('weights/')\n",
        "\n",
        "class StateSpace:\n",
        "    '''\n",
        "    State Space manager\n",
        "\n",
        "    Provides utilit functions for holding \"states\" / \"actions\" that the controller\n",
        "    must use to train and predict.\n",
        "\n",
        "    Also provides a more convenient way to define the search space\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.states = OrderedDict()\n",
        "        self.state_count_ = 0\n",
        "\n",
        "    def add_state(self, name, values):\n",
        "        '''\n",
        "        Adds a \"state\" to the state manager, along with some metadata for efficient\n",
        "        packing and unpacking of information required by the RNN Controller.\n",
        "\n",
        "        Stores metadata such as:\n",
        "        -   Global ID\n",
        "        -   Name\n",
        "        -   Valid Values\n",
        "        -   Number of valid values possible\n",
        "        -   Map from value ID to state value\n",
        "        -   Map from state value to value ID\n",
        "\n",
        "        Args:\n",
        "            name: name of the state / action\n",
        "            values: valid values that this state can take\n",
        "\n",
        "        Returns:\n",
        "            Global ID of the state. Can be used to refer to this state later.\n",
        "        '''\n",
        "        index_map = {0:0}\n",
        "        for i, val in enumerate(values):\n",
        "            index_map[i+1] = val\n",
        "\n",
        "        value_map = {0:0}\n",
        "        for i, val in enumerate(values):\n",
        "            value_map[val] = i+1\n",
        "\n",
        "        metadata = {\n",
        "            'id': self.state_count_,\n",
        "            'name': name,\n",
        "            'values': values,\n",
        "            'size': len(values) + 1,\n",
        "            'index_map_': index_map,\n",
        "            'value_map_': value_map,\n",
        "        }\n",
        "        self.states[self.state_count_] = metadata\n",
        "        self.state_count_ += 1\n",
        "\n",
        "        return self.state_count_ - 1\n",
        "\n",
        "    def embedding_encode(self, id, value):\n",
        "        '''\n",
        "        Embedding index encode the specific state value\n",
        "\n",
        "        Args:\n",
        "            id: global id of the state\n",
        "            value: state value\n",
        "\n",
        "        Returns:\n",
        "            embedding encoded representation of the state value\n",
        "        '''\n",
        "        state = self[id]\n",
        "        size = state['size']\n",
        "        value_map = state['value_map_']\n",
        "        value_idx = value_map[value]\n",
        "\n",
        "        one_hot = np.zeros((1, size), dtype=np.float32)\n",
        "        one_hot[np.arange(1), value_idx] = value_idx + 1\n",
        "        return one_hot\n",
        "\n",
        "    def get_state_value(self, id, index):\n",
        "        '''\n",
        "        Retrieves the state value from the state value ID\n",
        "\n",
        "        Args:\n",
        "            id: global id of the state\n",
        "            index: index of the state value (usually from argmax)\n",
        "\n",
        "        Returns:\n",
        "            The actual state value at given value index\n",
        "        '''\n",
        "        state = self[id]\n",
        "        index_map = state['index_map_']\n",
        "\n",
        "        if (type(index) == list or type(index) == np.ndarray) and len(index) == 1:\n",
        "            index = index[0]\n",
        "\n",
        "        value = index_map[index]\n",
        "        return value\n",
        "\n",
        "    def get_random_state_space(self, num_layers):\n",
        "        '''\n",
        "        Constructs a random initial state space for feeding as an initial value\n",
        "        to the Controller RNN\n",
        "\n",
        "        Args:\n",
        "            num_layers: number of layers to duplicate the search space\n",
        "\n",
        "        Returns:\n",
        "            A list of one hot encoded states\n",
        "        '''\n",
        "        states = []\n",
        "        num_left = 1000000\n",
        "        to_add = []\n",
        "\n",
        "        for id in range(self.size * num_layers):\n",
        "            if id == 0:\n",
        "              state = self[id]\n",
        "              size = state['size']\n",
        "              sample = np.random.choice(size - 1, size=1)\n",
        "              sample = state['index_map_'][sample[0] + 1]\n",
        "              state = self.embedding_encode(id, sample)\n",
        "              states.append(state)\n",
        "              num_left = sample * (self.size - 1)\n",
        "            elif id % self.size == 0:\n",
        "              state = self.embedding_encode(id, 0)\n",
        "              states.append(state)\n",
        "            elif id % self.size == 1 and num_left > 0:\n",
        "              state = self[id]\n",
        "              size = state['size']\n",
        "              sample = np.random.choice(size - 1, size=1)\n",
        "              sample = state['index_map_'][sample[0] + 1]\n",
        "              to_add = add_to_search_space(id, sample)\n",
        "              state = self.embedding_encode(id, sample)\n",
        "              states.append(state)\n",
        "              num_left -= 1\n",
        "            else:\n",
        "              num_left -= 1\n",
        "              if num_left < 0 or id not in to_add:\n",
        "                state = self.embedding_encode(id, 0)\n",
        "                states.append(state)\n",
        "              else:\n",
        "                state = self[id]\n",
        "                size = state['size']\n",
        "\n",
        "                sample = np.random.choice(size - 1, size=1)\n",
        "                sample = state['index_map_'][sample[0] + 1]\n",
        "\n",
        "                state = self.embedding_encode(id, sample)\n",
        "                states.append(state)\n",
        "        return states\n",
        "\n",
        "    def parse_state_space_list(self, state_list):\n",
        "        '''\n",
        "        Parses a list of one hot encoded states to retrieve a list of state values\n",
        "\n",
        "        Args:\n",
        "            state_list: list of one hot encoded states\n",
        "\n",
        "        Returns:\n",
        "            list of state values\n",
        "        '''\n",
        "        state_values = []\n",
        "        for id, state_one_hot in enumerate(state_list):\n",
        "            state_val_idx = np.argmax(state_one_hot, axis=-1)[0]\n",
        "            value = self.get_state_value(id, state_val_idx)\n",
        "            state_values.append(value)\n",
        "\n",
        "        return state_values\n",
        "\n",
        "    def print_state_space(self):\n",
        "        ''' Pretty print the state space '''\n",
        "        print('*' * 40, 'STATE SPACE', '*' * 40)\n",
        "\n",
        "        pp = pprint.PrettyPrinter(indent=2, width=100)\n",
        "        for id, state in self.states.items():\n",
        "            pp.pprint(state)\n",
        "            print()\n",
        "\n",
        "    def print_actions(self, actions):\n",
        "        ''' Print the action space properly '''\n",
        "        print('Actions :')\n",
        "\n",
        "        for id, action in enumerate(actions):\n",
        "            if id % self.size == 0:\n",
        "                print(\"*\" * 20, \"Layer %d\" % (((id + 1) // self.size) + 1), \"*\" * 20)\n",
        "\n",
        "            state = self[id]\n",
        "            name = state['name']\n",
        "            vals = [(n, p) for n, p in zip(state['values'], *action)]\n",
        "            print(\"%s : \" % name, vals)\n",
        "        print()\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        return self.states[id % self.size]\n",
        "\n",
        "    @property\n",
        "    def size(self):\n",
        "        return self.state_count_\n",
        "\n",
        "\n",
        "class Controller:\n",
        "    '''\n",
        "    Utility class to manage the RNN Controller\n",
        "    '''\n",
        "    def __init__(self, policy_session, num_layers, state_space,\n",
        "                 reg_param=0.001,\n",
        "                 discount_factor=0.99,\n",
        "                 exploration=0.8,\n",
        "                 controller_cells=32,\n",
        "                 embedding_dim=20,\n",
        "                 clip_norm=0.0,\n",
        "                 restore_controller=False):\n",
        "        self.policy_session = policy_session  # type: tf.Session\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.state_space = state_space  # type: StateSpace\n",
        "        self.state_size = self.state_space.size\n",
        "\n",
        "        self.controller_cells = controller_cells\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.reg_strength = reg_param\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration = exploration\n",
        "        self.restore_controller = restore_controller\n",
        "        self.clip_norm = clip_norm\n",
        "\n",
        "        self.reward_buffer = []\n",
        "        self.state_buffer = []\n",
        "\n",
        "        self.cell_outputs = []\n",
        "        self.policy_classifiers = []\n",
        "        self.policy_actions = []\n",
        "        self.policy_labels = []\n",
        "\n",
        "        self.build_policy_network()\n",
        "\n",
        "    def get_action(self, state):\n",
        "        '''\n",
        "        Gets a one hot encoded action list, either from random sampling or from\n",
        "        the Controller RNN\n",
        "\n",
        "        Args:\n",
        "            state: a list of one hot encoded states, whose first value is used as initial\n",
        "                state for the controller RNN\n",
        "\n",
        "        Returns:\n",
        "            A one hot encoded action list\n",
        "        '''\n",
        "\n",
        "        if np.random.random() < self.exploration:\n",
        "            print(\"Generating random action to explore\")\n",
        "            actions = []\n",
        "            num_left = 1000000\n",
        "            to_add = []\n",
        "\n",
        "            for i in range(self.state_size * self.num_layers):\n",
        "                if i == 0:\n",
        "                  state_ = self.state_space[i]\n",
        "                  size = state_['size']\n",
        "\n",
        "                  sample = np.random.choice(size - 1, size=1)\n",
        "                  sample = state_['index_map_'][sample[0] + 1]\n",
        "                  action = self.state_space.embedding_encode(i, sample)\n",
        "                  actions.append(action)\n",
        "                  num_left = sample * (self.state_size - 1)\n",
        "                elif i % self.state_size == 0:\n",
        "                  action = self.state_space.embedding_encode(i, 0)\n",
        "                  actions.append(action)\n",
        "                elif i % self.state_size == 1 and num_left > 0:\n",
        "                  state_ = self.state_space[i]\n",
        "                  size = state_['size']\n",
        "                  sample = np.random.choice(size - 1, size=1)\n",
        "                  sample = state_['index_map_'][sample[0] + 1]\n",
        "                  to_add = add_to_search_space(i, sample)\n",
        "                  action = self.state_space.embedding_encode(i, sample)\n",
        "                  actions.append(action)\n",
        "                  num_left -= 1\n",
        "                else:\n",
        "                  num_left -= 1\n",
        "                  if num_left < 0 or i not in to_add:\n",
        "                    action = self.state_space.embedding_encode(i, 0)\n",
        "                    actions.append(action)\n",
        "                  else:\n",
        "                    state_ = self.state_space[i]\n",
        "                    size = state_['size']\n",
        "\n",
        "                    sample = np.random.choice(size - 1, size=1)\n",
        "                    sample = state_['index_map_'][sample[0] + 1]\n",
        "                    action = self.state_space.embedding_encode(i, sample)\n",
        "                    actions.append(action)\n",
        "            return actions\n",
        "\n",
        "        else:\n",
        "            print(\"Prediction action from Controller\")\n",
        "            initial_state = self.state_space[0]\n",
        "            size = initial_state['size']\n",
        "\n",
        "            if state[0].shape != (1, size):\n",
        "                state = state[0].reshape((1, size)).astype('int32')\n",
        "            else:\n",
        "                state = state[0]\n",
        "\n",
        "            print(\"State input to Controller for Action : \", state.flatten())\n",
        "\n",
        "            with self.policy_session.as_default():\n",
        "                tf.compat.v1.keras.backend.set_session(self.policy_session)\n",
        "\n",
        "                with tf.name_scope('action_prediction'):\n",
        "                    pred_actions = self.policy_session.run(self.policy_actions, feed_dict={self.state_input: state})\n",
        "\n",
        "                return pred_actions\n",
        "\n",
        "    def build_policy_network(self):\n",
        "        with self.policy_session.as_default():\n",
        "            tf.compat.v1.keras.backend.set_session(self.policy_session)\n",
        "\n",
        "            with tf.name_scope('controller'):\n",
        "                with tf.compat.v1.variable_scope('policy_network'):\n",
        "\n",
        "                    # state input is the first input fed into the controller RNN.\n",
        "                    # the rest of the inputs are fed to the RNN internally\n",
        "                    with tf.name_scope('state_input'):\n",
        "                        state_input = tf.placeholder(dtype=tf.int32, shape=(1, None), name='state_input')\n",
        "\n",
        "                    self.state_input = state_input\n",
        "\n",
        "                    # we can use LSTM as the controller as well\n",
        "                    nas_cell = tf.nn.rnn_cell.LSTMCell(self.controller_cells)\n",
        "                    cell_state = nas_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
        "\n",
        "                    embedding_weights = []\n",
        "\n",
        "                    # for each possible state, create a new embedding. Reuse the weights for multiple layers.\n",
        "                    with tf.compat.v1.variable_scope('embeddings', reuse=tf.AUTO_REUSE):\n",
        "                        for i in range(self.state_size):\n",
        "                            state_ = self.state_space[i]\n",
        "                            size = state_['size']\n",
        "\n",
        "                            # size + 1 is used so that 0th index is never updated and is \"default\" value\n",
        "                            weights = tf.get_variable('state_embeddings_%d' % i,\n",
        "                                                      shape=[size + 1, self.embedding_dim],\n",
        "                                                      initializer=tf.initializers.random_uniform(-1., 1.))\n",
        "\n",
        "                            embedding_weights.append(weights)\n",
        "\n",
        "                        # initially, cell input will be 1st state input\n",
        "                        embeddings = tf.nn.embedding_lookup(embedding_weights[0], state_input)\n",
        "\n",
        "                    cell_input = embeddings\n",
        "\n",
        "                    # we provide a flat list of chained input-output to the RNN\n",
        "                    for i in range(self.state_size * self.num_layers):\n",
        "                        state_id = i % self.state_size\n",
        "                        state_space = self.state_space[i]\n",
        "                        size = state_space['size']\n",
        "\n",
        "                        with tf.name_scope('controller_output_%d' % i):\n",
        "                            # feed the ith layer input (i-1 layer output) to the RNN\n",
        "                            outputs, final_state = tf.nn.dynamic_rnn(nas_cell,\n",
        "                                                                     cell_input,\n",
        "                                                                     initial_state=cell_state,\n",
        "                                                                     dtype=tf.float32)\n",
        "\n",
        "                            # add a new classifier for each layers output\n",
        "                            classifier = tf.layers.dense(outputs[:, -1, :], units=size, name='classifier_%d' % (i),\n",
        "                                                         reuse=False)\n",
        "                            preds = tf.nn.softmax(classifier)\n",
        "\n",
        "                            # feed the previous layer (i-1 layer output) to the next layers input, along with state\n",
        "                            # take the class label\n",
        "                            cell_input = tf.argmax(preds, axis=-1)\n",
        "                            cell_input = tf.expand_dims(cell_input, -1, name='pred_output_%d' % (i))\n",
        "                            cell_input = tf.cast(cell_input, tf.int32)\n",
        "                            cell_input = tf.add(cell_input, 1)  # we avoid using 0 so as to have a \"default\" embedding at 0th index\n",
        "\n",
        "                            # embedding lookup of this state using its state weights ; reuse weights\n",
        "                            cell_input = tf.nn.embedding_lookup(embedding_weights[state_id], cell_input,\n",
        "                                                           name='cell_output_%d' % (i))\n",
        "\n",
        "                            cell_state = final_state\n",
        "\n",
        "                        # store the tensors for later loss computation\n",
        "                        self.cell_outputs.append(cell_input)\n",
        "                        self.policy_classifiers.append(classifier)\n",
        "                        self.policy_actions.append(preds)\n",
        "\n",
        "            policy_net_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='policy_network')\n",
        "\n",
        "            with tf.name_scope('optimizer'):\n",
        "                self.global_step = tf.Variable(0, trainable=False)\n",
        "                starter_learning_rate = 0.1\n",
        "                learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
        "                                                           500, 0.95, staircase=True)\n",
        "\n",
        "                tf.summary.scalar('learning_rate', learning_rate)\n",
        "\n",
        "                self.optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "            with tf.name_scope('losses'):\n",
        "                self.discounted_rewards = tf.placeholder(tf.float32, shape=(None,), name='discounted_rewards')\n",
        "                tf.summary.scalar('discounted_reward', tf.reduce_sum(self.discounted_rewards))\n",
        "\n",
        "                # calculate sum of all the individual classifiers\n",
        "                cross_entropy_loss = 0\n",
        "                for i in range(self.state_size * self.num_layers):\n",
        "                    classifier = self.policy_classifiers[i]\n",
        "                    state_space = self.state_space[i]\n",
        "                    size = state_space['size']\n",
        "\n",
        "                    with tf.name_scope('state_%d' % (i + 1)):\n",
        "                        labels = tf.placeholder(dtype=tf.float32, shape=(None, size), name='cell_label_%d' % i)\n",
        "                        self.policy_labels.append(labels)\n",
        "\n",
        "                        ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=classifier, labels=labels)\n",
        "                        tf.summary.scalar('state_%d_ce_loss' % (i + 1), tf.reduce_mean(ce_loss))\n",
        "\n",
        "                    cross_entropy_loss += ce_loss\n",
        "\n",
        "                policy_gradient_loss = tf.reduce_mean(cross_entropy_loss)\n",
        "                reg_loss = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in policy_net_variables])  # Regularization\n",
        "\n",
        "                # sum up policy gradient and regularization loss\n",
        "                self.total_loss = policy_gradient_loss + self.reg_strength * reg_loss\n",
        "                tf.summary.scalar('total_loss', self.total_loss)\n",
        "\n",
        "                self.gradients = self.optimizer.compute_gradients(self.total_loss)\n",
        "\n",
        "                with tf.name_scope('policy_gradients'):\n",
        "                    # normalize gradients so that they dont explode if argument passed\n",
        "                    if self.clip_norm is not None and self.clip_norm != 0.0:\n",
        "                        norm = tf.constant(self.clip_norm, dtype=tf.float32)\n",
        "                        gradients, vars = zip(*self.gradients)  # unpack the two lists of gradients and the variables\n",
        "                        gradients, _ = tf.clip_by_global_norm(gradients, norm)  # clip by the norm\n",
        "                        self.gradients = list(zip(gradients, vars))  # we need to set values later, convert to list\n",
        "\n",
        "                    # compute policy gradients\n",
        "                    for i, (grad, var) in enumerate(self.gradients):\n",
        "                        if grad is not None:\n",
        "                            self.gradients[i] = (grad * self.discounted_rewards, var)\n",
        "\n",
        "                # training update\n",
        "                with tf.name_scope(\"train_policy_network\"):\n",
        "                    # apply gradients to update policy network\n",
        "                    self.train_op = self.optimizer.apply_gradients(self.gradients, global_step=self.global_step)\n",
        "\n",
        "            self.summaries_op = tf.summary.merge_all()\n",
        "\n",
        "            timestr = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "            filename = 'logs/%s' % timestr\n",
        "\n",
        "            self.summary_writer = tf.summary.FileWriter(filename, graph=self.policy_session.graph)\n",
        "\n",
        "            self.policy_session.run(tf.global_variables_initializer())\n",
        "            self.saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "            if self.restore_controller:\n",
        "                path = tf.train.latest_checkpoint('weights/')\n",
        "\n",
        "                if path is not None and tf.train.checkpoint_exists(path):\n",
        "                    print(\"Loading Controller Checkpoint !\")\n",
        "                    self.saver.restore(self.policy_session, path)\n",
        "\n",
        "    def store_rollout(self, state, reward):\n",
        "        self.reward_buffer.append(reward)\n",
        "        self.state_buffer.append(state)\n",
        "\n",
        "        # dump buffers to file if it grows larger than 50 items\n",
        "        if len(self.reward_buffer) > 20:\n",
        "            with open('buffers.txt', mode='a+') as f:\n",
        "                for i in range(20):\n",
        "                    state_ = self.state_buffer[i]\n",
        "                    state_list = self.state_space.parse_state_space_list(state_)\n",
        "                    state_list = ','.join(str(v) for v in state_list)\n",
        "\n",
        "                    f.write(\"%0.4f,%s\\n\" % (self.reward_buffer[i], state_list))\n",
        "\n",
        "                print(\"Saved buffers to file `buffers.txt` !\")\n",
        "\n",
        "            self.reward_buffer = [self.reward_buffer[-1]]\n",
        "            self.state_buffer = [self.state_buffer[-1]]\n",
        "\n",
        "    def discount_rewards(self):\n",
        "        '''\n",
        "        Compute discounted rewards over the entire reward buffer\n",
        "\n",
        "        Returns:\n",
        "            Discounted reward value\n",
        "        '''\n",
        "        rewards = np.asarray(self.reward_buffer)\n",
        "        discounted_rewards = np.zeros_like(rewards)\n",
        "        running_add = 0\n",
        "        for t in reversed(range(0, rewards.size)):\n",
        "            if rewards[t] != 0:\n",
        "                running_add = 0\n",
        "            running_add = running_add * self.discount_factor + rewards[t]\n",
        "            discounted_rewards[t] = running_add\n",
        "        return discounted_rewards[-1]\n",
        "\n",
        "    def train_step(self):\n",
        "        '''\n",
        "        Perform a single train step on the Controller RNN\n",
        "\n",
        "        Returns:\n",
        "            the training loss\n",
        "        '''\n",
        "        states = self.state_buffer[-1]\n",
        "        label_list = []\n",
        "\n",
        "        # parse the state space to get real value of the states,\n",
        "        # then one hot encode them for comparison with the predictions\n",
        "        state_list = self.state_space.parse_state_space_list(states)\n",
        "        for id, state_value in enumerate(state_list):\n",
        "            state_one_hot = self.state_space.embedding_encode(id, state_value)\n",
        "            label_list.append(state_one_hot)\n",
        "\n",
        "        # the initial input to the controller RNN\n",
        "        state_input_size = self.state_space[0]['size']\n",
        "        state_input = states[0].reshape((1, state_input_size)).astype('int32')\n",
        "        print(\"State input to Controller for training : \", state_input.flatten())\n",
        "\n",
        "        # the discounted reward value\n",
        "        reward = self.discount_rewards()\n",
        "        reward = np.asarray([reward]).astype('float32')\n",
        "\n",
        "        feed_dict = {\n",
        "            self.state_input: state_input,\n",
        "            self.discounted_rewards: reward\n",
        "        }\n",
        "\n",
        "        # prepare the feed dict with the values of all the policy labels for each\n",
        "        # of the Controller outputs\n",
        "        for i, label in enumerate(label_list):\n",
        "            feed_dict[self.policy_labels[i]] = label\n",
        "\n",
        "        with self.policy_session.as_default():\n",
        "            tf.compat.v1.keras.backend.set_session(self.policy_session)\n",
        "\n",
        "            print(\"Training RNN (States ip) : \", state_list)\n",
        "            print(\"Training RNN (Reward ip) : \", reward.flatten())\n",
        "            _, loss, summary, global_step = self.policy_session.run([self.train_op, self.total_loss, self.summaries_op,\n",
        "                                                                     self.global_step],\n",
        "                                                                     feed_dict=feed_dict)\n",
        "\n",
        "            self.summary_writer.add_summary(summary, global_step)\n",
        "            self.saver.save(self.policy_session, save_path='weights/controller.ckpt', global_step=self.global_step)\n",
        "\n",
        "            # reduce exploration after many train steps\n",
        "            if global_step != 0 and global_step % 20 == 0 and self.exploration > 0.5:\n",
        "                self.exploration *= 0.99\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def remove_files(self):\n",
        "        files = ['train_history.csv', 'buffers.txt']\n",
        "\n",
        "        for file in files:\n",
        "            if os.path.exists(file):\n",
        "                os.remove(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlyU8FB0YFP7"
      },
      "source": [
        "## Starting Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "NUM_PARAMS = 8\n",
        "\n",
        "# Has to be 1-indexed because 0 means that layer has not been selected\n",
        "LAYER_TYPES = {\n",
        "  0: None,\n",
        "  1: \"Dense\",\n",
        "  2: \"LSTM\",\n",
        "  3: \"Dropout\",\n",
        "  4: \"GRU\"\n",
        "}\n",
        "\n",
        "LSTM_ACTIVATIONS = {\n",
        "    0: None,\n",
        "    1: \"tanh\",\n",
        "    2: \"relu\",\n",
        "    3: \"sigmoid\",\n",
        "    4: \"linear\",\n",
        "}\n",
        "\n",
        "GRU_ACTIVATIONS = {\n",
        "    0: None,\n",
        "    1: \"tanh\",\n",
        "    2: \"relu\",\n",
        "    3: \"sigmoid\",\n",
        "    4: \"linear\",\n",
        "}"
      ],
      "metadata": {
        "id": "yC1TqT__2BHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd6Cm3k5ovhm"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Input, GRU\n",
        "\n",
        "# generic model design\n",
        "def model_fn(actions):\n",
        "    num_layers = []\n",
        "    layer_types = []\n",
        "    dense_units = []\n",
        "    lstm_units = []\n",
        "    lstm_activations = []\n",
        "    dropout_units = []\n",
        "    GRU_units = []\n",
        "    GRU_activations = []\n",
        "\n",
        "    for i in range(len(actions)):\n",
        "      if i % NUM_PARAMS == 0:\n",
        "        num_layers.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 1:\n",
        "        layer_types.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 2:\n",
        "        dense_units.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 3:\n",
        "        lstm_units.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 4:\n",
        "        lstm_activations.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 5:\n",
        "        dropout_units.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 6:\n",
        "        GRU_units.append(actions[i])\n",
        "      elif i % NUM_PARAMS == 7:\n",
        "        GRU_activations.append(actions[i])\n",
        "\n",
        "    model = Sequential()\n",
        "    for i in range(len(num_layers)):\n",
        "      if num_layers[0] == i:\n",
        "        break\n",
        "      else:\n",
        "        to_add = LAYER_TYPES[layer_types[i]]\n",
        "        if to_add == None:\n",
        "          continue\n",
        "        if to_add == \"Dense\" and i == 0:\n",
        "          model.add(Dense(units = dense_units[i], input_shape = (X_train.shape[1], 1)))\n",
        "        elif to_add == \"Dense\":\n",
        "          model.add(Dense(units = dense_units[i]))\n",
        "        elif to_add == \"LSTM\" and i == 0:\n",
        "          act_to_use = LSTM_ACTIVATIONS[lstm_activations[i]]\n",
        "          if act_to_use == None:\n",
        "            act_to_use = \"tanh\"\n",
        "          model.add(LSTM(units = lstm_units[i], return_sequences = True, activation=act_to_use,\n",
        "                         input_shape = (X_train.shape[1], 1)))\n",
        "        elif to_add == \"LSTM\":\n",
        "          act_to_use = LSTM_ACTIVATIONS[lstm_activations[i]]\n",
        "          if act_to_use == None:\n",
        "            act_to_use = \"tanh\"\n",
        "          model.add(LSTM(units = lstm_units[i], return_sequences = True, activation=act_to_use))\n",
        "        elif to_add == \"Dropout\":\n",
        "          model.add(Dropout(dropout_units[i]))\n",
        "        elif to_add == \"GRU\" and i == 0:\n",
        "          act_to_use = GRU_ACTIVATIONS[GRU_activations[i]]\n",
        "          if act_to_use == None:\n",
        "            act_to_use = \"tanh\"\n",
        "          model.add(GRU(units = GRU_units[i], return_sequences = True, activation=act_to_use,\n",
        "                         input_shape = (X_train.shape[1], 1)))\n",
        "        elif to_add == \"GRU\":\n",
        "          act_to_use = GRU_ACTIVATIONS[GRU_activations[i]]\n",
        "          if act_to_use == None:\n",
        "            act_to_use = \"tanh\"\n",
        "          model.add(GRU(units = GRU_units[i], return_sequences = True, activation=act_to_use))\n",
        "          \n",
        "    model.add(LSTM(units = 50))\n",
        "    model.add(Dense(units = 1))\n",
        "\n",
        "    # Compiling the RNN\n",
        "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y38DdXtxYDti"
      },
      "source": [
        "## Network Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm6jDrvkosD6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "class NetworkManager:\n",
        "    '''\n",
        "    Helper class to manage the generation of subnetwork training given a dataset\n",
        "    '''\n",
        "    def __init__(self, dataset, epochs=5, child_batchsize=128, acc_beta=0.8, clip_rewards=0.0):\n",
        "        '''\n",
        "        Manager which is tasked with creating subnetworks, training them on a dataset, and retrieving\n",
        "        rewards in the term of accuracy, which is passed to the controller RNN.\n",
        "\n",
        "        Args:\n",
        "            dataset: a tuple of 4 arrays (X_train, y_train, X_val, y_val)\n",
        "            epochs: number of epochs to train the subnetworks\n",
        "            child_batchsize: batchsize of training the subnetworks\n",
        "            acc_beta: exponential weight for the accuracy\n",
        "            clip_rewards: float - to clip rewards in [-range, range] to prevent\n",
        "                large weight updates. Use when training is highly unstable.\n",
        "        '''\n",
        "        self.dataset = dataset\n",
        "        self.epochs = epochs\n",
        "        self.batchsize = child_batchsize\n",
        "        self.clip_rewards = clip_rewards\n",
        "\n",
        "        self.beta = acc_beta\n",
        "        self.beta_bias = acc_beta\n",
        "        self.moving_loss = 10\n",
        "\n",
        "    def get_rewards(self, model_fn, actions):\n",
        "        '''\n",
        "        Creates a subnetwork given the actions predicted by the controller RNN,\n",
        "        trains it on the provided dataset, and then returns a reward.\n",
        "\n",
        "        Args:\n",
        "            model_fn: a function which accepts one argument, a list of\n",
        "                parsed actions, obtained via an inverse mapping from the\n",
        "                StateSpace.\n",
        "            actions: a list of parsed actions obtained via an inverse mapping\n",
        "                from the StateSpace. It is in a specific order as given below:\n",
        "\n",
        "                Consider 4 states were added to the StateSpace via the `add_state`\n",
        "                method. Then the `actions` array will be of length 4, with the\n",
        "                values of those states in the order that they were added.\n",
        "\n",
        "                If number of layers is greater than one, then the `actions` array\n",
        "                will be of length `4 * number of layers` (in the above scenario).\n",
        "                The index from [0:4] will be for layer 0, from [4:8] for layer 1,\n",
        "                etc for the number of layers.\n",
        "\n",
        "                These action values are for direct use in the construction of models.\n",
        "\n",
        "        Returns:\n",
        "            a reward for training a model with the given actions\n",
        "        '''\n",
        "        with tf.Session(graph=tf.Graph()) as network_sess:\n",
        "            tf.compat.v1.keras.backend.set_session(network_sess)\n",
        "\n",
        "            # generate a submodel given predicted actions\n",
        "            model = model_fn(actions)  # type: Model\n",
        "            model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "            # unpack the dataset\n",
        "            X_train, y_train, X_val, y_val = self.dataset\n",
        "\n",
        "            # train the model using Keras methods\n",
        "            model.fit(X_train, y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
        "                      verbose=1, validation_data=(X_val, y_val),\n",
        "                      callbacks=[ModelCheckpoint('weights/temp_network.h5',\n",
        "                                                 monitor='val_loss', verbose=1,\n",
        "                                                 save_best_only=True,\n",
        "                                                 save_weights_only=True)])\n",
        "\n",
        "            # load best performance epoch in this training session\n",
        "            model.load_weights('weights/temp_network.h5')\n",
        "\n",
        "            # evaluate the model\n",
        "            loss = model.evaluate(X_val, y_val, batch_size=self.batchsize)\n",
        "\n",
        "            # compute the reward\n",
        "            reward = (self.moving_loss - loss)\n",
        "\n",
        "            # if rewards are clipped, clip them in the range -0.05 to 0.05\n",
        "            if self.clip_rewards:\n",
        "                reward = np.clip(reward, -0.05, 0.05)\n",
        "\n",
        "            print()\n",
        "            print(\"Manager: EWA Loss = \", self.moving_loss)\n",
        "\n",
        "        # clean up resources and GPU memory\n",
        "        network_sess.close()\n",
        "\n",
        "        return reward, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrT-D8WvYGff"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1k6hhMCokLQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# create a shared session between Keras and Tensorflow\n",
        "policy_sess = tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session(policy_sess)\n",
        "\n",
        "# construct a state space\n",
        "state_space = StateSpace()\n",
        "\n",
        "# add states\n",
        "state_space.add_state(name='num_layers', values=[x+1 for x in range(1,NUM_LAYERS)])\n",
        "state_space.add_state(name='layer_type', values=[1, 2, 3, 4])\n",
        "state_space.add_state(name='dense_unit', values=[10, 20, 40, 50, 60, 80, 100])\n",
        "state_space.add_state(name='lstm_unit', values=[20, 40, 50, 60, 80, 100])\n",
        "state_space.add_state(name='lstm_activation', values=[1, 2, 3, 4])\n",
        "state_space.add_state(name='dropout_unit', values=[0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "state_space.add_state(name='gru_unit', values=[20, 40, 50, 60, 80, 100])\n",
        "state_space.add_state(name='gru_activation', values=[1, 2, 3, 4])\n",
        "\n",
        "# print the state space being searched\n",
        "state_space.print_state_space()\n",
        "\n",
        "dataset = [X_train, y_train, X_test, y_test]  # pack the dataset for the NetworkManager\n",
        "\n",
        "previous_loss = 10000000\n",
        "total_reward = 0.0\n",
        "\n",
        "with policy_sess.as_default():\n",
        "    # create the Controller and build the internal policy network\n",
        "    controller = Controller(policy_sess, NUM_LAYERS, state_space,\n",
        "                            reg_param=REGULARIZATION,\n",
        "                            exploration=EXPLORATION,\n",
        "                            controller_cells=CONTROLLER_CELLS,\n",
        "                            embedding_dim=EMBEDDING_DIM,\n",
        "                            restore_controller=RESTORE_CONTROLLER)\n",
        "\n",
        "# create the Network Manager\n",
        "manager = NetworkManager(dataset, epochs=MAX_EPOCHS, child_batchsize=CHILD_BATCHSIZE, clip_rewards=CLIP_REWARDS,\n",
        "                         acc_beta=ACCURACY_BETA)\n",
        "\n",
        "# get an initial random state space if controller needs to predict an\n",
        "# action from the initial state\n",
        "state = state_space.get_random_state_space(NUM_LAYERS)\n",
        "print(\"Initial Random State : \", state_space.parse_state_space_list(state))\n",
        "print()\n",
        "\n",
        "# clear the previous files\n",
        "controller.remove_files()\n",
        "\n",
        "best_actions = None\n",
        "best_reward = 0\n",
        "second_run_start = False\n",
        "\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "for run in range(2):\n",
        "  if run == 1:\n",
        "    EXPLORATION = 0.2\n",
        "    second_run_start = True\n",
        "  # train for number of trails\n",
        "  for trial in range(MAX_TRIALS):\n",
        "      if run == 0:\n",
        "        with policy_sess.as_default():\n",
        "            tf.compat.v1.keras.backend.set_session(policy_sess)\n",
        "            actions = controller.get_action(state)  # get an action for the previous state\n",
        "      elif second_run_start:\n",
        "        actions = best_actions\n",
        "        second_run_start = False\n",
        "\n",
        "      print(\"trying these actions: \", actions)\n",
        "      # print the action probabilities\n",
        "      state_space.print_actions(actions)\n",
        "      print(\"Predicted actions : \", state_space.parse_state_space_list(actions))\n",
        "\n",
        "      # build a model, train and get reward and accuracy from the network manager\n",
        "      reward, previous_loss = manager.get_rewards(model_fn, state_space.parse_state_space_list(actions))\n",
        "      print(\"Rewards : \", reward, \"Loss : \", previous_loss)\n",
        "\n",
        "      if reward > best_reward:\n",
        "        best_reward = reward\n",
        "        best_actions = actions\n",
        "\n",
        "      with policy_sess.as_default():\n",
        "          tf.compat.v1.keras.backend.set_session(policy_sess)\n",
        "\n",
        "          total_reward += reward\n",
        "          print(\"Total reward : \", total_reward)\n",
        "\n",
        "          # actions and states are equivalent, save the state and reward\n",
        "          state = actions\n",
        "          controller.store_rollout(state, reward)\n",
        "\n",
        "          # train the controller on the saved state and the discounted rewards\n",
        "          loss = controller.train_step()\n",
        "          print(\"Trial %d: Controller loss : %0.6f\" % (trial + 1, loss))\n",
        "\n",
        "          # write the results of this trial into a file\n",
        "          with open('train_history.csv', mode='a+') as f:\n",
        "              data = [previous_loss, reward]\n",
        "              data.extend(state_space.parse_state_space_list(state))\n",
        "              writer = csv.writer(f)\n",
        "              writer.writerow(data)\n",
        "      print()\n",
        "\n",
        "print(\"Total Reward : \", total_reward)\n",
        "\n",
        "with open('train_results.txt', mode='a+') as f:\n",
        "  t2 = time.perf_counter()\n",
        "  time = t2-t1\n",
        "  time_taken = 'Time taken to run:' + str(t2-t1)\n",
        "  f.write(time_taken)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "RL-NAS-Improved.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}